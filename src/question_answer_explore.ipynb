{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: openai in /Users/hao/.pyenv/versions/3.10.8/lib/python3.10/site-packages (0.26.4)\n",
      "Requirement already satisfied: tqdm in /Users/hao/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/hao/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from openai) (2.28.0)\n",
      "Requirement already satisfied: aiohttp in /Users/hao/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from openai) (3.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/hao/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/hao/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from requests>=2.20->openai) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hao/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from requests>=2.20->openai) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hao/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from requests>=2.20->openai) (3.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/hao/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/hao/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/hao/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from aiohttp->openai) (1.8.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/hao/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from aiohttp->openai) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/hao/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from aiohttp->openai) (22.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/hao/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: srt in /Users/hao/.pyenv/versions/3.10.8/lib/python3.10/site-packages (3.5.2)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: faiss-cpu in /Users/hao/.pyenv/versions/3.10.8/lib/python3.10/site-packages (1.7.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install openai\n",
    "! pip install srt\n",
    "! pip install faiss-cpu\n",
    "! pip install sentence_transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_HANDLER\"] = \"langchain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import string\n",
    "\n",
    "import srt\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import VectorDBQAWithSourcesChain\n",
    "from langchain import OpenAI\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS_FOLDER = Path(\"docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lecture_titles():\n",
    "    return {\n",
    "        1: \"lecture-1-course-vision-and-when-to-use-ml\",\n",
    "        2: \"lecture-2-development-infrastructure-and-tooling\",\n",
    "        3: \"lecture-3-troubleshooting-and-testing\",\n",
    "        4: \"lecture-4-data-management\",\n",
    "        5: \"lecture-5-deployment\",\n",
    "        6: \"lecture-6-continual-learning\",\n",
    "        7: \"lecture-7-foundation-models\",\n",
    "        8: \"lecture-8-teams-and-pm\",\n",
    "        9: \"lecture-9-ethics\"\n",
    "    }\n",
    "\n",
    "\n",
    "def get_srt_urls():\n",
    "    return {\n",
    "        1: \"https://www.youtube.com/watch?v=-Iob-FW5jVM\",\n",
    "        2: \"https://www.youtube.com/watch?v=BPYOsDCZbno\",\n",
    "        3: \"https://www.youtube.com/watch?v=RLemHNAO5Lw\",\n",
    "        4: \"https://www.youtube.com/watch?v=Jlm4oqW41vY\",\n",
    "        5: \"https://www.youtube.com/watch?v=W3hKjXg7fXM\",\n",
    "        6: \"https://www.youtube.com/watch?v=nra0Tt3a-Oc\",\n",
    "        7: \"https://www.youtube.com/watch?v=Rm11UeGwGgk\",\n",
    "        8: \"https://www.youtube.com/watch?v=a54xH6nT4Sw\",\n",
    "        9: \"https://www.youtube.com/watch?v=7FQpbYTqjAA\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('docs/lecture-04.md'),\n",
       " PosixPath('docs/lecture-01.md'),\n",
       " PosixPath('docs/lecture-05.md'),\n",
       " PosixPath('docs/lecture-08.md'),\n",
       " PosixPath('docs/lecture-09.md'),\n",
       " PosixPath('docs/lecture-02.md'),\n",
       " PosixPath('docs/lecture-06.md'),\n",
       " PosixPath('docs/lecture-07.md'),\n",
       " PosixPath('docs/lecture-03.md')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lecture_md_filenames = [\n",
    "    elem for elem in DOCS_FOLDER.iterdir() if elem.is_file() and \"lecture\" in str(elem) and str(elem).endswith(\"md\")]\n",
    "lecture_md_filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'lecture-1-course-vision-and-when-to-use-ml',\n",
       " 2: 'lecture-2-development-infrastructure-and-tooling',\n",
       " 3: 'lecture-3-troubleshooting-and-testing',\n",
       " 4: 'lecture-4-data-management',\n",
       " 5: 'lecture-5-deployment',\n",
       " 6: 'lecture-6-continual-learning',\n",
       " 7: 'lecture-7-foundation-models',\n",
       " 8: 'lecture-8-teams-and-pm',\n",
       " 9: 'lecture-9-ethics'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lecture_titles = get_lecture_titles()\n",
    "lecture_titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lecture_texts = {}\n",
    "for fn in lecture_md_filenames:\n",
    "    idx = int(\"\".join(elem for elem in str(fn) if elem in string.digits))\n",
    "    lecture = fn.open().read()\n",
    "    lecture_texts[idx] = lecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lecture_texts[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "lecture_texts_split = {idx: text_splitter.split_text(\n",
    "    lecture_text) for idx, lecture_text in lecture_texts.items()}\n",
    "website_url_base = \"https://fullstackdeeplearning.com/course/2022/\"\n",
    "source_urls = {idx: website_url_base +\n",
    "               title for idx, title in lecture_titles.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source URL as the key, lecture split list as the value\n",
    "url_to_text_split = dict([(url, text_split) for url, text_split in zip(\n",
    "    source_urls.values(), lecture_texts_split.values())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://fullstackdeeplearning.com/course/2022/lecture-1-course-vision-and-when-to-use-ml': ['---\\ndescription: Sourcing, storing, exploring, processing, labeling, and versioning data for deep learning.\\n---\\n\\n# Lecture 4: Data Management\\n\\n<div align=\"center\">\\n<iframe width=\"720\" height=\"405\" src=\"https://www.youtube-nocookie.com/embed/Jlm4oqW41vY?list=PL1T8fO7ArWleMMI8KPJ_5D5XSlovTW_Ur\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\\n</div>\\n\\nLecture by [Sergey Karayev](https://sergeykarayev.com).<br />\\nNotes by [James Le](https://twitter.com/le_james94) and [Vishnu Rachakonda](https://www.linkedin.com/in/vrachakonda/).<br />\\nPublished August 29, 2022.\\n[Download slides](https://fsdl.me/2022-lecture-04-slides).\\n\\n## 1 - Introduction\\n\\nOne thing people don\\'t quite get as they enter the field of ML is how\\nmuch of it deals with data - putting together datasets, exploring the\\ndata, wrangling the data, etc. The key points of this lecture are:',\n",
       "  '1.  Spend 10x as much time exploring the data as you would like to.\\n\\n2.  Fixing, adding, and augmenting the data is usually the best way to\\nimprove performance.\\n\\n3.  Keep it all simple!\\n\\n## 2 - Data Sources\\n\\n![](./media/image9.png)\\n\\nThere are many possibilities for the sources of data. You might have\\nimages, text files, logs, or database records. In deep learning, you\\nneed to get that data into a local filesystem disk next to a GPU. **How\\nyou send data from the sources to training is different for each\\nproject**.\\n\\n-   With images, you can simply download them from S3.\\n\\n-   With text files, you need to process them in some distributed way,\\nanalyze the data, select a subset, and put that on a local\\nmachine.\\n\\n-   With logs and database records, you can use a data lake to aggregate\\nand process the data.\\n\\n![](./media/image2.png)\\n\\n\\nThe basics will be the same - a filesystem, object storage, and\\ndatabases.\\n\\n### Filesystem',\n",
       "  'The **filesystem** is a fundamental abstraction. Its fundamental unit is\\na file - which can be text or binary, is not versioned, and is easily\\noverwritten. The filesystem is usually on a disk connected to your\\nmachine - physically connected on-prem, attached in the cloud, or even\\ndistributed.\\n\\nThe first thing to know about discs is that their speed and bandwidth\\nrange - from hard discs to solid-state discs. There are two orders of\\nmagnitude differences between the slowest (SATA SSD) and the fastest\\n(NVMe SSD) discs. Below are some latency numbers you should know, with\\nthe human-scale numbers in parentheses:\\n\\n![](./media/image12.png)\\n\\n\\nWhat formats should the data be stored on the local disc?\\n\\n-   If you work with binary data like images and audio, just use the\\nstandard formats like JPEG or MP3 that it comes in.',\n",
       "  \"-   If you work with metadata (like labels), tabular data, or text data,\\nthen compressed JSON or text files are just fine. Alternatively,\\nParquet is a table format that is fast, compact, and widely used.\\n\\n### Object Storage\\n\\nThe **object storage** is an API over the filesystem. Its fundamental\\nunit is an object, usually in a binary format (an image, a sound file, a\\ntext file, etc.). We can build versioning or redundancy into the object\\nstorage service. It is not as fast as the local filesystem, but it can b\\nfast enough within the cloud.\\n\\n### Databases\\n\\n**Databases** are persistent, fast, and scalable storage and retrieval\\nof structured data systems. A helpful mental model for this is: all the\\ndata that the databases hold is actually in the computer\\\\'s RAM, but the\\ndatabase software ensures that if the computer gets turned off,\\neverything is safely persisted to disk. If too much data is in the RAM,\\nit scales out to disk in a performant way.\",\n",
       "  'You should not store binary data in the database but the object-store\\nURLs instead. [Postgres](https://www.postgresql.org/) is\\nthe right choice most of the time. It is an open-source database that\\nsupports unstructured JSON and queries over that JSON.\\n[SQLite](https://www.sqlite.org/) is also perfectly good\\nfor small projects.\\n\\nMost coding projects that deal with collections of objects that\\nreference each other will eventually implement a crappy database. Using\\na database from the beginning with likely save you time. In fact, most\\nMLOps tools are databases at their core (e.g.,\\n[W&B](https://wandb.ai/site) is a database of experiments,\\n[HuggingFace Hub](https://huggingface.co/models) is a\\ndatabase of models, and [Label\\nStudio](https://labelstud.io/) is a database of labels).\\n\\n![](./media/image11.png)',\n",
       "  '**Data warehouses** are stores for online analytical processing (OLAP),\\nas opposed to databases being the data stores for online transaction\\nprocessing (OLTP). You get data into the data warehouse through a\\nprocess called **ETL (Extract-Transform-Load)**: Given a number of data\\nsources, you extract the data, transform it into a uniform schema, and\\nload it into the data warehouse. From the warehouse, you can run\\nbusiness intelligence queries. The difference between OLAP and OLTP is\\nthat: OLAPs are column-oriented, while OLTPs are row-oriented.\\n\\n![](./media/image13.png)\\n\\n\\n**Data lakes** are unstructured aggregations of data from multiple\\nsources. The main difference between them and data warehouses is that\\ndata lakes use ELT (Extract-Load-Transform) process: dumping all the\\ndata in and transforming them for specific needs later.',\n",
       "  '**The big trend is unifying both data lake and data warehouse, so that\\nstructured data and unstructured data can live together**. The two big\\nplatforms for this are\\n[Snowflake](https://www.snowflake.com/) and\\n[Databricks](https://www.databricks.com/). If you are\\nreally into this stuff, \"[Designing Data-Intensive\\nApplications](https://dataintensive.net/)\" is a great book\\nthat walks through it from first principles.\\n\\n## 3 - Data Exploration\\n\\n![](./media/image4.png)\\n\\nTo explore the data, you must speak its language, mostly SQL and,\\nincreasingly, DataFrame. **SQL** is the standard interface for\\nstructured data, which has existed for decades. **Pandas** is the main\\nDataFrame in the Python ecosystem that lets you do SQL-like things. Our\\nadvice is to become fluent in both to interact with both transactional\\ndatabases and analytical warehouses and lakes.',\n",
       "  \"[Pandas](https://pandas.pydata.org/) is the workhorse of\\nPython data science. You can try [DASK\\nDataFrame](https://examples.dask.org/dataframe.html) to\\nparallelize Pandas operations over cores and\\n[RAPIDS](https://rapids.ai/) to do Pandas operations on\\nGPUs.\\n\\n## 4 - Data Processing\\n\\n![](./media/image8.png)\\n\\nTalking about data processing, it's useful to have a motivational\\nexample. Let's say we have to train a photo popularity predictor every\\nnight. For each photo, the training data must include:\\n\\n1.  Metadata (such as posting time, title, and location) that sits in\\nthe database.\\n\\n2.  Some features of the user (such as how many times they logged in\\ntoday) that are needed to be computed from logs.\\n\\n3.  Outputs of photo classifiers (such as content and style) that are\\nneeded to run the classifiers.\",\n",
       "  \"Our ultimate task is to train the photo predictor model, but we need to\\noutput data from the database, compute the logs, and run classifiers to\\noutput their predictions. As a result, we have **task dependencies**.\\nSome tasks can't start until others are finished, so finishing a task\\nshould kick off its dependencies.\\n\\nIdeally, dependencies are not always files but also programs and\\ndatabases. We should be able to spread this work over many machines and\\nexecute many dependency graphs all at once.\\n\\n![](./media/image7.png)\\n\\n\\n-   [Airflow](https://airflow.apache.org/) is a standard\\nscheduler for Python, where it's possible to specify the DAG\\n(directed acyclic graph) of tasks using Python code. The operator\\nin that graph can be SQL operations or Python functions.\\n\\n-   To distribute these jobs, the workflow manager has a queue for the\\ntasks and manages the workers that pull from them. It will restart\\njobs if they fail and ping you when the jobs are finished.\",\n",
       "  \"-   [Prefect](https://www.prefect.io/) and\\n[Dagster](https://dagster.io/) are contenders to\\nimprove and replace Airflow in the long run.\\n\\nThe primary advice here is not to **over-engineer things**. You can get\\nmachines with many CPU cores and a lot of RAM nowadays. For example,\\nUNIX has powerful parallelism, streaming, and highly optimized tools.\\n\\n## 5 - Feature Store\\n\\n![](./media/image3.png)\\n\\nLet's say your data processing generates artifacts you need for\\ntraining. How do you make sure that, in production, the trained model\\nsees the same processing taking place (which happened during training)?\\nHow do you avoid recomputation during retraining?\\n\\n**Feature stores** are a solution to this (that you may not need!).\",\n",
       "  '-   The first mention of feature stores came from [this Uber blog post\\ndescribing their ML platform,\\nMichelangelo](https://eng.uber.com/michelangelo-machine-learning-platform/).\\nThey had an offline training process and an online prediction\\nprocess, so they built an internal feature store for both\\nprocesses to be in sync.\\n\\n-   [Tecton](https://www.tecton.ai/) is the leading SaaS\\nsolution to feature store.\\n\\n-   [Feast](https://feast.dev/) is a common open-source\\noption.\\n\\n-   [Featureform](https://www.featureform.com/) is a\\nrelatively new option.\\n\\n## 6 - Datasets\\n\\n![](./media/image1.png)\\n\\nWhat about datasets specifically made for machine learning?',\n",
       "  \"[HuggingFace\\nDatasets](https://huggingface.co/docs/datasets) is a great\\nsource of machine learning-ready data. There are 8000+ datasets covering\\na wide variety of tasks, like computer vision, NLP, etc. The Github-Code\\ndataset on HuggingFace is a good example of how these datasets are\\nwell-suited for ML applications. Github-Code can be streamed, is in the\\nmodern Apache Parquet format, and doesn't require you to download 1TB+\\nof data in order to properly work with it. Another sample dataset is\\nRedCaps, which consists of 12M image-text pairs from Reddit.\\n\\n![](./media/image15.png)\\n\\n\\nAnother interesting dataset solution for machine learning is\\n[Activeloop](https://www.activeloop.ai/). This tool is\\nparticularly well equipped to work with data and explore samples without\\nneeding to download it.\\n\\n## 7 - Data Labeling\\n\\n![](./media/image10.png)\\n\\n### No Labeling Required\",\n",
       "  \"The first thing to talk about when it comes to labeling data\\nis...**maybe we don\\\\'t have to label data?** There are a couple of\\noptions here we will cover.\\n\\n**Self-supervised learning** is a very important idea that allows you to\\navoid painstakingly labeling all of your data. You can use parts of your\\ndata to label other parts of your data. This is very common in NLP right\\nnow. This is further covered in the foundation model lecture. The long\\nand short of it is that models can have elements of their data masked\\n(e.g., the end of a sentence can be omitted), and models can use earlier\\nparts of the data to predict the masked parts (e.g., I can learn from\\nthe beginning of the sentence and predict the end). This can even be\\nused across modalities (e.g., computer vision *and* text), as [OpenAI\\nCLIP](https://github.com/openai/CLIP) demonstrates.\\n\\n![](./media/image14.png)\",\n",
       "  '**Image data augmentation** is an almost compulsory technique to adopt,\\nespecially for vision tasks. Frameworks like\\n[torchvision](https://github.com/pytorch/vision) help with\\nthis. In data augmentation, samples are modified (e.g., brightened)\\nwithout actually changing their core \"meaning.\" Interestingly,\\naugmentation can actually replace labels.\\n[SimCLR](https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html)\\nis a model that demonstrates this - where its learning objective is to\\nmaximize agreement between augmented views of the same image and\\nminimize agreement between different images.\\n\\nFor other forms of data, there are a couple of augmentation tricks that\\ncan be applied. You can delete some cells in tabular data to simulate\\nmissing data. In text, there aren\\'t established techniques, but ideas\\ninclude changing the order of words or deleting words. In speech, you\\ncould change the speed, insert pauses, etc.',\n",
       "  \"**Synthetic data** is an underrated idea. You can synthesize data based\\non your knowledge of the label. For example, you can [create\\nreceipts](https://github.com/amoffat/metabrite-receipt-tests)\\nif your need is to learn how to recognize receipts from images. This can\\nget very sophisticated and deep, so tread carefully.\\n\\nYou can also get creative and ask your users to label data for you.\\nGoogle Photos, as any user of the app knows, regularly gets users to\\nlabel images about where people in photos are the same or different.\\n\\n![](./media/image16.png)\\n\\n\\nThis is an example of the data flywheel. Improving the data allows the\\nuser to improve the model, which in turn makes their product experience\\nbetter.\\n\\n### Labeling Solutions\\n\\nThese are all great options for avoiding labeling data. However,\\n**you'll usually have to label some data to get started.**\",\n",
       "  'Labeling has standard annotation features, like bounding boxes, that\\nhelp capture information properly. Training annotators properly is more\\nimportant than the particular kind of annotation. Standardizing how\\nannotators approach a complex, opinable task is crucial. Labeling\\nguidelines can help capture the exact right label from an annotator.\\nQuality assurance is key to ensuring annotation and labeling are\\nhappening properly.\\n\\nThere are a few options for sourcing labor for annotations:\\n\\n1.  Full-service data labeling vendors offer end-to-end labeling\\nsolutions.\\n\\n2.  You can hire and train annotators yourself.\\n\\n3.  You can crowdsource annotation on a platform like Mechanical Turk.',\n",
       "  '**Full-service companies offer a great solution that abstracts the need\\nto build software, manage labor, and perform quality checks**. It makes\\nsense to use one. Before settling on one, make sure to dedicate time to\\nvet several. Additionally, label some gold standard data yourself to\\nunderstand the data yourself and to evaluate contenders. Take calls with\\nseveral contenders, ask for work samples on your data, and compare them\\nto your own labeling performance.\\n\\n-   [Scale AI](https://scale.com/) is the dominant data\\nlabeling solution. It offers an API that allows you to spin up\\ntasks.\\n\\n-   Additional contenders include\\n[Labelbox](https://labelbox.com/) and\\n[Supervisely](https://supervise.ly/).\\n\\n-   [LabelStudio](https://labelstud.io/) is an open-source\\nsolution for performing annotation yourself, with a companion\\nenterprise version. It has a great set of features that allow you\\nto design your interface and even plug-in models for active\\nlearning!',\n",
       "  '-   [Diffgram](https://diffgram.com/) is a competitor to\\nLabel Studio.\\n\\n-   Recent offerings, like\\n[Aquarium](https://www.aquariumlearning.com/) and\\n[Scale Nucleus](https://scale.com/nucleus), have\\nstarted to help concentrate labeling efforts on parts of the\\ndataset that are most troublesome for models.\\n\\n-   [Snorkel](https://snorkel.ai/) is a dataset management\\nand labeling platform that uses weak supervision, which is a\\nsimilar concept. You can leverage composable rules (e.g., all\\nsentences that have the term \"amazing\" are positive sentiments)\\nthat allow you to quickly label data faster than if you were to\\ntreat every data point the same.\\n\\nIn conclusion, try to avoid labeling using techniques like\\nself-supervised learning. If you can\\'t, use labeling software and\\neventually outsource the work to the right vendor. If you can\\'t afford\\nvendors, consider hiring part-time work rather than crowdsourcing the\\nwork to ensure quality.\\n\\n## 8 - Data Versioning\\n\\n![](./media/image6.png)',\n",
       "  'Data versioning comes with a spectrum of approaches:\\n\\n1.  Level 0 is bad. In this case, data just lives on some file system.\\nIn these cases, the issue arises because the models are\\nunversioned since their data is unversioned. Models are part code,\\npart data. This will lead to the consequence of being unable to\\nget back to a previous level of performance if need be.\\n\\n2.  You can prevent this event with Level 1, where you snapshot your\\ndata each time you train. This somewhat works but is far from\\nideal.\\n\\n3.  In Level 2, data is versioned like code, as a commingled asset with\\nversioned code. You can use a system like\\n[git-lfs](https://git-lfs.github.com/) that allows\\nyou to store large data assets alongside code. This works really\\nwell!\\n\\n4.  Level 3 involves specialized solutions for working with large data\\nfiles, but this may not be needed unless you have a very specific\\nneed (i.e., uniquely large or compliance-heavy files).\\n\\n![](./media/image5.png)',\n",
       "  \"[DVC](https://dvc.org/) is a great tool for this. DVC\\nhelps upload your data asset to a remote storage location every time you\\ncommit changes to the data file or trigger a commit; it functions like a\\nfancier git-lfs. It adds features like lineage for data and model\\nartifacts, allowing you to recreate pipelines.\\n\\nSeveral techniques are associated with privacy-controlled data, like\\n[federated\\nlearning](https://blog.ml.cmu.edu/2019/11/12/federated-learning-challenges-methods-and-future-directions/),\\ndifferential privacy, and learning on encrypted data. These techniques\\nare still in research, so they aren't quite ready for an FSDL\\nrecommendation.\"],\n",
       " 'https://fullstackdeeplearning.com/course/2022/lecture-2-development-infrastructure-and-tooling': ['---\\ndescription: Introduction to planning, developing, and shipping ML-powered products.\\n---\\n\\n# Lecture 1: Course Vision and When to Use ML\\n\\n<div align=\"center\">\\n<iframe width=\"720\" height=\"405\" src=\"https://www.youtube-nocookie.com/embed/-Iob-FW5jVM?list=PL1T8fO7ArWleMMI8KPJ_5D5XSlovTW_Ur\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\\n</div>\\n\\nLecture by [Josh Tobin](https://twitter.com/josh_tobin_).\\nNotes by [James Le](https://twitter.com/le_james94) and [Vishnu Rachakonda](https://www.linkedin.com/in/vrachakonda/).<br />\\nPublished August 8, 2022.\\n[Download slides](https://drive.google.com/file/d/18EVuJpnJ9z5Pz7oRYcgax_IzRVhbuAMC/view?usp=sharing).\\n\\n## 1 - Course Vision\\n\\n### History of FSDL',\n",
       "  \"**Full Stack Deep Learning (FSDL) is the course and community for people\\nwho are building products that are powered by machine learning (ML).**\\nIt's an exciting time to talk about ML-powered products because ML is\\nrapidly becoming a mainstream technology - as you can see in startup\\nfunding, job postings, and continued investments of large companies.\\n\\nFSDL was originally started in 2018 when the most exciting ML-powered\\nproducts were built by the biggest companies. However, the broader\\nnarrative in the field was that very few companies could get value out\\nof this technology.\",\n",
       "  \"Now in 2022, there's a proliferation of powerful products that are\\npowered by ML. The narrative has shifted as well: There's\\nstandardization that has emerged around the tech stack - with\\ntransformers and NLP starting to seep their way into more use cases, as\\nwell as practices around how to apply ML technologies in the world. One\\nof the biggest changes in the field in the past four years has been the\\nemergence of the term **MLOps**.\\n\\nIn addition to the\\nfield being more mature and research continuing to progress, a big\\nreason for this rapid change is that **the training of models is starting to become\\ncommoditized**.\\n\\n-   With tools like [HuggingFace](https://huggingface.co), you can deploy a state-of-the-art NLP\\nor CV model in one or two lines of code.\\n\\n-   AutoML is starting to work for a lot of applications.\",\n",
       "  \"-   Companies like [OpenAI](https://openai.com/api/) are starting to provide models as a service where you\\ndon't even have to download open-source packages to use them. You\\ncan make a network call to get predictions from a state-of-the-art\\nmodel.\\n\\n-   Many libraries are starting to standardize around frameworks like [Keras](https://keras.io/) and [PyTorch\\nLightning](https://www.pytorchlightning.ai/).\\n\\n### AI Progress\\n\\nThe history of ML is characterized by stratospheric rises and meteoric falls of the public\\nperception of the technology. These were driven by a few different AI\\nwinters that happened over the history of the field - where the\\ntechnology didn't live up to its hype. If you project forward a few\\nyears, what will happen to ML?\\n\\n![](./media/image6.png)\\n\\n\\n*Source: [5 Things You Should Know About\\nAI](https://www.cambridgewireless.co.uk/media/uploads/resources/AI%20Group/AIMobility-11.05.17-Cambridge_Consultants-Monty_Barlow.pdf)\\n(Cambridge Consultants, May 2017)*\",\n",
       "  \"Here are the major categories of possible outcomes and our guess about their likelihoods:\\n\\n1.  A true AI winter, where people\\nbecome skeptical about AI as a technology.\\nWe think this is less likely.\\n\\n2.  A slightly more likely outcome is that the overall luster of the\\ntechnology starts to wear off, but specific applications are\\ngetting a ton of value out of it.\\n\\n3.  The upside outcome for the field is that AI continues to accelerate\\nrapidly and becomes pervasive and incredibly effective.\\n\\nOur conjecture is that: **The way we, as a field, avoid an AI winter is\\nby translating research progress into real-world products.** That's how\\nwe avoid repeating what has happened in the past.\\n\\n### ML-Powered Products Require a Different Process\\n\\nBuilding ML-powered products requires a fundamentally different process\\nin many ways than developing ML models in an academic setting.\\n\\n![](./media/image7.png)\",\n",
       "  'In academia, you build **\"flat-earth\" ML** - selecting a problem,\\ncollecting data, cleaning and labeling the data, iterating on model\\ndevelopment until you have a model that performs well on the dataset\\ncollected, evaluating that model, and writing a report at the end.\\n\\n![](./media/image5.png)\\n\\n\\nBut ML-powered products require **an outer loop** where after you deploy\\nthe model into production, you measure how that model performs when it\\ninteracts with real users. Then, you use real-world data to\\nimprove your model, setting up a data flywheel that enables\\ncontinual improvement.\\n\\n### This Course\\n\\n![](./media/image2.png)\\n\\n\\nThis class is about the unique aspects you need to know beyond training\\nmodels to build great ML-powered products. Here are some concrete goals\\nfor us:\\n\\n1.  Teaching you **generalist skills** and an understanding of the\\n**components of ML-powered products** (and ML projects more\\ngenerally).\\n\\n2.  Teaching you **enough MLOps to get things done**.',\n",
       "  \"3.  Sharing **best practices** and **explaining the motivation** behind them.\\n\\n4.  Learning things that might **help you with job interviews** for ML engineering roles.\\n\\n5.  **Forming a community** to learn together and from each other.\\n\\nWe do NOT try to:\\n\\n1.  Teach you ML or software engineering from scratch.\\n\\n2.  Cover the whole breadth of deep learning techniques.\\n\\n3.  Make you an expert in any single aspect of ML.\\n\\n4.  Do research in deep learning.\\n\\n5.  Cover the full spectrum of MLOps.\\n\\nIf you feel rusty on your pre-requisites but want to get started with\\nFSDL, here are our recommendations to get up to speed with the\\nfundamentals:\\n\\n-   Andrew Ng's [Machine Learning Coursera\\ncourse](https://www.coursera.org/collections/machine-learning)\\n\\n-   Google's [crash course on Machine\\nLearning](https://developers.google.com/machine-learning/crash-course)\\n\\n-   MIT's [The Missing\\nSemester](https://missing.csail.mit.edu/) on software\\nengineering\\n\\n### ML-Powered Products vs MLOps\",\n",
       "  \"MLOps, as a discipline, has emerged in just the last few years. It is\\nabout practices for deploying, maintaining, and operating ML systems\\nthat generate ML models in production. A lot of MLOps is about:\\n\\n-   How do we put together an infrastructure that allows us to build\\nmodels in a repeatable and governable way?\\n\\n-   How can we run ML systems in a potentially high-scale production\\nsetting?\\n\\n-   How can we collaborate on these systems as a team?\\n\\n![](./media/image1.png)\\n\\n\\nML-powered product building is a distinct but overlapping discipline. A lot of\\nwhat it takes to build a great ML-powered product goes beyond the\\ninfrastructure side of ML systems. It focuses on how to fit ML into the\\ncontext of the product or the application that you're building.\\n\\nOther topics in the scope of the ML product discipline include:\\n\\n-   How do you understand how your users are interacting with your\\nmodel?\\n\\n-   How do you build a team or an organization that can work together\\neffectively on ML systems?\",\n",
       "  \"-   How do you do product management in the context of ML?\\n\\n-   What are the best practices for designing products that use ML as\\npart of them?\\n\\nThis class focuses on teaching you end-to-end what it takes to get a\\nproduct out in the world that uses ML and will cover aspects of MLOps\\nthat are most critical in order to do that.\\n\\n### Chapter Summary\\n\\n1.  **ML-powered products are going mainstream** thanks to the\\ndemocratization of modeling.\\n\\n2.  However, building **great ML-powered products requires a different\\nprocess** from building models.\\n\\n3.  Full-Stack Deep Learning is **here to help**!\\n\\n## 2 - When To Use Machine Learning\\n\\n### When to Use ML At All\\n\\n**ML projects have a higher failure rate than software projects in\\ngeneral**. One reason that's worth acknowledging is that for many\\napplications, ML is fundamentally still research. Therefore, we\\nshouldn't aim for 100% success.\\n\\nAdditionally, many ML projects are\\ndoomed to fail even before they are undertaken due to a variety of\\nreasons:\",\n",
       "  '1.  They are technically infeasible or poorly scoped.\\n\\n2.  They never make the leap to a production environment.\\n\\n3.  The broader organization is not all on the same page about what\\nwould be considered success criteria for them.\\n\\n4.  They solve the problem that you set out to solve but do not solve a\\nbig enough problem to be worth their complexity.\\n\\nThe bar for your ML projects should be that **their value must outweigh\\nnot just the cost of developing them but also the additional complexity\\nthat these ML systems introduce to your software** (as introduced in the\\nclassic paper \"[The High-Interest Credit Card of Technical\\nDebt](https://research.google/pubs/pub43146/)\").\\n\\nIn brief,\\nML systems erode the boundaries between other systems, rely on expensive\\ndata dependencies, are commonly plagued by system design anti-patterns,\\nand are subject to the instability of the external world.\\n\\nBefore starting an ML project, ask yourself:',\n",
       "  '1.  **Are you ready to use ML?** More specifically, do you have a\\nproduct? Are you collecting data and storing it in a sane way? Do\\nyou have the right people?\\n\\n2.  **Do you really need ML to solve this problem?** More specifically,\\ndo you need to solve the problem at all? Have you tried using\\nrules or simple statistics to solve the problem?\\n\\n3.  **Is it ethical to use ML to solve this problem?** We have a\\n[whole lecture about ethics](../lecture-9-ethics/)!\\n\\n### How to Pick Problems to Solve with ML\\n\\nJust like any other project prioritization, you want to look for use\\ncases that have **high impact** and **low cost**:\\n\\n1.  **High-impact problems** are likely to be those that address friction in\\nyour product, complex parts of your pipeline, places where cheap\\nprediction is valuable, and generally what other people in your\\nindustry are doing.\\n\\n2.  **Low-cost projects** are those with available data, where bad\\npredictions are not too harmful.\\n\\n![](./media/image11.png)\\n\\n\\n#### High-Impact Projects',\n",
       "  'Here are some heuristics that you can use to find high-impact ML\\nprojects:\\n\\n1.  **Find problems that ML takes from economically infeasible to feasible**.\\nA good resource here is the book \"[Prediction Machines:\\nThe Simple Economics of\\nAI](https://www.amazon.com/Prediction-Machines-Economics-Artificial-Intelligence/dp/1633695670).\"\\nThe book\\'s central thesis is that AI reduces the cost of\\nprediction, which is central to decision-making. Therefore, look\\nfor projects where making prediction cheaper will have a huge impact.\\n\\n2.  **Think about what your product needs**.\\n[This article from the ML team at Spotify](https://spotify.design/article/three-principles-for-designing-ml-powered-products)\\ntalks about the three principles for designing Discover Weekly,\\none of Spotify\\'s most powerful and popular ML-powered features.',\n",
       "  '3.  **Think about the types of problems that ML is particularly good at**.\\nOne common class of problem that is overlooked is\\n[\"Software 2.0\"](https://karpathy.medium.com/software-2-0-a64152b37c35),\\nas coined by Andrej Kaparthy. Essentially, if you have a part of your\\nsystem that is complex and manually defined, then that\\'s\\npotentially a good candidate to be automated with ML.\\n\\n4.  **Look at what other people in the industry are doing**.\\nGenerally, you can read papers and blog posts from both Big Tech and top\\nearlier-stage companies.\\n\\n#### Low-Cost Projects\\n\\n![](./media/image12.png)\\n\\n\\nThere are three main drivers for how much a project will cost:\\n\\n1.  **Data availability**: How hard is it to acquire data? How expensive\\nis data labeling? How much data will be needed? How stable is the\\ndata? What data security requirements do you have?',\n",
       "  \"2.  **Accuracy requirement**: How costly are wrong predictions? How\\nfrequently does the system need to be right to be useful? What are\\nthe ethical implications of your model making wrong predictions?\\nIt is noteworthy that **ML project costs tend to scale\\nsuper-linearly in the accuracy requirement**.\\n\\n3.  **Problem difficulty**: Is the problem well-defined enough to be\\nsolved with ML? Is there good published work on similar problems?\\nHow much compute does it take to solve the problem? **Generally,\\nit's hard to reason about what's feasible in ML**.\\n\\n#### What's Hard in ML?\\n\\n![](./media/image8.png)\\n\\n\\nHere are the three types of hard problems:\\n\\n1.  **Output is complex**: The model predictions are ambiguous or in a\\nhigh-dimensional structure.\\n\\n2.  **Reliability is required**: ML systems tend to fail in unexpected\\nways, so anywhere you need high precision or high robustness is\\ngoing to be more difficult to solve with ML.\",\n",
       "  '3.  **Generalization is required**: These problems tend to be more in\\nthe research domain. They can deal with out-of-distribution data\\nor do tasks such as reasoning, planning, or understanding\\ncausality.\\n\\n#### ML Feasibility Assessment\\n\\nThis is a quick checklist you can use to assess the feasibility of your\\nML projects:\\n\\n1.  Make sure that you actually need ML.\\n\\n2.  Put in the work upfront to define success criteria with all of the\\nstakeholders.\\n\\n3.  Consider the ethics of using ML.\\n\\n4.  Do a literature review.\\n\\n5.  Try to rapidly build a labeled benchmark dataset.\\n\\n6.  Build a \"minimum\" viable model using manual rules or simple\\nheuristics.\\n\\n7.  Answer this question again: \"Are you sure that you need ML at all?\"\\n\\n### Not All ML Projects Should Be Planned The Same Way\\n\\nNot all ML projects have the same characteristics; therefore, they\\nshouldn\\'t be planned the same way. Understanding different archetypes of\\nML projects can help select the right approach.\\n\\n#### ML Product Archetypes',\n",
       "  'The three archetypes offered here are defined by how they interact with\\nreal-world use cases:\\n\\n1.  **Software 2.0 use cases**: Broadly speaking, this means taking\\nsomething that software or a product does in an automated fashion\\ntoday and augmenting its automation with machine learning. An\\nexample of this would be improving code completion in the IDE\\n(like [Github\\nCopilot](https://github.com/features/copilot)).\\n\\n2.  **Human-in-the-loop systems:** Machine learning can be applied for\\ntasks where automation is not currently deployed - but where\\nhumans could have their judgment or efficiency augmented. Simply\\nput, helping humans do their jobs better by complementing them\\nwith ML-based tools. An example of this would be turning sketches\\ninto slides, a process will usually involve humans approving the\\noutput of a machine learning model that made the slides.',\n",
       "  '3.  **Autonomous systems:** Systems that apply machine learning to\\naugment existing or implement new processes without human input.\\nAn example of this would be full self-driving, where there is no\\nopportunity for a driver to intervene in the functioning of the\\ncar.\\n\\nFor each archetype, some key considerations inform how you should go\\nabout planning projects.\\n\\n![](./media/image10.png)\\n\\n\\n1.  In the case of Software 2.0 projects, you should focus more on\\nunderstanding **how impactful the performance of the new model\\nis**. Is the model truly much better? How can the performance\\ncontinue to increase across iterations?\\n\\n2.  In the case of human-in-the-loop systems, consider more **the\\ncontext of the human user and what their needs might be**. How\\ngood does the system actually have to be to improve the life of a\\nhuman reviewing its output? In some cases, a model that does even\\n10% better with accuracy (nominally a small increase) might have\\noutsize impacts on human users in the loop.',\n",
       "  \"3.  For autonomous systems, focus heavily on t**he failure rate and its\\nconsequences**. When there is no opportunity for human\\nintervention, as is the case with autonomous systems, failures\\nneed to be carefully monitored to ensure outsize harm doesn't\\noccur. Self-driving cars are an excellent example of an autonomous\\nsystem where failure rates are carefully monitored.\\n\\n#### Data Flywheels\\n\\nAs you build a software 2.0 project, strongly consider the concept of\\nthe **data flywheel**. For certain ML projects, as you improve your\\nmodel, your product will get better and more users will engage with the\\nproduct, thereby generating more data for the model to get even better.\\nIt's a classic virtuous cycle and truly the gold standard for ML\\nprojects.\\n\\n![](./media/image4.png)\\n\\n\\nAs you consider implementing data flywheels, remember to know the answer\\nto these three questions:\",\n",
       "  \"1.  **Do you have a data loop?** To build a data flywheel, you crucially\\nneed to be able to get labeled data from users in a scalable\\nfashion. This helps increase access to high-quality data and\\ndefine a data loop.\\n\\n2.  **Can you turn more data into a better model?** This somewhat falls\\nonto you as the modeling expert, but it may also not be the case\\nthat more data leads to significantly better performance. Make\\nsure you can actually translate data scale into better model\\nperformance.\\n\\n3.  **Does better model performance lead to better product use?** You\\nneed to verify that improvements with models are actually tied to\\nusers enjoying the product more and benefiting from it!\\n\\n#### Impact and Feasibility of ML Product Archetypes\\n\\nLet's visit our impact vs. feasibility matrix. Our three product\\narchetypes differ across the spectrum.\\n\\n![](./media/image9.png)\",\n",
       "  'This is a pretty intuitive evaluation you can apply to all your ML\\nprojects: **If it\\'s harder to build (like autonomous systems), it\\'s\\nlikely to have a greater impact**! There are ways, however, to change\\nthis matrix in the context of specific projects.\\n\\n1.  For **Software 2.0**, data flywheels can magnify impact by allowing\\nmodels to get much better and increase customer delight over time.\\n\\n2.  For **human-in-the-loop systems**, you can increase feasibility by\\nleveraging good product design. Thoughtful design can help reduce\\nexpectations and accuracy requirements. Alternatively, a \"good\\nenough\" mindset that prioritizes incremental delivery over time\\ncan make such systems more feasible.\\n\\n3.  For **autonomous systems**, leveraging humans in the loop can make\\ndevelopment more feasible by adding guardrails and reducing the\\npotential impact of failures.\\n\\n### Just Get Started!',\n",
       "  \"With all this discussion about archetypes and impact matrices, don't\\nforget the most important component of engineering: **actually\\nbuilding**! Dive in and get started. Start solving problems and iterate\\non solutions.\\n\\nOne common area practitioners trip up in is **tool fetishization.** As\\nMLOps and production ML have flourished, so too has the number of tools\\nand platforms that address various aspects of the ML process. You don't\\nneed to be perfect with your tooling before driving value from machine\\nlearning. Just because Google and Uber are doing things in a very\\nstructured, at-scale way doesn't mean you need to as well!\\n\\nIn this course, we will primarily focus on how to set things up the\\nright way to do machine learning in production without overcomplicating\\nit. This is an ML products-focused class, not an MLOps class! Check out\\nthis talk by Jacopo Tagliabue describing [MLOps at Reasonable\\nScale](https://www.youtube.com/watch?v=Ndxpo4PeEms) for a\\ngreat exposition of this mindset.\",\n",
       "  \"### Chapter Summary\\n\\n1.  ML adds complexity. Consider whether you really need it.\\n\\n2.  Make sure what you're working on is high impact, or else it might\\nget killed.\\n\\n## 3 - Lifecycle\\n\\nML adds complexity to projects and isn't always a value driver. Once you\\nknow, however, that it's the right addition to your project, what does\\nthe actual lifecycle look like? What steps do we embark upon as we\\nexecute?\\n\\nIn this course, the common running example we use is of **a pose\\nestimation problem**. We'll use this as a case study to demonstrate the\\nlifecycle and illustrate various points about ML-powered products.\\n\\n![](./media/image13.png)\\n\\n\\nHere's a graphic that visualizes the lifecycle of ML projects:\\n\\n![](./media/image3.png)\\n\\n\\nIt provides a very helpful structure. Watch from 48:00 to 54:00 to dive\\ndeeper into how this lifecycle occurs in the context of a real machine\\nlearning problem around pose estimation that Josh worked on at OpenAI.\\n\\nLet's comment on some specific nuances:\",\n",
       "  '-   **Machine learning projects tend to be very iterative**. Each of\\nthese phases can feed back into any of the phases that go before\\nit, as you learn more about the problem that you\\'re working on.\\n\\n    -   For example, you might realize that \"Actually, it\\'s way too\\nhard for us to get data in order to solve this problem!\" or\\n\"It\\'s really difficult for us to label the pose of these\\nobjects in 3D space\".\\n\\n    -   A solution might actually be to go back a step in the lifecycle\\nand set up the problem differently. For example, what if it\\nwere cheaper to annotate per pixel?\\n\\n    -   This could repeat itself multiple times as you progress through\\na project. It\\'s a normal and expected part of the machine\\nlearning product development process.\\n\\n-   In addition to iteration during execution, there\\'s also\\ncross-project \"platform\" work that matters! **Hiring and\\ninfrastructure development are crucial to the long-term health of\\nyour project**.',\n",
       "  \"-   Going through this lifecycle and winning each step is what we'll\\ncover in this class!\\n\\n## Lecture Summary\\n\\nIn summary, here's what we covered in this lecture:\\n\\n1.  ML is NOT a cure-all. It's a complex technology that needs to be\\nused thoughtfully.\\n\\n2.  You DON'T need a perfect setup to get going. Start building and\\niterate!\\n\\n3.  The lifecycle of machine learning is purposefully iterative and\\ncircuitous. We'll learn how to master this process together!\"],\n",
       " 'https://fullstackdeeplearning.com/course/2022/lecture-3-troubleshooting-and-testing': ['---\\ndescription: How to turn an ML model into an ML-powered product\\n---\\n\\n# Lecture 5: Deployment\\n\\n<div align=\"center\">\\n<iframe width=\"720\" height=\"405\" src=\"https://www.youtube-nocookie.com/embed/W3hKjXg7fXM?list=PL1T8fO7ArWleMMI8KPJ_5D5XSlovTW_Ur\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\\n</div>\\n\\nLecture by [Josh Tobin](https://twitter.com/josh_tobin_).<br />\\nNotes by [James Le](https://twitter.com/le_james94) and [Vishnu Rachakonda](https://www.linkedin.com/in/vrachakonda/).<br />\\nPublished September 5, 2022.\\n[Download slides](https://fsdl.me/2022-lecture-05-slides).\\n\\n## Introduction\\n\\n![](./media/image21.png)',\n",
       "  \"Deploying models is a critical part of making your models good, to begin\\nwith. When you only evaluate the model offline, it's easy to miss the\\nmore subtle flaws that the model has, where it doesn't actually solve\\nthe problem that your users need it to solve. Oftentimes, when we deploy\\na model for the first time, only then do we really see whether that\\nmodel is actually doing a good job or not. Unfortunately, for many data\\nscientists and ML engineers, model deployment is an afterthought\\nrelative to other techniques we have covered.\\n\\nMuch like other parts of the ML lifecycle, we'll focus on deploying a\\nminimum viable model as early as possible, which entails **keeping it\\nsimple and adding complexity later**. Here is the process that this\\nlecture covers:\\n\\n-   Build a prototype\\n\\n-   Separate your model and UI\\n\\n-   Learn the tricks to scale\\n\\n-   Consider moving your model to the edge when you really need to go\\nfast\\n\\n## 1 - Build a Prototype To Interact With\",\n",
       "  'There are many great tools for building model prototypes.\\n[HuggingFace](https://huggingface.co/) has some tools\\nbuilt into its playground. They have also recently acquired a startup\\ncalled [Gradio](https://gradio.app/), which makes it easy\\nto wrap a small UI around the model.\\n[Streamlit](https://streamlit.io/) is another good option\\nwith a bit more flexibility.\\n\\n![](./media/image19.png)\\n\\n\\nHere are some best practices for prototype deployment:\\n\\n1.  **Have a basic UI**: The goal at this stage is to play around with\\nthe model and collect feedback from other folks. Gradio and\\nStreamlit are your friends here - often as easy as adding a couple\\nof lines of code to create a simple interface for the model.',\n",
       "  \"2.  **Put it behind a web URL**: An URL is easier to share. Furthermore,\\nyou will start thinking about the tradeoffs you'll be making when\\ndealing with more complex deployment schemes. There are cloud\\nversions of [Streamlit](https://streamlit.io/cloud)\\nand [HuggingFace](https://huggingface.co/) for this.\\n\\n3.  **Do not stress it too much**: You should not take more than a day\\nto build a prototype.\\n\\nA model prototype won't be your end solution to deploy. Firstly, a\\nprototype has limited frontend flexibility, so eventually, you want to\\nbe able to build a fully custom UI for the model. Secondly, a prototype\\ndoes not scale to many concurrent requests. Once you start having users,\\nyou'll hit the scaling limits quickly.\\n\\n![](./media/image18.png)\",\n",
       "  \"Above is an abstract diagram of how your application might look. The\\n**client** is your user's device that interacts with your application.\\nThis device can be a browser, a vehicle, or a mobile phone. This device\\ncalls over a network to a **server**. The server talks to a **database**\\n(where data is stored), used to power the application.\\n\\n![](./media/image6.png)\\n\\n\\nThere are different ways of structuring your application to fit an ML\\nmodel inside. The prototype approach mentioned in the beginning fits\\ninto the **model-in-service** approach - where your hosted web server\\nhas a packaged version of the model sitting inside it. This pattern has\\npros and cons.\\n\\nThe biggest pro is that if you are doing something complex, you get to\\nreuse your existing infrastructure. It does not require you as a model\\ndeveloper to set up new things from scratch.\\n\\nHowever, there is a number of pronounced cons:\",\n",
       "  \"1.  **Your web server may be written in a different language**, so\\ngetting your model into that language can be difficult.\\n\\n2.  **Models may change more frequently than server code** (especially\\nearly in the lifecycle of building your model). If you have a\\nwell-established application and a nascent model, you do not want\\nto redeploy the entire application every time that you make an\\nupdate to the model (sometimes multiple updates per day).\\n\\n3.  If you have a large model to run inference on, you'll have to load\\nthat model on your web server. **Large models can eat into the\\nresources for your web server**. That might affect the user\\nexperience for people using that web server, even if they are not\\ninteracting with the model.\\n\\n4.  **Server hardware is generally not optimized for ML workloads**. In\\nparticular, you rarely will have a GPU on these devices.\\n\\n5.  **Your model and application may have different scaling\\nproperties**, so you might want to be able to scale them\\ndifferently.\",\n",
       "  '## 2 - Separate Your Model From Your UI\\n\\n### 2.1 - Batch Prediction\\n\\n![](./media/image8.png)\\n\\n\\nThe first pattern to pull your model from your UI is called **batch\\nprediction**. You get new data in and run your model on each data point.\\nThen, you save the results of each model inference into a database. This\\ncan work well under some circumstances. For example, if there are not a\\nlot of potential inputs to the model, you can re-run your model on some\\nfrequency (every hour, every day, or every week). You can have\\nreasonably fresh predictions to return to those users that are stored in\\nyour database. Examples of these problems include the early stages of\\nbuilding recommender systems and internal-facing tools like marketing\\nautomation.',\n",
       "  \"To run models on a schedule, you can leverage the data processing and\\nworkflow tools mentioned in our previous lecture on data management. You\\nneed to re-run data processing, load the model, run predictions, and\\nstore those predictions in your database. This is exactly a **Directed\\nAcyclic Graph workflow of data operations** that tools like\\n[Dagster](https://dagster.io/),\\n[Airflow](https://airflow.apache.org/), or\\n[Prefect](https://www.prefect.io/) are designed to solve.\\nIt's worth noting that there are also tools like\\n[Metaflow](https://metaflow.org/) that are designed more\\nfor ML or data science use cases that might be potentially even an\\neasier way to get started.\\n\\nLet's visit the pros and cons of this batch prediction pattern. Starting\\nwith the pros:\\n\\n1.  Batch prediction is **simple to implement** since it reuses existing\\nbatch processing tools that you may already be using for training\\nyour model.\",\n",
       "  \"2.  It **scales very easily** because databases have been engineered for\\ndecades for such a purpose.\\n\\n3.  Even though it looks like a simple pattern, it has been **used in\\nproduction by large-scale production systems for years**. This is\\na tried-and-true pattern you can run and be confident that it'll\\nwork well.\\n\\n4.  It is **fast to retrieve the prediction** since the database is\\ndesigned for the end application to interact with.\\n\\nSwitching to the cons:\\n\\n1.  Batch prediction **doesn't scale to complex input types**. For\\ninstance, if the universe of inputs is too large to enumerate\\nevery single time you need to update your predictions, this won't\\nwork.\\n\\n2.  **Users won't be getting the most up-to-date predictions from your\\nmodel**. If the feature that goes into your model changes every\\nhour, minute, or subsecond, but you only run your batch prediction\\njob every day, the predictions your users see might be slightly\\nstale.\",\n",
       "  '3.  **Models frequently become \"stale.\"** If your batch jobs fail for\\nsome reason, it can be hard to detect these problems.\\n\\n### 2.2 - Model-as-Service\\n\\nThe second pattern is called **model-as-service**: we run the model\\nonline as its own service. The service is going to interact with the\\nbackend or the client itself by making requests to the model service and\\nreceiving responses back.\\n\\n![](./media/image16.png)\\n\\n\\nThe pros of this pattern are:\\n\\n1.  **Dependability** - model bugs are less likely to crash the web\\napplication.\\n\\n2.  **Scalability** - you can choose optimal hardware for the model and\\nscale it appropriately.\\n\\n3.  **Flexibility** - you can easily reuse a model across multiple\\napplications.\\n\\nThe cons of this pattern are:\\n\\n1.  Since this is a separate service, you add a network call when your\\nserver or client interacts with the model. That can **add\\nlatency** to your application.',\n",
       "  \"2.  It also **adds infrastructural complexity** because you are on the\\nhook for hosting and managing a separate service.\\n\\nEven with these cons, **the model-as-service pattern is still a sweet\\nspot for most ML-powered products** since you really need to be able to\\nscale independently of the application in most complex use cases. We'll\\nwalk through the basic components of building your model service -\\nincluding REST APIs, dependency management, performance optimization,\\nhorizontal scaling, rollout, and managed options.\\n\\n#### REST APIs\\n\\n**Rest APIs** serve predictions in response to canonically-formatted\\nHTTP requests. There are other alternative protocols to interact with a\\nservice that you host on your infrastructures, such as\\n[GRPC](https://grpc.io/) (used in TensorFlow Serving) and\\n[GraphQL](https://graphql.org/) (common in web development\\nbut not terribly relevant to model services).\\n\\n![](./media/image3.png)\",\n",
       "  'Unfortunately, there is currently no standard for formatting requests\\nand responses for REST API calls.\\n\\n1.  [Google Cloud](https://cloud.google.com/) expects a\\nbatch of inputs structured as a list called \"instances\" (with keys\\nand values).\\n\\n2.  [Azure](https://azure.microsoft.com/en-us/) expects a\\nlist of things called \"data\", where the data structure itself\\ndepends on what your model architecture is.\\n\\n3.  [AWS Sagemaker](https://aws.amazon.com/sagemaker/)\\nexpects instances that are formatted differently than they are in\\nGoogle Cloud.\\n\\nOur aspiration for the future is to move toward **a standard interface\\nfor making REST API calls for ML services**. Since the types of data\\nthat you might send to these services are constrained, we should be able\\nto develop a standard as an industry.\\n\\n#### Dependency Management',\n",
       "  'Model predictions depend on **code**, **model weights**, and\\n**dependencies**. In order for your model to make a correct prediction,\\nall of these dependencies need to be present on your web server.\\nUnfortunately, dependencies are a notorious cause of trouble as it is\\nhard to ensure consistency between your development environment and your\\nserver. It is also hard to update since even changing a TensorFlow\\nversion can change your model.\\n\\nAt a high level, there are two strategies for managing dependencies:\\n\\n1.  **Constrain the dependencies for your model** by saving your model\\nin an agnostic format that can be run anywhere.\\n\\n2.  **Use containers** to constrain the entire inference program.\\n\\n![](./media/image11.png)\\n\\n\\n##### Constraining Model Dependencies',\n",
       "  \"The primary way to constrain the dependencies of just your model is\\nthrough a library called [ONNX](https://onnx.ai/) - the\\nOpen Neural Network Exchange. The goal of ONNX is to be **an\\ninteroperability standard for ML models**. The promise is that you can\\ndefine a neural network in any language and run it consistently\\nanywhere. The reality is that since the underlying libraries used to\\nbuild these models change quickly, there are often bugs in the\\ntranslation layer, which creates even more problems to solve for you.\\nAdditionally, ONNX doesn't deal with non-library code such as feature\\ntransformations.\\n\\n##### Containers\",\n",
       "  'To understand how to manage dependencies with containers, we need to\\nunderstand [the differences between Docker and Virtual\\nMachines](https://medium.freecodecamp.org/a-beginner-friendly-introduction-to-containers-vms-and-docker-79a9e3e119b),\\nhow Docker images are built via Docker files and constructed via layers,\\nthe ecosystem around Docker, and specific wrappers around Docker that\\nyou can use for ML.\\n\\n![](./media/image10.png)\\n\\n\\nIn a **virtual machine**, you package up the entire operating system\\n(OS) as well as the libraries and applications that are built on top of\\nthat OS. A virtual machine tends to be very heavyweight because the OS\\nitself has a lot of code and is expensive to run. A **container** such\\nas Docker removes that need by packaging the libraries and applications\\ntogether. A Docker engine that runs on top of your OS knows how to\\nvirtualize the OS and run the libraries/applications.',\n",
       "  'By virtue of being **lightweight**, Docker is used differently than how\\nVirtual Machines were used. A common pattern is to spin up [a new\\nDocker container](https://www.docker.com/what-container)\\nfor every discrete task. For example, a web application might have four\\ncontainers: a web server, a database, a job queue, and a worker. These\\ncontainers are run together as part of an orchestration system.\\n\\n![](./media/image15.png)\\n\\n\\nDocker containers are created from [Docker\\nfiles](https://docs.docker.com/engine/reference/builder/).\\nEach Docker file runs a sequence of steps to define the environment\\nwhere you will run your code. Docker also allows you to build, store,\\nand pull Docker containers from a Docker Hub that is hosted on some\\nother servers or your cloud. You can experiment with a code environment\\nthat is on your local machine but will be identical to the environment\\nyou deploy on your server.',\n",
       "  \"Docker is separated into [three different\\ncomponents](https://docs.docker.com/engine/docker-overview):\\n\\n1.  The **client** is where you'll be running on your laptop to build an\\nimage from a Dockerfile that you define locally using some\\ncommands.\\n\\n2.  These commands are executed by a **Docker Host**, which can run on\\neither your laptop or your server (with more storage or more\\nperformance).\\n\\n3.  That Docker Host talks to a **registry** - which is where all the\\ncontainers you might want to access are stored.\\n\\n![](./media/image1.png)\\n\\n\\nWith this separation of concerns, you are not limited by the amount of\\ncompute and storage you have on your laptop to build, pull, and run\\nDocker images. You are also not limited by what you have access to on\\nyour Docker Host to decide which images to run.\",\n",
       "  \"In fact, there is a powerful ecosystem of Docker images that are\\navailable on different public Docker Hubs. You can easily find these\\nimages, modify them, and contribute them back to the Hubs. It's easy to\\nstore private images in the same place as well. Because of this\\ncommunity and the lightweight nature of Docker, it has become\\n[incredibly popular in recent\\nyears](https://www.docker.com/what-container#/package_software)\\nand is ubiquitous at this point.\\n\\nThere is a bit of a learning curve to Docker. For ML, there are a few\\nopen-source packages designed to simplify this:\\n[Cog](https://github.com/replicate/cog),\\n[BentoML](https://github.com/bentoml/BentoML), and\\n[Truss](https://github.com/trussworks). They are built by\\ndifferent model hosting providers that are designed to work well with\\ntheir model hosting service but also just package your model and all of\\nits dependencies in a standard Docker container format.\\n\\n![](./media/image12.png)\",\n",
       "  'These packages have **two primary components**: The first one is a\\nstandard way of defining your prediction service. The second one is a\\nYAML file that defines the other dependencies and package versions that\\nwill go into the Docker container running on your laptop or remotely.\\n\\nIf you want to have the advantages of using Docker for making your ML\\nmodels reproducible but do not want to go through the learning curve of\\nlearning Docker, it\\'s worth checking out these three libraries.\\n\\n#### Performance Optimization\\n\\n!!! info \"What about performance _monitoring_?\"\\n    In this section, we focus on ways to improve the performance of your\\n    models, but we spend less time on how exactly that performance is monitored,\\n    which is a challenge in its own right.',\n",
       "  \"Luckily, one of the\\n    [student projects](../project-showcase/) for the 2022 cohort,\\n    [Full Stack Stable Diffusion](../project-showcase/#full-stack-stable-diffusion),\\n    took up that challenge and combined\\n    [NVIDIA's Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server),\\n    the [Prometheus monitoring tool](https://en.wikipedia.org/wiki/Prometheus_(software)),\\n    and\\n    the [Grafana analytics dashboarding tool](https://en.wikipedia.org/wiki/Grafana)\\n    to monitor a robust, scalable, and observable deployment of Stable Diffusion models.\\n\\n    Check out the repo on GitHub\\n    [here](https://github.com/okanlv/fsdl-full-stack-stable-diffusion-2022)\\n    if you want to see a worked example of a fully-monitored DL-powered application.\\n\\nTo make model inference on your machine more efficient, we need to\\ndiscuss GPU, concurrency, model distillation, quantization, caching,\\nbatching, sharing the GPU, and libraries that automate these tasks for\\nyou.\",\n",
       "  \"##### GPU or no GPU?\\n\\nThere are some advantages to hosting your model on a GPU:\\n\\n1.  It's probably the same hardware you train your model on, to begin\\nwith. That can eliminate any lost-in-translation issues.\\n\\n2.  As your model gets big and your techniques get advanced, your\\ntraffic gets large. GPUs provide high throughput to deal with\\nthat.\\n\\nHowever, GPUs introduce a lot of complexity:\\n\\n1.  They are more complex to set up.\\n\\n2.  They are more expensive.\\n\\nAs a result, **just because your model is trained on a GPU does not mean\\nthat you need to actually host it on a GPU in order for it to work**. In\\nthe early version of your model, hosting it on a CPU should suffice. In\\nfact, it's possible to get high throughput from CPU inference at a low\\ncost by using some other techniques.\\n\\n##### Concurrency\",\n",
       "  \"With **concurrency**, multiple copies of the model run in parallel on\\ndifferent CPUs or cores on a single host machine. To do this, you need\\nto be careful about thread tuning. There's [a great Roblox\\npresentation](https://www.youtube.com/watch?v=Nw77sEAn_Js)\\non how they scaled BERT to serve a billion daily requests, just using\\nCPUs.\\n\\n##### Model Distillation\",\n",
       "  \"With **model distillation**, once you have a large model that you've\\ntrained, you can train a smaller model that imitates the behavior of\\nyour larger one. This entails taking the knowledge that your larger\\nmodel learned and compressing that knowledge into a much smaller model\\nthat you may not have trained to the same degree of performance from\\nscratch. There are several model distillation techniques pointed out in\\n[this blog\\npost](https://heartbeat.comet.ml/research-guide-model-distillation-techniques-for-deep-learning-4a100801c0eb).\\nThey can be finicky to do by yourself and are infrequently used in\\npractice. An exception is distilled versions of popular models (such as\\n[DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)).\\n\\n##### Quantization\",\n",
       "  \"With **quantization**, you execute some or potentially all of the\\noperations in your model in a lower fidelity representation of the\\nnumbers that you are doing the math. These representations can be 16-bit\\nfloating point numbers or 8-bit integers. This introduces some tradeoffs\\nwith accuracy, but it's worth making these tradeoffs because the\\naccuracy you lose is limited relative to the performance you gain.\\n\\nThe recommended path is to use built-in quantization methods in\\n[PyTorch](https://pytorch.org/blog/introduction-to-quantization-on-pytorch/)\\nand TensorFlow. More specifically, [HuggingFace\\nOptimum](https://huggingface.co/docs/optimum) is a good\\nchoice if you have already been using HuggingFace's pre-trained models.\\nYou can also run **quantization-aware training**, which often results in\\nhigher accuracy.\\n\\n![](./media/image5.png)\\n\\n\\n##### Caching\",\n",
       "  \"With **caching**, you realize that for some ML models, some inputs are\\nmore common than others. Instead of always calling the model every time\\na user makes a request, let's store the common requests in a cache.\\nThen, let's check that cache before running an expensive operation.\\nCaching techniques can get fancy, but the basic way of doing this is to\\nuse [functools library in\\nPython](https://docs.python.org/3/library/functools.html).\\n\\n![](./media/image2.png)\\n\\n\\n##### Batching\",\n",
       "  \"With **batching**, you take advantage of the fact that ML models often\\nachieve a higher throughput when doing prediction in parallel,\\nespecially in a GPU. To accomplish this, you need to gather predictions\\nuntil you have a batch, run those predictions, and return them to your\\nuser. You want to tune the batch size that deals optimally with the\\nlatency-throughput tradeoff. You also need to have a way to shortcut the\\nprocess if latency becomes too long. Batching is complicated to\\nimplement, so you probably do not want to implement this yourself.\\n\\n##### Sharing the GPU\\n\\nYour model may not take up all of the GPU memory with your inference\\nbatch size. **Why don't you run multiple models on the same GPU?** This\\nis a place where you want to use a model serving solution that supports\\nGPU sharing out of the box.\\n\\n##### Libraries\",\n",
       "  \"There are offerings from TensorFlow, PyTorch, and third-party tools from\\nNVIDIA and Anyscale. NVIDIA's choice is probably the most powerful but\\ncan be difficult to get started with. Starting with Anyscale's [Ray\\nServe](https://docs.ray.io/en/latest/serve/index.html) may\\nbe an easier way to get started.\\n\\n![](./media/image20.png)\\n\\n\\n#### Horizontal Scaling\\n\\nIf you're going to scale up to a large number of users interacting with\\nyour model, it's not going to be enough to get the most efficiency out\\nof one server. At some point, you'll need to scale horizontally to have\\ntraffic going to multiple copies of your model running on different\\nservers. This is called **horizontal scaling**. This technique involves\\ntaking traffic that would usually go to a single machine and splits\\nacross multiple machines.\",\n",
       "  \"Each machine has a copy of the service, and a tool called a load\\nbalancer distributes traffic to each machine. In practice, there are two\\nways to do this: with either **container orchestration** (e.g.\\nKubernetes) or **serverless** (e.g. AWS Lambda).\\n\\n##### Container Orchestration\\n\\nIn container orchestration, we use\\n[Kubernetes](https://kubernetes.io/) to help manage\\ncontainerized applications (in Docker containers, for example) and run\\nthem across machines.\\n\\n![](./media/image14.png)\\n\\n\\nKubernetes is quite interesting, but it's probably overkilled to learn\\ntoo much about it if your only goal is to deploy machine learning\\nmodels. There are a number of frameworks that make it easiest to deploy\\nML models with Kubernetes, including\\n[Kubeflow](https://www.kubeflow.org/),\\n[Seldon](https://www.seldon.io/), etc.\\n\\n##### Serverless\",\n",
       "  \"If Kubernetes isn't the path for you (e.g. you don't want to have to\\nworry about infrastructure at all), serverless is another option for\\ndeploying models. In this paradigm, app code and dependencies are\\npackaged into .zip files or Docker containers with a single entry point\\nfunction, which is a single function (e.g. *model.predict()*) that will\\nbe run repeatedly. This package is then deployed to a service like [AWS\\nLambda](https://aws.amazon.com/lambda/), which almost\\ntotally manages the infrastructure required to run the code based on the\\ninput. Scaling to thousands of requests and across multiple machines is\\ntaken care of by these services. In return, you pay for the compute time\\nthat you consume.\\n\\nSince model services tend to run discretely and not continuously (like a\\nweb server), serverless is a great fit for machine learning deployment.\\n\\n![](./media/image7.png)\",\n",
       "  '**Start with serverless!** It\\'s well worth the time saved in managing\\ninfrastructure and dealing with associated challenges. There are still\\nsome problems you should be aware of though.\\n\\n1.  First, the size of the actual deployment package that can be sent to\\na serverless service tends to be limited, which makes large models\\nimpossible to run.\\n\\n2.  Second, there is also a cold start problem. If there is no traffic\\nbeing sent to the service in question, the service will \"wind\\ndown\" to zero compute use, at which point it takes time to start\\nagain. This lag in starting up upon the first request to the\\nserverless service is known as the \"cold start\" time. This can\\ntake seconds or even minutes.\\n\\n3.  Third, it can be hard to actually build solid software engineering\\nconcepts, like pipelines, with serverless. Pipelines enable rapid\\niteration, while serverless offerings often do not have the tools\\nto support rapid, automated changes to code of the kind pipelines\\nare designed to do.',\n",
       "  '4.  Fourth, state management and deployment tooling are related\\nchallenges here.\\n\\n5.  Finally, most serverless functions are CPU only and have limited\\nexecution time. If you need GPUs for inference, serverless might\\nnot be for you quite yet. There are, however, new offerings like\\n[Banana](https://www.banana.dev/) and\\n[Pipeline](https://www.pipeline.ai/) that are\\nseeking to solve this problem of serverless GPU inference!\\n\\n#### Model Rollouts\\n\\nIf serving is how you turn a model into something that can respond to\\nrequests, rollouts are how you manage and update these services. To be\\nable to make updates effectively, you should be able to do the\\nfollowing:\\n\\n1.  **Roll out gradually**: You may want to incrementally send traffic\\nto a new model rather than the entirety.\\n\\n2.  **Roll back instantly**: You may want to immediately pull back a\\nmodel that is performing poorly.\\n\\n3.  **Split traffic between versions**: You may want to test differences\\nbetween models and therefore send some traffic to each.',\n",
       "  \"4.  **Deploy pipelines of models**: Finally, you may want to have entire\\npipeline flows that ensure the delivery of a model\\n\\nBuilding these capabilities in a reasonably challenging infrastructure\\nproblem that is beyond the scope of this course. In short, managed\\nservices are a good option for this that we'll now discuss!\\n\\n#### Managed Options\\n\\nAll of the major cloud providers offer their managed service options for\\nmodel deployment. There are a number of startups offering solutions as\\nwell, like BentoML or Banana.\\n\\n![](./media/image9.png)\",\n",
       "  \"The most popular managed service is [AWS\\nSagemaker](https://aws.amazon.com/sagemaker/). Working with\\nSagemaker is easier if your model is already in a common format like a\\nHuggingface class or a SciKit-Learn model. Sagemaker has convenient\\nwrappers for such scenarios. Sagemaker once had a reputation for being a\\ndifficult service to work with, but this is much less the case for the\\nclear-cut use case of model inference. Sagemaker, however, does have\\nreal drawbacks around ease of use for custom models and around cost. In\\nfact, Sagemaker instances tend to be 50-100% more expensive than EC2.\\n\\n### 2.3 - Takeaways\\n\\nTo summarize this section, remember the following:\\n\\n1.  You *probably* don't need GPU inference, which is hard to access and\\nmaintain. Scaling CPUs horizontally or using serverless can\\ncompensate.\\n\\n2.  Serverless is probably the way to go!\\n\\n3.  Sagemaker is a great way to get started for the AWS user, but it can\\nget quite expensive.\",\n",
       "  '4.  Don\\'t try to do your own GPU inference; use existing tools like\\nTFServing or Triton to save time.\\n\\n5.  Watch out for new startups focused on GPU inference.\\n\\n## 3 - Move to the Edge?\\n\\nLet\\'s now consider the case of moving models out of web service and all\\nthe way to the \"edge\", or wholly on-device. Some reasons you may need to\\nconsider this include a lack of reliable internet access for users or\\nstrict data security requirements.\\n\\nIf such hard and fast requirements aren\\'t in place, you\\'ll need to take\\ninto account the tradeoff between accuracy and latency and how this can\\naffect the end-user experience. Put simply, **if you have exhausted all\\noptions to reduce model prediction time (a component of latency),\\nconsider edge deployment**.\\n\\n![](./media/image4.png)',\n",
       "  'Edge deployment adds considerable complexity, so it should be considered\\ncarefully before being selected as an option. In edge prediction, model\\nweights are directly loaded on our client device after being sent via a\\nserver (shown above), and the model is loaded and interacted with\\ndirectly on the device.\\n\\nThis approach has compelling pros and cons:\\n\\n1.  Some pros to particularly call out are the latency advantages that\\ncome without the need for a network and the ability to scale for\\n\"free,\" or the simple fact that you don\\'t need to worry about the\\nchallenges of running a web service if all inference is done\\nlocally.\\n\\n2.  Some specific cons to call out are the often limited hardware and\\nsoftware resources available to run machine learning models on\\nedge, as well as the challenge of updating models since users\\ncontrol this process more than you do as the model author.\\n\\n### 3.1 - Frameworks',\n",
       "  \"Picking the right framework to do edge deployment depends both on how\\nyou train your model and what the target device you want to deploy it on\\nis.\\n\\n-   [TensorRT](https://developer.nvidia.com/tensorrt): If\\nyou're deploying to NVIDIA, this is the choice to go with.\\n\\n-   [MLKit](https://developers.google.com/ml-kit) and\\n[CoreML](https://developer.apple.com/documentation/coreml)**:**\\nFor phone-based deployment on either Android **or** iPhone, go\\nwith MLKit for the former and CoreML for the latter.\\n\\n-   [PyTorch Mobile](https://pytorch.org/mobile)**:** For\\ncompatibility with both iOS and Android, use PyTorch Mobile.\\n\\n-   [TFLite](https://www.tensorflow.org/lite): A great\\nchoice for using TensorFlow in a variety of settings, not just on\\na phone or a common device.\\n\\n-   [TensorFlow JS](https://www.tensorflow.org/js)**:**\\nThe preferred framework for deploying machine learning in the\\nbrowser.\",\n",
       "  '-   [Apache TVM](https://tvm.apache.org/): A library\\nagnostic, target device agnostic option. This is the choice for\\nanyone trying to deploy to as diverse a number of settings as\\npossible.\\n\\nKeep paying attention to this space! There are a lot of startups like\\n[MLIR](https://mlir.llvm.org/),\\n[OctoML](https://octoml.ai/),\\n[TinyML](https://www.tinyml.org/), and\\n[Modular](https://www.modular.com/) that are aiming to\\nsolve some of these problems.\\n\\n### 3.2 - Efficiency',\n",
       "  'No software can help run edge-deployed models that are simply too large;\\n**model efficiency** is important for edge deployment! We previously\\ndiscussed quantization and distillation as options for model efficiency.\\nHowever, there are also network architectures specifically designed to\\nwork better in edge settings like\\n[MobileNets](https://medium.com/@yu4u/why-mobilenet-and-its-variants-e-g-shufflenet-are-fast-1c7048b9618d).\\nMobileNets replace the more expensive computations typical of server-run\\nmodels with simpler computations and achieve acceptable performance\\noftentimes.\\n\\n![](./media/image17.png)\\n\\n\\nMobileNets are a great tool for model deployments and are a great case\\nstudy in model efficiency. Another similarly great case study is\\n[DistillBERT](https://medium.com/huggingface/distilbert-8cf3380435b5).\\n\\n![](./media/image13.png)\\n\\n### 3.3 - Mindsets\\n\\nAs we wrap up this lecture, keep in mind the following mindsets as you\\nconsider edge deployment:',\n",
       "  \"1.  **Start with the edge requirement, not the architecture choice**.\\nIt's easy to pick a high-performing model architecture, only to\\nthen find it impossible to run on the edge device. Avoid this\\nscenario at all costs! Tricks like quantization can account for up\\nto 10x improvement, but not much more.\\n\\n2.  **Once you have a model that works on the edge, you can iterate\\nlocally without too much additional re-deployment.** In this case,\\nmake sure to add metrics around the model size and edge\\nperformance to your experiment tracking.\\n\\n3.  **Treat tuning the model as an additional risk and test\\naccordingly.** With the immaturity of edge deployment frameworks,\\nit's crucial to be especially careful when testing your model on\\nthe exact hardware you'll be deploying on.\\n\\n4.  **Make sure to have fallbacks!** Models are finicky and prone to\\nunpredictable behavior. In edge cases, it's especially important\\nto have easily available fallback options for models that aren't\\nworking.\\n\\n### 3.4 - Conclusion\",\n",
       "  'To summarize this section:\\n\\n1.  Web deployment is easier, so use edge deployment only if you need\\nto.\\n\\n2.  Choose your framework to match the available hardware and\\ncorresponding mobile frameworks, or try Apache TVM to be more\\nflexible.\\n\\n3.  Start considering hardware constraints at the beginning of the\\nproject and choose architectures accordingly.'],\n",
       " 'https://fullstackdeeplearning.com/course/2022/lecture-4-data-management': ['---\\ndescription: Building ML-powered products and the teams who create them\\n---\\n\\n# Lecture 8: ML Teams and Project Management\\n\\n<div align=\"center\">\\n<iframe width=\"720\" height=\"405\" src=\"https://www.youtube-nocookie.com/embed/a54xH6nT4Sw?list=PL1T8fO7ArWleMMI8KPJ_5D5XSlovTW_Ur\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\\n</div>\\n\\nLecture by [Josh Tobin](https://twitter.com/josh_tobin_).\\nNotes by [James Le](https://twitter.com/le_james94) and [Vishnu Rachakonda](https://www.linkedin.com/in/vrachakonda/).<br />\\nPublished September 26, 2022.\\n[Download slides](https://fsdl.me/2022-lecture-08-slides).\\n\\n## 0 - Why is this hard?\\n\\nBuilding any product is hard:\\n\\n-   You have to hire great people.\\n\\n-   You have to manage and develop those people.\\n\\n-   You have to manage your team\\'s output and make sure your vectors are\\naligned.',\n",
       "  '-   You have to make good long-term technical choices and manage\\ntechnical debt.\\n\\n-   You have to manage expectations from leadership.\\n\\n-   You have to define and communicate requirements with stakeholders.\\n\\nMachine Learning (ML) adds complexity to that process:\\n\\n-   ML talent is expensive and scarce.\\n\\n-   ML teams have a diverse set of roles.\\n\\n-   Projects have unclear timelines and high uncertainty.\\n\\n-   The field is moving fast, and ML is the \"[high-interest credit card\\nof technical\\ndebt](https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf).\"\\n\\n-   Leadership often doesn\\'t understand ML.\\n\\n-   ML products fail in ways that are hard for laypeople to understand.\\n\\nIn this lecture, we\\'ll talk about:\\n\\n1.  ML-related **roles** and their required skills.\\n\\n2.  How to **hire** ML engineers (and how to get hired).\\n\\n3.  How ML teams are **organized** and fit into the broader\\norganization.\\n\\n4.  How to **manage** an ML team and ML products.\\n\\n5.  **Design** considerations for ML products.',\n",
       "  \"## 1 - Roles\\n\\n### Common Roles\\n\\nLet's look at the most common ML roles and the skills they require:\\n\\n-   The **ML Product Manager** works with the ML team, other business\\nfunctions, the end-users, and the data owners. This person designs\\ndocs, creates wireframes, and develops a plan to prioritize and\\nexecute ML projects.\\n\\n-   The **MLOps/ML Platform Engineer** builds the infrastructure to make\\nmodels easier and more scalable to deploy. This person handles the\\nML infrastructure that runs the deployed ML product using\\nplatforms like AWS, GCP, Kafka, and other ML tooling vendors.\\n\\n-   The **ML Engineer** trains and deploys prediction models. This\\nperson uses tools like TensorFlow and Docker to work with\\nprediction systems running on real data in production.\",\n",
       "  '-   The **ML Researcher** trains prediction models, often those that are\\nforward-looking or not production-critical. This person uses\\nlibraries like TensorFlow and PyTorch on notebook environments to\\nbuild models and reports describing their experiments.\\n\\n-   The **Data Scientist** is a blanket term used to describe all of the\\nroles above. In some organizations, this role entails answering\\nbusiness questions via analytics. This person can work with\\nwide-ranging tools from SQL and Excel to Pandas and Scikit-Learn.\\n\\n![](./media/image9.png)\\n\\n### Skills Required\\n\\nWhat skills are needed for these roles? The chart below displays a nice\\nvisual - where the horizontal axis is the level of ML expertise and the\\nsize of the bubble is the level of communication and technical writing\\n(the bigger, the better).\\n\\n![](./media/image4.png)\\n\\n-   The **MLOps** is primarily a software engineering role, which often\\ncomes from a standard software engineering pipeline.',\n",
       "  '-   The **ML Engineer** requires a rare mix of ML and Software\\nEngineering skills. This person is either an engineer with\\nsignificant self-teaching OR a science/engineering Ph.D. who works\\nas a traditional software engineer after graduate school.\\n\\n-   The **ML Researcher** is an ML expert who usually has an MS or Ph.D.\\ndegree in Computer Science or Statistics or finishes an industrial\\nfellowship program.\\n\\n-   The **ML Product Manager** is just like a traditional Product\\nManager but with a deep knowledge of the ML development process\\nand mindset.\\n\\n-   The **Data Scientist** role constitutes a wide range of backgrounds,\\nfrom undergraduate to Ph.D. students.\\n\\nThere is an important distinction between a task ML engineer and a\\nplatform ML engineer, coined by Shreya Shankar in [this blog\\npost](https://www.shreya-shankar.com/phd-year-one/):',\n",
       "  \"1.  **Task ML engineers** are responsible for maintaining specific ML\\npipelines. They only focus on ensuring that these ML models are\\nhealthy and updated frequently. They are often overburdened.\\n\\n2.  **Platform ML engineers** help task ML engineers automate tedious\\nparts of their jobs. They are called MLOps/ML Platform engineers\\nin our parlance.\\n\\n## 2 - Hiring\\n\\n### The AI Talent Gap\\n\\nIn 2018 (when we started FSDL), the AI talent gap was the main story.\\nThere were so few people who understood this technology, so the biggest\\nblock for organizations was that they couldn't find people who were good\\nat ML.\\n\\nIn 2022, the AI talent gap persists. But it tends to be less of a\\nblocker than it used to be because we have had four years of folks\\nswitching careers into ML and software engineers emerging from\\nundergraduate with at least a couple of ML classes under their belts.\",\n",
       "  \"The gap tends to be in folks that understand more than just the\\nunderlying technology but also have experience in seeing how ML fails\\nand how to make ML successful when it's deployed. That's the reality of\\nhow difficult it is to hire ML folks today, especially those with\\n**production experience**.\\n\\n### Sourcing\\n\\nBecause of this shallow talent pool and the skyrocketing demand, hiring\\nfor ML positions is pretty hard. Typical ML roles come in the following\\nstructure:\\n\\n-   ML Adjacent roles: ML product manager, DevOps, Data Engineer\\n\\n-   Core ML Roles: ML Engineer, ML Research/ML Scientist\\n\\n-   Business analytics roles: Data Scientist\\n\\nFor ML-adjacent roles, traditional ML knowledge is less important, as\\ndemonstrated interest, conversational understanding, and experience can\\nhelp these professionals play an impactful role on ML teams. Let's focus\\non how to hire for **the core ML roles**.\\n\\n![](./media/image6.png)\",\n",
       "  \"While there's no perfect way to **hire ML engineers**, there's\\ndefinitely a wrong way to hire them, with extensive job descriptions\\nthat demand only the best qualifications (seen above). Certainly, there\\nare many good examples of this bad practice floating around.\\n\\n-   Rather than this unrealistic process, consider hiring for software\\nengineering skills, an interest in ML, and a desire to learn. You\\ncan always train people in the art and science of ML, especially\\nwhen they come with strong software engineering fundamentals.\\n\\n-   Another option is to consider adding junior talent, as many recent\\ngrads come out with good ML knowledge nowadays.\\n\\n-   Finally, and most importantly, be more specific about what you need\\nthe position and professional to do. It's impossible to find one\\nperson that can do everything from full-fledged DevOps to\\nalgorithm development.\\n\\nTo **hire ML researchers**, here are our tips:\",\n",
       "  '-   Evaluate the quality of publications, over the quantity, with an eye\\ntoward the originality of the ideas, the execution, etc.\\n\\n-   Prioritize researchers that focus on important problems instead of\\ntrendy problems.\\n\\n-   Experience outside academia is also a positive, as these researchers\\nmay be able to transition to industry more effectively.\\n\\n-   Finally, keep an open mind about research talent and consider\\ntalented people without PhDs or from adjacent fields like physics,\\nstatistics, etc.\\n\\nTo find quality candidates for these roles, here are some ideas for\\nsourcing:\\n\\n-   Use standard sources like LinkedIn, recruiters, on-campus\\nrecruiting, etc.\\n\\n-   Monitor arXiv and top conferences and flag the first authors of\\npapers you like.\\n\\n-   Look for good implementations of papers you like.\\n\\n-   Attend ML research conferences (NeurIPS, ICML, ICLR).\\n\\n![](./media/image7.png)',\n",
       "  \"As you seek to recruit, stay on top of what professionals want and make\\nan effort to position your company accordingly. ML practitioners want to\\nbe empowered to do great work with interesting data. Building a culture\\nof learning and impact can help recruit the best talent to your team.\\nAdditionally, sell sell sell! Talent needs to know how good your team is\\nand how meaningful the mission can be.\\n\\n### Interviewing\\n\\nAs you interview candidates for ML roles, try to **validate your\\nhypotheses of their strengths while testing a minimum bar on weaker\\naspects**. For example, ensure ML researchers can think creatively about\\nnew ML problems while ensuring they meet a baseline for code quality.\\nIt's essential to test ML knowledge and software engineering skills for\\nall industry professionals, though the relative strengths can vary.\",\n",
       "  'The actual ML interview process is much less well-defined than software\\nengineering interviews, though it is modeled off of it. Some helpful\\ninclusions are projects or exercises that test the ability to work with\\nML-specific code, like take-home ML projects. Chip Huyen\\'s\\n\"[Introduction to ML Interviews\\nBook](https://huyenchip.com/ml-interviews-book/)\" is a\\ngreat resource.\\n\\n### Finding A Job\\n\\nTo find an ML job, you can take a look at the following sources:\\n\\n-   Standard sources such as LinkedIn, recruiters, on-campus recruiting,\\netc.\\n\\n-   ML research conferences (NeurIPS, ICLR, ICML).\\n\\n-   Apply directly (remember, there\\'s a talent gap!).\\n\\nStanding out for competitive roles can be tricky! Here are some tips (in\\nincreasing order of impressiveness) that you can apply to differentiate\\nyourself:\\n\\n1.  Exhibit ML interest (e.g., conference attendance, online course\\ncertificates, etc.).\\n\\n2.  Build software engineering skills (e.g., at a well-known software\\ncompany).',\n",
       "  \"3.  Show you have a broad knowledge of ML (e.g., write blog posts\\nsynthesizing a research area).\\n\\n4.  Demonstrate ability to get ML projects done (e.g., create side\\nprojects, re-implement papers).\\n\\n5.  Prove you can think creatively in ML (e.g., win Kaggle competitions,\\npublish papers).\\n\\n## 3 - Organizations\\n\\n### Organization Archetypes\\n\\nThere exists not yet a consensus on the right way to structure an ML\\nteam. Still, a few best practices are contingent upon different\\norganization archetypes and their ML maturity level. First, let's see\\nwhat the different ML organization archetypes are.\\n\\n**Archetype 1 - Nascent and Ad-Hoc ML**\\n\\n-   These are organizations where no one is doing ML, or ML is done on\\nan ad-hoc basis. Obviously, there is little ML expertise in-house.\\n\\n-   They are either small-to-medium businesses or less\\ntechnology-forward large companies in industries like education or\\nlogistics.\\n\\n-   There is often low-hanging fruit for ML.\",\n",
       "  \"-   But there is little support for ML projects, and it's challenging to\\nhire and retain good talent.\\n\\n**Archetype 2 - ML R&D**\\n\\n-   These are organizations in which ML efforts are centered in the R&D\\narm of the organization. They often hire ML researchers and\\ndoctorate students with experience publishing papers.\\n\\n-   They are larger companies in sectors such as oil and gas,\\nmanufacturing, or telecommunications.\\n\\n-   They can hire experienced researchers and work on long-term business\\npriorities to get big wins.\\n\\n-   However, it is very difficult to get quality data. Most often, this\\ntype of research work rarely translates into actual business\\nvalue, so usually, the amount of investment remains small.\\n\\n**Archetype 3 - ML Embedded Into Business and Product Teams**\\n\\n-   These are organizations where certain product teams or business\\nunits have ML expertise alongside their software or analytics\\ntalent. These ML individuals report up to the team's\\nengineering/tech lead.\",\n",
       "  '-   They are either software companies or financial services companies.\\n\\n-   ML improvements are likely to lead to business value. Furthermore,\\nthere is a tight feedback cycle between idea iteration and product\\nimprovement.\\n\\n-   Unfortunately, it is still very hard to hire and develop top talent,\\nand access to data and compute resources can lag. There are also\\npotential conflicts between ML project cycles and engineering\\nmanagement, so long-term ML projects can be hard to justify.\\n\\n**Archetype 4 - Independent ML Function**\\n\\n-   These are organizations in which the ML division reports directly to\\nsenior leadership. The ML Product Managers work with Researchers\\nand Engineers to build ML into client-facing products. They can\\nsometimes publish long-term research.\\n\\n-   They are often large financial services companies.',\n",
       "  '-   Talent density allows them to hire and train top practitioners.\\nSenior leaders can marshal data and compute resources. This gives\\nthe organizations to invest in tooling, practices, and culture\\naround ML development.\\n\\n-   A disadvantage is that model handoffs to different business lines\\ncan be challenging since users need the buy-in to ML benefits and\\nget educated on the model use. Also, feedback cycles can be slow.\\n\\n**Archetype 5 - ML-First Organizations**\\n\\n-   These are organizations in which the CEO invests in ML, and there\\nare experts across the business focusing on quick wins. The ML\\ndivision works on challenging and long-term projects.\\n\\n-   They are large tech companies and ML-focused startups.\\n\\n-   They have the best data access (data thinking permeates the\\norganization), the most attractive recruiting funnel (challenging\\nML problems tends to attract top talent), and the easiest\\ndeployment procedure (product teams understand ML well enough).',\n",
       "  '-   This type of organization archetype is hard to implement in practice\\nsince it is culturally difficult to embed ML thinking everywhere.\\n\\n### Team Structure Design Choices\\n\\nDepending on the above archetype that your organization resembles, you\\ncan make the appropriate design choices, which broadly speaking follow\\nthese three categories:\\n\\n1.  **Software Engineer vs. Research**: To what extent is the ML team\\nresponsible for building or integrating with software? How\\nimportant are Software Engineering skills on the team?\\n\\n2.  **Data Ownership**: How much control does the ML team have over data\\ncollection, warehousing, labeling, and pipelining?\\n\\n3.  **Model Ownership**: Is the ML team responsible for deploying models\\ninto production? Who maintains the deployed models?\\n\\nBelow are our design suggestions:\\n\\nIf your organization focuses on **ML R&D**:',\n",
       "  '-   Research is most definitely prioritized over Software Engineering\\nskills. Because of this, there would potentially be a lack of\\ncollaboration between these two groups.\\n\\n-   ML team has no control over the data and typically will not have\\ndata engineers to support them.\\n\\n-   ML models are rarely deployed into production.\\n\\nIf your organization has **ML embedded into the product**:\\n\\n-   Software Engineering skills will be prioritized over Research\\nskills. Often, the researchers would need strong engineering\\nskills since everyone would be expected to product-ionize his/her\\nmodels.\\n\\n-   ML teams generally do not own data production and data management.\\nThey will need to work with data engineers to build data\\npipelines.\\n\\n-   ML engineers totally own the models that they deploy into\\nproduction.\\n\\nIf your organization has **an independent ML division**:\\n\\n-   Each team has a potent mix of engineering and research skills;\\ntherefore, they work closely together within teams.',\n",
       "  '-   ML team has a voice in data governance discussions, as well as a\\nrobust data engineering function.\\n\\n-   ML team hands-off models to users but is still responsible for\\nmaintaining them.\\n\\nIf your organization is **ML-First**:\\n\\n-   Different teams are more or less research-oriented, but in general,\\nresearch teams collaborate closely with engineering teams.\\n\\n-   ML team often owns the company-wide data infrastructure.\\n\\n-   ML team hands the models to users, who are responsible for operating\\nand maintaining them.\\n\\nThe picture below neatly sums up these suggestions:\\n\\n![](./media/image12.png)\\n\\n## 4 - Managing\\n\\n### Managing ML Teams Is Challenging\\n\\nThe process of actually managing an ML team is quite challenging for\\nfour reasons:',\n",
       "  \"1.  **Engineering Estimation:** It's hard to know how easy or hard an ML\\nproject is in advance. As you explore the data and experiment with\\ndifferent models, there is enormous scope for new learnings about\\nthe problem that materially impact the timeline. Furthermore,\\nknowing what methods will work is often impossible. This makes it\\nhard to say upfront how long or how much work may go into an ML\\nproject.\\n\\n2.  **Nonlinear Progress:** As the chart below from a [blog\\npost](https://medium.com/@l2k/why-are-machine-learning-projects-so-hard-to-manage-8e9b9cf49641)\\nby Lukas Biewald (CEO of [Weights and\\nBiases](https://wandb.ai/site)) shows, progress on ML\\nprojects is unpredictable over time, even when the effort expended\\ngrows considerably. It's very common for projects to stall for\\nextended periods of time.\\n\\n![](./media/image1.png)\",\n",
       "  \"3.  **Cultural gaps:** The relative culture of engineering and research\\nprofessionals is very different. Research tends to favor novel,\\ncreative ideas, while engineering prefers tried and true methods\\nthat work. As a result, ML teams often experience a clash of\\ncultures, which can turn toxic if not appropriately managed. A\\ncore challenge of running ML teams is addressing the cultural\\nbarriers between ML and software engineering so that teams can\\nharmoniously experiment and deliver ML products.\\n\\n4.  **Leadership Deficits**: It's common to see a lack of detailed\\nunderstanding of ML at senior levels of management in many\\ncompanies. As a result, expressing feasibility and setting the\\nright expectations for ML projects, especially high-priority ones,\\ncan be hard.\\n\\n### How To Manage ML Teams Better\\n\\nManaging ML teams is hardly a solved problem, but you can take steps to\\nimprove the process.\\n\\n**Plan probabilistically**\",\n",
       "  \"Many engineering projects are managed in a waterfall fashion, with the\\nsequential tasks defined up front clearly. Instead of forcing this\\nmethod of engineering management on difficult ML projects, try assigning\\na likelihood of success to different tasks to better capture the\\nexperimental process inherent to ML engineering. As these tasks progress\\nor stall, rapidly re-evaluate your task ordering to better match what is\\nworking. Having this sense of both (1) **how likely a task is to\\nsucceed** and (2) **how important it is** makes project planning\\nconsiderably more realistic.\\n\\n![](./media/image10.png)\\n\\n\\n**Have a portfolio of approaches**\\n\\nEmbrace multiple ideas and approaches to solve crucial research\\nchallenges that gate production ML. Don't make your plan dependent on\\none approach working!\\n\\n**Measure inputs, not results**\",\n",
       "  \"As you work through several approaches in your portfolio, do not overly\\nemphasize whose ideas ultimately work as a reflection of contribution\\nquality. This can negatively impact team members' creativity, as they\\nfocus more on trying to find only what they currently think could work,\\nrather than experimenting in a high-quality fashion (which is ultimately\\nwhat leads to ML success).\\n\\n**Have researchers and engineers work together**\\n\\nThe collaboration between engineering and research is essential for\\nquality ML products to get into production. Emphasize collaboration\\nacross the groups and professionals!\\n\\n**Get quick wins**\\n\\nTaking this approach makes it more likely that your ML project will\\nsucceed in the long term. It allows you to demonstrate progress to your\\nleadership more effectively and clearly.\\n\\n**Educate leadership on uncertainty**\",\n",
       "  'This can be hard, as leadership is ultimately accountable for addressing\\nblind spots and understanding timeline risk. There are things you can\\ndo, however, to help improve leadership\\'s knowledge about ML timelines.\\n\\n-   Avoid building hype around narrow progress metrics material only to\\nthe ML team (e.g., \"*We improved F1 score by 0.2 and have achieved\\nawesome performance!*\").\\n\\n-   Instead, be realistic, communicate risk, and emphasize real product\\nimpact (e.g., \"Our model improvements should increase the number\\nof conversions by 10%, though we must continue to validate its\\nperformance on additional demographic factors.)\\n\\n-   Sharing resources like [this a16z primer](https://a16z.com/2016/06/10/ai-deep-learning-machines/),\\n[this class from Prof. Pieter\\nAbbeel](https://executive.berkeley.edu/programs/artificial-intelligence),\\nand [this Google\\'s People + AI\\nguidebook](https://pair.withgoogle.com/guidebook) can\\nincrease awareness of your company\\'s leadership.',\n",
       "  '### ML PMs are well-positioned to educate the organization\\n\\nThere are two types of ML product managers.\\n\\n1.  **Task PMs**: These are the more common form of ML PM. They are\\ngenerally specialized into a specific product area (e.g. trust and\\nsafety) and have a strong understanding of the particular use\\ncase.\\n\\n2.  **Platform PMs**: These are a newer form of PMs. They have a broader\\nmandate to ensure that the ML team (generally centralized in this\\ncontext) is highest leverage. They manage workflow and priorities\\nfor this centralized team. To support this, they tend to have a\\nbroad understanding of ML themselves. These PMs are critical for\\neducating the rest of the company about ML and ensuring that teams\\ntrust the output of models.\\n\\nBoth types of PMs are crucial for ML success. Platform PMs tend to have\\na particularly powerful role to play in pushing an organization\\'s\\nadoption of machine learning and making it successful.\\n\\n### What is \"Agile\" for ML?',\n",
       "  'There are two options similar to what Agile is for software development\\nin the ML context. They are shown below:\\n\\n![](./media/image2.png)\\n\\n\\nThey are both structured, data-science native approaches to project\\nmanagement. You can use them to provide standardization for project\\nstages, roles, and artifacts.\\n\\n**TDSP** tends to be more structured and is a strong alternative to the\\nAgile methodology. **CRISP-DM** is somewhat higher level and does not\\nprovide as structured a project management workflow. If you genuinely\\nhave a large-scale coordination problem, you can try these frameworks,\\nbut don\\'t otherwise. They can slow you down since they are more oriented\\naround \"traditional\" data science and not machine learning.\\n\\n## 5 - Design\\n\\nLet\\'s talk about how to actually design machine learning products now.\\nThe biggest challenge with designing such products often isn\\'t\\nimplementing them; it\\'s **bridging the gap between users\\' inflated\\nexpectations and the reality**.',\n",
       "  'Users often expect extremely sophisticated systems capable of solving\\nmany more problems than they actually can.\\n\\n![](./media/image11.png)\\n\\nIn reality, machine learning systems are more like dogs that are trained\\nto do a special task; weird little guys with a penchant for distraction\\nand an inability to do much more than they are explicitly told.\\n\\n![](./media/image13.png)\\n\\nAll this leads to a big gap between what can be done and what users\\nexpect!\\n\\n### The Keys to Good ML Product Design\\n\\nIn practice, **good ML product design bridges users expectations and\\nreality**. If you can help users understand the benefits and limitations\\nof the model, they tend to be more satisfied. Furthermore, always have\\nbackup plans for model failures! Over-automating systems tends to be a\\nrecipe for unhappy users. Finally, building in feedback loops can really\\nincrease satisfaction over time.\\n\\nThere are a couple ways to **explain the benefits and limitations** of\\nan ML system to users.',\n",
       "  '-   Focus on the problems it solves, not the fact that the system is\\n\"AI-powered\".\\n\\n-   If you make the system feel \"human-like\" (unconstrained input,\\nhuman-like responses), expect users to treat it as human-like.\\n\\n-   Furthermore, seek to include guardrails or prescriptive interfaces\\nover open-ended, human-like experiences. A good example of the\\nformer approach is [Amazon\\nAlexa](https://alexa.amazon.com/), which has specific\\nprompts that its ML system responds to.\\n\\n![](./media/image5.png)',\n",
       "  '**Handling failures** is a key part of keeping ML systems users happy.\\nThere\\'s nothing worse than a \"smart\" system that conks out when you do\\nsomething slightly unexpected. Having built-in solutions to solve for\\nautomation issues is extremely important. One approach is letting users\\nbe involved to correct improper responses. Another is to focus on the\\nnotion of \"model confidence\" and only offer responses when the threshold\\nis met. A good example of a handling failure approach is how Facebook\\nrecommends photo tags for users, but doesn\\'t go so far as to autoassign.\\n\\n### Types of User Feedback\\n\\nHow can you collect feedback from users in a way that avoids these\\nissues? There are different types of user feedback and how they help\\nwith model improvement.\\n\\n![](./media/image3.png)\\n\\n\\nLet\\'s go across this chart.',\n",
       "  '1.  The simplest form of feedback is **indirect implicit feedback**. For\\nexample, did the user churn from the product? That tells you\\nimmediately how the user felt about the system without them giving\\na clear signal themselves.\\n\\n2.  Another form is **direct implicit feedback**, which involves the\\nuser \"taking the next step\". For example, in an automated user\\nonboarding flow, did the user click through into ensuing steps?\\nThis is trickier to implement, but can be useful for future\\ntraining iterations.\\n\\n3.  The next type of feedback is **binary explicit feedback**, wherein\\nusers are specifically asked (e.g. via thumbs up/down buttons) how\\nthey feel about the model performance.\\n\\n4.  You can make this more sophisticated and add **categorical explicit\\nfeedback**, which allows users to sort their feedback into various\\ntypes.',\n",
       "  \"5.  To really get a sense of how users feel, consider offering **free\\ntext feedback**. This is tricky to use for model training and can\\nbe involved for users, but it's very useful to highlight the\\nhighest friction predictions.\\n\\n6.  The gold standard, of course, are **model corrections**; they are\\nfree labels!\\n\\nWhenever building explicit feedback into ML systems, avoid relying on\\nusers' altruism and be clear about why they should engage in the\\nfeedback. Instead, build positive feedback loops by allowing users to\\nexperience the benefits of their feedback quickly.\\n\\n**Great ML product experiences are designed from scratch**. ML is a very\\nspecific technology with clear advantages and drawbacks. Design needs to\\nbe thoughtfully executed around these products. It's especially\\nimportant to allow users to interact safely with ML products that may\\nfail in unexpected ways. Always try to find ways to build in feedback\\nloops to make the ML product better over time.\",\n",
       "  \"There are tons of resources that can help you get started with this\\nemerging field.\\n\\n-   [Google's People + AI\\nGuidebook](https://pair.withgoogle.com/guidebook)\\n\\n-   [Guidelines for Human-AI\\nInteraction](https://dl.acm.org/doi/abs/10.1145/3290605.3300233)\\n\\n-   [Agency Plus Automation: Designing AI into Interactive\\nSystems](http://idl.cs.washington.edu/files/2019-AgencyPlusAutomation-PNAS.pdf)\\n\\n-   [Designing Collaborative\\nAI](https://medium.com/@Ben_Reinhardt/designing-collaborative-ai-5c1e8dbc8810)\\n\\nIn conclusion, we talked through a number of adjacent considerations to\\nbuilding ML systems and products. In short, you ship the team as much\\nyou do the code; be thoughtful about how you hire, manage, and structure\\nML teams as much as ML products!\\n\\n![](./media/image8.png)\"],\n",
       " 'https://fullstackdeeplearning.com/course/2022/lecture-5-deployment': ['---\\ndescription: Building ML for good while building good ML\\n---\\n\\n# Lecture 9: Ethics\\n\\n<div align=\"center\">\\n<iframe width=\"720\" height=\"405\" src=\"https://www.youtube-nocookie.com/embed/7FQpbYTqjAA?list=PL1T8fO7ArWleMMI8KPJ_5D5XSlovTW_Ur\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\\n</div>\\n\\nLecture by [Charles Frye](https://twitter.com/charles_irl).\\nNotes by [James Le](https://twitter.com/le_james94) and [Vishnu Rachakonda](https://www.linkedin.com/in/vrachakonda/).<br />\\nPublished October 03, 2022.\\n[Download slides](https://fsdl.me/2022-lecture-09-slides).\\n\\nIn this final lecture of FSDL 2022, we\\'ll talk about ethics. After going\\nthrough the context of what we mean by ethics, we\\'ll go through three\\ndifferent areas where ethics come up:\\n\\n1.  **Tech Ethics**: ethics that anybody who works in the tech industry\\nbroadly needs to think about.',\n",
       "  \"2.  **ML Ethics**: what ethics has specifically meant for the ML\\nindustry.\\n\\n3.  **AI Ethics**: what ethics might mean in the future where true AGI\\nexists.\\n\\n## 1 - Overview and Context\\n\\nAll ethics lectures are wrong, but some are useful. They are more useful\\nif we admit and state what our assumptions or biases are. We'll also\\ntalk about three general themes that come up often when ethical concerns\\nare raised in tech/ML: alignment, trade-offs, and humility.\\n\\n![](./media/image17.png)\",\n",
       "  'In this lecture, we\\'ll approach ethics on the basis of **concrete\\ncases** - specific instances where people have raised concerns. We\\'ll\\ntalk about **cases where people have taken actions that have led to\\nclaims and counter-claims of ethical or unethical behavior** - such as\\nthe use of automated weapons, the use of ML systems to make decisions\\nlike sentencing and bail, and the use of ML algorithms to generate art.\\nIn each case when criticism has been raised, part of that criticism has\\nbeen that the technology is unethical.\\n\\nApproaching ethics in this way allows us to answer the question of \"What\\nis ethics?\" by way of Ludwig Wittgenstein\\'s quote: \"*The meaning of a\\nword is its use in the language*.\" We\\'ll focus on times when people have\\nused the word \"ethics\" to describe what they like or dislike about a\\nspecific technology.',\n",
       "  'If you want to try it out for yourself, you should check out the game\\n\"[Something Something Soup\\nSomething](https://soup.gua-le-ni.com/).\" In this browser\\ngame, you are presented with a bunch of dishes and have to decide\\nwhether they are soup or not soup, as well as whether they can be served\\nto somebody who ordered soup. By playing a game like this, you\\'ll\\ndiscover (1) how difficult it is to come up with a concrete definition\\nof soup and (2) how poorly your working definition of soup fits with any\\ngiven soup theory.\\n\\nBecause of this case-based approach, we won\\'t be talking about ethical\\nschools or \"trolley\" problems. Rather than considering [these\\nhypothetical\\nscenarios](https://www.currentaffairs.org/2017/11/the-trolley-problem-will-tell-you-nothing-useful-about-morality),\\nwe\\'ll talk about concrete and specific examples from the past decade of\\nwork in our field and adjacent fields.\\n\\n![](./media/image19.png)',\n",
       "  'If you want another point of view that emphasizes the trolley problems,\\nyou should check out [Sergey\\'s lecture from the last edition of the\\ncourse from\\n2021](https://fullstackdeeplearning.com/spring2021/lecture-9/).\\nIt presented similar ideas from a different perspective and came to the\\nsame conclusion and some different conclusions.\\n\\nA useful theme from that lecture that we should all have in mind when we\\nponder ethical dilemmas is \"What Is Water?\" - which came up from [a\\nfamous commencement speech by David Foster\\nWallace](https://www.youtube.com/watch?v=PhhC_N6Bm_s). If\\nwe aren\\'t thoughtful and paying attention, things that are very\\nimportant can become background, assumptions, and invisible to us.',\n",
       "  \"The approach of **relying on prominent cases risks replicating social\\nbiases**. Some ethical claims are amplified and travel more because\\npeople (who are involved) have more resources and are better connected.\\nUsing these forms of case-based reasoning (where you explain your\\nbeliefs in concrete detail) can **hide the principles that are actually\\nin operation**, making them disappear like water.\\n\\nBut in the end, **so much of ethics is deeply personal** that we can't\\nexpect to have a perfect approach. We can just do the best we can and\\nhopefully become better every day.\\n\\n## 2 - Themes\\n\\nWe'll see three themes repeatedly coming up throughout this lecture:\\n\\n1.  **Alignment**: a conflict between what we want and what we get.\\n\\n2.  **Trade-Offs**: a conflict between what we want and what others\\nwant.\\n\\n3.  **Humility**: a response when we don't know what we want or how to\\nget it.\\n\\n### Alignment\",\n",
       "  'The problem of **alignment** (where what we want and what we get differ)\\ncome up over and over again. A primary driver of this is called the\\n**proxy problem** - in which we often optimize or maximize some proxies\\nfor the thing that we really care about. If the alignment (or loosely\\nthe correlation between that proxy and the thing we care about) is poor\\nenough, then by trying to maximize that proxy, we can end up hurting the\\nthing we originally cared about.\\n\\n![](./media/image16.png)\\n\\nThere was [a recent\\npaper](https://arxiv.org/abs/2102.03896) that did a\\nmathematical analysis of this idea. You can see these kinds of proxy\\nproblems everywhere once you look for them.',\n",
       "  \"-   On the top right, we have a train and validation loss chart from one\\nof the training runs for the FSDL text recognizer. The thing we\\ncan optimize is the training loss. That's what we can use to\\ncalculate gradients and improve the parameters of our network. But\\nthe thing we really care about is the performance of the network\\non data points that it has not seen (like the validation set, the\\ntest set, or data in production). If we optimize our training loss\\ntoo much, we can actually cause our validation loss to go up.\\n\\n-   Similarly, there was [an interesting\\npaper](https://openreview.net/forum?id=qrGKGZZvH0)\\nsuggesting that increasing your accuracy on classification tasks\\ncan actually result in a decrease in the utility of your\\nembeddings in downstream tasks.\",\n",
       "  \"-   You can find these proxy problems outside of ML as well. [This\\nthread](https://skeptics.stackexchange.com/questions/22375/did-a-soviet-nail-factory-produce-useless-nails-to-improve-metrics)\\nreveals an example where a factory that was making chemical\\nmachines (rather than creating a machine that was cheaper and\\nbetter) chose not to adopt producing that machine because their\\noutput was measured in weight. So the thing that the planners\\nactually cared about, economic efficiency and output, was not\\noptimized because it was too difficult to measure.\\n\\nOne reason why these kinds of proxy problems arise so frequently is due\\nto issues of information. **The information that we are able to measure\\nis not the information that we want**. At a higher level, we often don't\\nknow what it is that we truly needed. We may want the validation loss,\\nbut what we need is the loss in production or really the value our users\\nwill derive from this model.\\n\\n### Trade-Offs\",\n",
       "  'Even when we know what we want or what we need, we are likely to run\\ninto the second problem - **the tradeoff between stakeholders**. It is\\nsometimes said that the need to negotiate tradeoffs is one of the\\nreasons why engineers do not like thinking about some of these problems\\naround ethics. That\\'s not quite right because we do accept tradeoffs as\\na key component of engineering.\\n\\n-   In [this O\\'Reilly book on the fundamentals of software\\narchitecture](https://www.oreilly.com/library/view/fundamentals-of-software/9781492043447/),\\nthe first thing they state at the beginning is that **everything\\nin software architecture is a tradeoff.**\\n\\n-   [This satirical O\\'Reilly\\nbook](https://www.reddit.com/r/orlybooks/comments/50meb5/it_depends/)\\nsays that every programming question has the answer: \"It depends.\"\\n\\n![](./media/image20.png)',\n",
       "  'The famous chart above compares the different convolutional networks on\\nthe basis of their accuracy and the number of operations to run them.\\nThinking about these tradeoffs between speed and correctness is exactly\\nthe thing we have to do all the time in our job as engineers.\\n\\nWe can select the **Pareto Front** for the metrics we care about. A way\\nto remember what a Pareto front is [this definition of a data scientist\\nfrom Josh\\nWills](https://twitter.com/josh_wills/status/198093512149958656?lang=en):\\n\"Person who is better at statistics than any software engineer and\\nbetter at software engineering than any statistician.\" The Pareto Front\\nin the chart above includes the models that are more accurate than those\\nwith fewer FLOPs and use fewer FLOPs than those that are more accurate.',\n",
       "  \"A reason why engineers may dislike thinking about these problems is that\\n**it's hard to identify and quantify these tradeoffs**. These are indeed\\nproxy problems. Even further, once measured, where on that front do we\\nfall? As engineers, we may develop expertise in knowing whether we want\\nhigh accuracy or low latency, but we are not as comfortable deciding how\\nmany current orphans we want to trade for what amount of future health.\\nThis raises questions both in terms of measurement and decision-making\\nthat are outside of our expertise.\\n\\n### Humility\\n\\nThe appropriate response is **humility** because most engineers do not\\nexplicitly train in these skills. Many engineers and managers in tech,\\nin fact, constitutionally prefer optimizing single metrics that are not\\nproxies. Therefore, when encountering a different kind of problem, it's\\nimportant to bring a humble mindset, ask for help from experts, and\\nrecognize that the help you get might not be immediately obvious to what\\nyou are used to.\",\n",
       "  \"Additionally, when intervening due to an ethical concern, it's important\\nto remember this humility. It's easy to think that when you are on the\\ngood side, this humility is not necessary. But even trying to be helpful\\nis a delicate and dangerous undertaking. We want to make sure that as we\\nresolve ethical concerns, we come up with solutions that are not just\\nparts of the problem.\\n\\n### User Orientation Undergirds Each Theme\\n\\nWe can resolve all of these via **user orientation**.\\n\\n1.  By getting feedback from users, we maintain **alignment** between\\nour system and our users.\\n\\n2.  When making **tradeoffs**, we should resolve them in consultation\\nwith users.\\n\\n3.  **Humility** means we actually listen to our users because we\\nrecognize we don't have the answers to all the questions.\\n\\n## 3 - Tech Ethics\",\n",
       "  \"The tech industry can't afford to ignore ethics as public trust in tech\\ndeclines. We need to learn from other nearby industries that have done a\\nbetter job on professional ethics. We'll also touch on some contemporary\\ntopics.\\n\\n### Tech Industry's Ethical Crisis\\n\\nThroughout the past decade, the tech industry has been plagued by\\nscandal - whether that's how tech companies interface with national\\ngovernments at the largest scale or how tech systems are being used or\\nmanipulated by people who create disinformation or fake social media\\naccounts that hack the YouTube recommendation system.\\n\\nAs a result, distrust in tech companies has risen markedly in the last\\nten years. [This Public Affairs Pulse\\nsurvey](https://pac.org/public-affairs-pulse-survey-2021)\\nshows that in 2013, the tech industry was one of the industries with\\nless trustworthiness on average. In 2021, it has rubbed elbows with\\nfamously more distrusted industries such as energy and pharmaceuticals.\\n\\n![](./media/image10.png)\",\n",
       "  \"Politicians care quite a bit about public opinion polls. In the last few\\nyears, the fraction of people who believe that large tech companies\\nshould be more regulated has gone up a substantial amount. [Comparing\\nit to 10 years ago, it's astronomically\\nhigher](https://news.gallup.com/poll/329666/views-big-tech-worsen-public-wants-regulation.aspx).\\nSo there will be a substantial impact on the tech industry due to this\\nloss of public trust.\",\n",
       "  \"We can learn from nearby fields: from the culture of professional ethics\\nin engineering in Canada (by wearing [the Iron\\nRing](https://en.wikipedia.org/wiki/Iron_Ring)) to ethical\\nstandards for human subjects research ([Nuremberg\\nCode](https://en.wikipedia.org/wiki/Nuremberg_Code), [1973\\nNational Research\\nAct](https://en.wikipedia.org/wiki/National_Research_Act)).\\nWe are at the point where we need a professional code of ethics for\\nsoftware. Hopefully, many codes of ethics developed in different\\ncommunities can compete with each other and merge into something that\\nmost of us can agree on. That can be incorporated into our education for\\nnew members of our field.\\n\\nLet's talk about two particular ethical concerns that arise in tech in\\ngeneral: carbon emissions and dark/user-hostile design patterns.\\n\\n### Tracking Carbon Emissions\",\n",
       "  \"Because carbon emissions scale with cost, you only need to worry about\\nthem when the costs of what you are working on are very large. Then you\\nwon't be alone in making these decisions and can move a bit more\\ndeliberately to make these choices more thoughtfully.\\n\\nAnthropogenic climate change from carbon emissions raises ethical\\nconcerns - tradeoffs between the present and future generations. The\\nother view is that this is an issue that arises from a classic alignment\\nproblem: many organizations are trying to maximize their profit, which\\nis based on prices for goods that don't include externalities (such as\\nenvironmental damage caused by carbon emissions, leading to increased\\ntemperatures and lactic change).\\n\\n![](./media/image8.png)\",\n",
       "  \"The primary dimension along which we have to worry about carbon\\nemissions is in **compute jobs that require power**. That power can\\nresult in carbon emissions. [This\\npaper](https://aclanthology.org/P19-1355/) walks through\\nhow much carbon dioxide was emitted using typical US-based cloud\\ninfrastructure.\\n\\n-   The top headline shows that training a large Transformer model with\\nneural architecture search produces as much carbon dioxide as five\\ncars create during their lifetimes.\\n\\n-   It's important to remember that power is not free. On US-based cloud\\ninfrastructure, \\\\$10 of cloud spent is roughly equal to \\\\$1 of air\\ntravel costs. That's on the basis of something like the numbers\\nand the chart indicating air travel across the US from New York to\\nSan Francisco.\\n\\n-   Just changing cloud regions can actually reduce your emissions quite\\na bit. There's [a factor of\\n50x](https://www.youtube.com/watch?v=ftWlj4FBHTg)\\nfrom regions with the most to least carbon-intensive power\\ngeneration.\",\n",
       "  'The interest in this problem has led to new tools.\\n[Codecarbon.io](https://codecarbon.io/) allows you to\\ntrack power consumption and reduce carbon emissions from your computing.\\n[ML CO2 Impact](https://mlco2.github.io/impact/) is\\noriented directly towards machine learning.\\n\\n### Deceptive Design and Dark Patterns',\n",
       "  'The other ethical concern in tech is **deceptive design**. An\\nunfortunate amount of deception is tolerated in some areas of software.\\nAs seen below, on the left is a nearly complete history of the way\\nGoogle displays ads in its search engine results. It started off very\\nclearly colored and separated out with bright colors from the rest of\\nthe results. Then about ten years ago, that colored background was\\nremoved and replaced with a tiny little colored snippet that said \"Ad.\"\\nNow, as of 2020, that small bit is no longer even colored. It is just\\nbolded. This makes it difficult for users to know which content is being\\nserved to them because somebody paid for it (versus content served up\\norganically).\\n\\n![](./media/image15.png)',\n",
       "  \"A number of **dark patterns** of deceptive design have emerged over the\\nlast ten years. You can read about them on the website called\\n[deceptive.design](https://www.deceptive.design/). There's\\nalso a Twitter account called\\n[\\\\@darkpatterns](https://twitter.com/darkpatterns) that\\nshares examples found in the wild.\\n\\nA practice in the tech industry that's on a very shaky ethical /legal\\nground is **growth hacking**. This entails a set of techniques for\\nachieving rapid growth in user base or revenue for a product and has all\\nthe connotations you might expect from the name - with examples\\nincluding LinkedIn and Hotmail.\\n\\n![](./media/image14.png)\",\n",
       "  '**ML can actually make this problem worse if we optimize short-term\\nmetrics**. These growth hacks and deceptive designs can often drive user\\nand revenue growth in the short term but worsen user experience and draw\\ndown on goodwill towards the brand in a way that can erode the long-term\\nvalue of customers. When we incorporate ML into the design of our\\nproducts with A/B testing, we have to watch out to make sure that the\\nmetrics that we are optimizing do not encourage this kind of deception.',\n",
       "  \"These arise inside another alignment problem. One broadly-accepted\\njustification for the private ownership of the means of production is\\nthat private enterprise delivers broad social value aligned by price\\nsignals and market focus. But these private enterprises optimize metrics\\nthat are, at best, a proxy for social value. There's the possibility of\\nan alignment problem where **companies pursuing and maximizing their\\nmarket capitalization can lead to net negative production of value**. If\\nyou spend time at the intersection of funding, leadership, and\\ntechnology, you will encounter it.\\n\\n![](./media/image12.png)\\n\\n\\nIn the short term, you can **push for longer-term thinking within your\\norganization** to allow for better alignment between metrics and goals\\nand between goals and utility. You can also learn to recognize\\nuser-hostile designs and **advocate for user-centered design instead**.\\n\\nTo wrap up this section on tech ethics:\",\n",
       "  \"1.  The tech industry should learn from other disciplines if it wants to\\navoid a trust crisis.\\n\\n2.  We can start by educating ourselves about common deceptive or\\nuser-hostile practices in our industry.\\n\\n## 4 - ML Ethics\\n\\nThe ethical concerns raised about ML have gone beyond just the ethical\\nquestions about other kinds of technology. We'll talk about common\\nethical questions in ML and lessons learned from Medical ML.\\n\\n### Why Not Just Tech Ethics?\\n\\nML touches human lives more intimately than other technologies. Many ML\\nmethods, especially deep neural networks, make human-legible data into\\ncomputer-legible data. Humans are more sensitive to errors and have more\\nopinions about visual and text data than they do about the type of data\\nmanipulated by computers. As a result, there are more stakeholders with\\nmore concerns that need to be traded off in ML applications.\",\n",
       "  'Broadly speaking, ML involves being wrong pretty much all the time. Our\\nmodels are statistical and include \"randomness.\" Randomness is almost\\nalways an admission of ignorance. As we admit a certain degree of\\nignorance in our models, our models will be wrong and misunderstand\\nsituations that they are put into. It can be upsetting and even harmful\\nto be misunderstood by our models.\\n\\nAgainst this backlash of greater interest or higher stakes, a number of\\ncommon types of ethical concerns have coalesced in the last couple of\\nyears. There are somewhat established camps of answers to these\\nquestions, so you should at least know where you stand on the four core\\nquestions:\\n\\n1.  Is the model \"fair\"?\\n\\n2.  Is the system accountable?\\n\\n3.  Who owns the data?\\n\\n4.  Should the system be built at all?\\n\\n### Common Ethical Questions in ML\\n\\n#### Is The Model \"Fair\"?',\n",
       "  'The classic case on this comes from criminal justice with [the COMPAS\\nsystem](https://en.wikipedia.org/wiki/COMPAS_(software))\\nfor predicting whether a defendant will be arrested again before trial.\\nIf they are arrested again, that suggests they committed a crime during\\nthat time. This assesses a certain degree of risk for additional harm\\nwhile the justice system decides what to do about a previous arrest and\\npotential crime.\\n\\nThe operationalization here was a 10-point re-arrest probability based\\non past data about this person, and they set a goal from the very\\nbeginning to be less biased than human judges. They operationalize that\\nby calibrating these arrest probabilities across subgroups. Racial bias\\nis a primary concern in the US criminal justice system, so they took\\ncare to make sure that these probabilities of re-arrest were calibrated\\nfor all racial groups.\\n\\n![](./media/image2.png)',\n",
       "  \"The system was deployed and used all around the US. It's proprietary and\\ndifficult to analyze. But using the Freedom of Information Act and\\ncoalescing together a bunch of records, [people at ProPublica were able\\nto run their own analysis of this\\nalgorithm](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing).\\nThey determined that the model was not more or less wrong for one racial\\ngroup or another. It tended to have more false positives for Black\\ndefendants and more false negatives for White defendants. So despite the\\ncreators of COMPAS taking into account bias from the beginning, they\\nstill ended up with an algorithm with this undesirable property of being\\nmore likely to falsely accuse Black defendants than White defendants.\",\n",
       "  'It turned out that some quick algebra revealed that some form of\\nrace-based bias is inevitable in this setting, as indicated [in this\\npaper](https://arxiv.org/abs/1610.07524). There are a large\\nnumber of fairness definitions that are mutually incompatible. [This\\ntutorial by Arvind\\nNarayanan](https://www.youtube.com/watch?v=jIXIuYdnyyk&ab_channel=ArvindNarayanan)\\nis an excellent one to display them.\\n\\nIt is noteworthy that **the impact of \"unfairness\" is not fixed**. The\\nstory is often presented as \"no matter what, the journalists would have\\nfound something to complain about.\" But note that equalizing false\\npositive rates and positive predictive value across groups would lead to\\na higher false negative rate for Black defendants relative to White\\ndefendants. In the context of American politics, that\\'s not going to\\nlead to complaints from the same people.\\n\\n![](./media/image6.png)',\n",
       "  \"This is the story about the necessity of confronting the tradeoffs that\\nwill inevitably come up. Researchers at Google made [a nice little\\ntool](https://research.google.com/bigpicture/attacking-discrimination-in-ml/)\\nwhere you can think through and make these tradeoffs for yourself. It's\\nhelpful for building intuition on these fairness metrics and what it\\nmeans to pick one over the other.\\n\\nEvents in this controversy kicked off a flurry of research on fairness.\\n[The Fairness, Accountability, and Transparency\\nconference](https://facctconference.org/) has been held for\\nseveral years. There has been a ton of work on both **algorithmic-level\\napproaches** on measuring and incorporating fairness metrics into\\ntraining and **qualitative work** on designing systems that are more\\ntransparent and accountable.\",\n",
       "  'In the case of COMPAS, **re-arrest is not the same as recidivism**.\\nBeing rearrested requires that a police officer believes you committed a\\ncrime. Police officers are subject to their own biases and patterns of\\npolicing, which result in a far higher fraction of crimes being caught\\nfor some groups than for others. Our real goal, in terms of fairness and\\ncriminal justice, might be around reducing those kinds of unfair impacts\\nand using past rearrest data that have these issues.\\n\\n#### Representation Matters for Model Fairness\\n\\n![](./media/image18.png)\\n\\nUnfortunately, it is easy to make ML-powered tech that fails for\\nminoritized groups. For example, off-the-shelf computer vision tools\\noften fail on darker sins (as illustrated in [this talk by Joy\\nBuolamwini](https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms)).\\nThis is not a new issue in technology, just a more salient one with ML.',\n",
       "  \"There has been a good amount of progress on this in the last five years.\\nAn example is [Google's Model\\nCards](https://modelcards.withgoogle.com/about) which show\\nhow well a model will perform on human subgroups of interest.\\nHuggingFace has good integrations for creating these kinds of model\\ncards.\\n\\nWhen you invite people for talks or hire people to join your\\norganizations, you should work to reduce the bias of that discovery\\nprocess by diversifying your network. Some good resources include\\n[Black in AI](https://blackinai.github.io/#/), [Diversify\\nTech Job Board](https://www.diversifytech.co/job-board/),\\n[Women in Data Science](https://www.widsconference.org/),\\nand the [You Belong in AI\\npodcast](https://anchor.fm/ucla-acm-ai). You can make\\nprofessional connections via them to improve the representation of\\nminoritized groups in the engineering, design, and product management\\nprocess.\\n\\n#### Is The System Accountable?\",\n",
       "  'At a broader level than fairness, we should expect \"accountability\" from\\nML systems. Some societies and states, including the EU, consider \"[the\\nright to an explanation](https://arxiv.org/abs/1606.08813)\"\\nin the face of important judgments to be a part of human rights.\\n\\nIn the GDPR act, there is [a section that enshrines\\naccountability](https://www.consumerfinance.gov/rules-policy/regulations/1002/interp-9/#9-b-1-Interp-1).\\nThis isn\\'t quite a totally new requirement; credit denials in the US\\nhave been required to be explained since 1974. People have a right to\\nknow what and why into making decisions for them!',\n",
       "  'If you want to impose this \"accountability\" on a deep neural network and\\nunderstand its selections, there are a number of methods that use the\\ninput-output gradient to explain the model. You can see a list of\\nseveral methods in order of increasing performance below (from [this\\npaper](https://arxiv.org/abs/1810.03292)). These approaches\\ndon\\'t quite have strong theoretical underpinnings or a holistic\\nexplanation, and are not that robust as a result. A lot of these methods\\nact primarily as edge detectors. The paper shows how even randomizing\\nlayers in a model does not materially change the interpretability output\\nof GradCAM methods.\\n\\n![](./media/image11.png)\\n\\n\\nAs a result, introspecting DNNs effectively requires reverse engineering\\nthe system to really understand what is going on, largely thanks to\\nefforts like [Distil](https://distil.pub/) and\\n[Transfomer Circuits](https://transformer-circuits.pub/).',\n",
       "  'Due to these technical challenges, machine learning systems are prone to\\nunaccountability that impacts most those least able to understand and\\ninfluence their outputs. Books such as [Automating\\nInequality](https://www.amazon.com/Automating-Inequality-High-Tech-Profile-Police/dp/1250074312)\\ndescribe the impacts of these systems. In such a context, you should\\nseek to question the purpose of model, involve those impacted by the\\ndecisions (either through direct human inputs or through other means),\\nand ensure that equal attention is paid to benefits and harms of\\nautomation.\\n\\n#### Who Owns The Data?\\n\\n**Humans justifiably feel ownership of the data they creat**e, which is\\nsubsequently used to train machine learning models. Large datasets used\\nto train models like GPT-3 are created by mining this data without the\\nexplicit involvement of those who create the data. Many people are not\\naware that this is both possible and legal. As technology has changed,\\nwhat can be done with data has changed.',\n",
       "  '[You can even verify if your data has been used to train models\\non](https://haveibeentrained.com/). Some of these images\\nare potentially [obtained\\nillegally](https://arstechnica.com/information-technology/2022/09/artist-finds-private-medical-record-photos-in-popular-ai-training-data-set/),\\nas a result of sensitive data being posted openly without the recorded\\nconsent of the originator.\\n\\n![](./media/image5.png)\\n\\n\\nEach of these controversies around image generation and illegal data has\\nopened up a new frontier in **data governance**. Focus will be placed on\\nensuring new ML systems are sensitive to personal and professional\\nconcerns of those who generate the data ML systems are trained on.\\n[Emad Mostaque](https://uk.linkedin.com/in/emostaque), CEO\\nof [Stability AI](https://stability.ai/), has gone so far\\nas to offer future opt out systems from systems similar to Stable\\nDiffusion.',\n",
       "  'Here are some practical tips: [Dataset\\ncards](https://huggingface.co/docs/datasets/dataset_card)\\ncan be helpful in providing documentation in a similar fashion to model\\ncards. There are also ethics lists, like [the deon ethic\\nchecklist](https://deon.drivendata.org/examples/) that\\nhelps design proper systems. Deon also has a helpful list of failure\\ncases.\\n\\n#### Should This Be Built At All?\\n\\nThe undercurrent behind this conversation is the justifiable question of\\nwhether some of these systems should be built at all, let alone in an\\nethical way.\\n\\n**ML-powered weaponry** is the canonical example here, which is already\\nin use. The definition of these systems are blurry, as both systems old\\nand new have had various autonomous capacities. This is difficult to get\\na sense of due to the secrecy associated with weapon systems.',\n",
       "  'Some have argued that \"autonomous weapons\" have existed for hundreds of\\nyears, but even this does not mean that they are ethical. Mines are good\\nexamples of these systems. Movements like t[he Campaign Against Killer\\nRobots](https://www.stopkillerrobots.org/about-us/) are\\ntrying to prevent the cycle we entered with mines - where we invented\\nthem, when we realized the incredible harm, and why we are trying to ban\\nthem. Why invent these at all?\\n\\nLet\\'s wrap up this entire section with some closing questions that you\\nshould always have a thoughtful answer to as you build a machine\\nlearning system.\\n\\n1.  **Is the model \"fair\"?** Fairness is possible, but requires\\ntrade-offs.\\n\\n2.  **Is the system accountable?** Accountability is easier than\\ninterpretability.\\n\\n3.  **Who owns the data?** Answer this upfront. Changes are on the way.\\n\\n4.  **Should the system be built at all?** Repeatedly ask this and use\\nit to narrow scope.\\n\\n### What Can We Learn from Medical ML',\n",
       "  \"*Note: The FSDL team would like to thank [Dr. Amir Ashraf\\nGanjouei](https://scholar.google.com/citations?user=pwLadpcAAAAJ)\\nfor his feedback on this section.*\\n\\nInterestingly, medicine can teach us a lot about how to apply machine\\nlearning in a responsible way. Fundamentally, this has led to a mismatch\\nbetween how medicine works and how machine learning systems are built\\ntoday.\\n\\nLet's start with a startling fact: **the machine learning response to\\nCOVID-19 was an abject failure**. In contrast, the biomedical response\\nwas a major triumph. For example, the vaccines were developed with\\ntremendous speed and precision.\\n\\n![](./media/image9.png)\",\n",
       "  'Machine learning did not acquit itself well with the COVID-19 problem.\\nTwo reviews ([Roberts et al.,\\n2021](https://www.nature.com/articles/s42256-021-00307-0)\\nand [Wynants et al.,\\n2020-2022](https://www.bmj.com/content/369/bmj.m1328))\\nfound that nearly all machine learning models were insufficiently\\ndocumented, had little to no external validation, and did not follow\\nmodel development best practices. A full 25% of the papers used a\\ndataset incorrect for the task, which simply highlighted the difference\\nbetween children and adults, not pneumonia and COVID.',\n",
       "  'Medicine has a strong culture of ethics that professionals are\\nintegrated into from the point they start training. Medical\\nprofessionals take the Hippocratic oath of practicing two things: either\\nhelp or do not harm the patient. In contrast, the foremost belief\\nassociated with software development tends to be the infamous \"Move fast\\nand break things.\" While this approach works for harmless software like\\nweb apps, **it has serious implications for medicine and other more\\ncritical sectors**. Consider the example of a retinal implant that was\\nsimply deprecated by developers and left hundreds without sight [in\\nthis Statnews\\narticle](https://www.statnews.com/2022/08/10/implant-recipients-shouldnt-be-left-in-the-dark-when-device-company-moves-on/).\\n\\n![](./media/image4.png)\\n\\n**Researchers are drawing inspiration from medicine to develop similar\\nstandards for ML**.',\n",
       "  '-   For example, clinical trial standards have been extended to ML.\\nThese standards were developed through extensive surveys,\\nconferences, and consensus building (detailed in\\n[these](https://www.nature.com/articles/s41591-020-1037-7)\\n[papers](https://www.nature.com/articles/s41591-020-1034-x)).\\n\\n-   Progress is being made in understanding how this problem presents.\\n[A recent\\nstudy](https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2796833)\\nfound that while clinical activities are generally performed at a\\nhigh compliance level, statistical and data issues tend to suffer\\nlow compliance.\\n\\n-   New approaches are developing [entire \"auditing\"\\nprocedures](https://www.thelancet.com/journals/landig/article/PIIS2589-7500(22)00003-6/fulltext)\\nthat exquisitely identify the activities required to effectively\\ndevelop models.',\n",
       "  'Like medicine, machine learning is intimately intertwined with people\\'s\\nlives. The most important question to ask is \"Should this system be\\nbuilt at all?\". Always ask yourselves this and understand the\\nimplications!\\n\\n## 5 - AI Ethics\\n\\nAI ethics are a frontier in both the technology and the ethics worlds.\\nFalse claims and hype are the most pressing concerns, but other risks\\ncould present themselves soon.\\n\\n### AI Snake Oils\\n\\n**False claims outpace the performance of AI**. This poses a serious\\nthreat to adoption and satisfaction with AI systems long term.\\n\\n-   For example, if you call something \"AutoPilot\", people might truly\\nassume it is fully autonomous, as happened in the below case of a\\nTesla user. This goes back to our discussion about how AI systems\\nare more like funky dogs than truly human intelligent systems.',\n",
       "  '-   Another example of this is [IBM\\'s Watson\\nsystem](https://www.ibm.com/ibm/history/ibm100/us/en/icons/watson/),\\nwhich went from tackling the future of healthcare to being sold\\noff for parts.\\n\\n![](./media/image13.png)\\n\\nThese false claims tend to be amplified in the media. But this isn\\'t\\nconfined to traditional media. Even Geoff Hinton, a godfather of modern\\nmachine learning, has been [a little too aggressive in his forecasts\\nfor AI\\nperformance](https://www.youtube.com/watch?v=2HMPRXstSvQ)!\\n\\nYou can call this **\"AI Snake Oil\"** as Arvind Narayanan does in [his\\nSubstack](https://aisnakeoil.substack.com/) and\\n[talk](https://www.cs.princeton.edu/~arvindn/talks/MIT-STS-AI-snakeoil.pdf).\\n\\nLet\\'s separate out where true progress has been made versus where\\nprogress is likely to be overstated. On some level, AI perception has\\nseen tremendous progress, AI judgment has seen moderate progress, and AI\\nprediction of social outcomes has seen not nearly as much progress.\\n\\n![](./media/image3.png)',\n",
       "  \"### Frontiers: AI Rights and X-Risk\\n\\nThere's obvious rationale that should artificial sentient beings exist,\\ntremendous ethical implications would be raised. Few people believe that\\nwe are truly on the precipice of sentient beings, but there is\\ndisagreement on how close we are.\\n\\n![](./media/image1.png)\\n\\nThere's a different set of concerns around how to regard self-improving\\nintelligent beings, for which there is already evidence. Large Language\\nModels have been show to be able to improve themselves in a range of\\nstudies\\n([here](https://openreview.net/forum?id=92gvk82DE-) and\\n[here](https://arxiv.org/abs/2207.14502v1)).\\n\\nFailing to pursue this technology would lead to [a huge opportunity\\ncost](https://nickbostrom.com/astronomical/waste) (as\\nargued by Nick Bostrom)! There truly is a great opportunity in having\\nsuch systems help us sold major problems and lead better lives. The key\\nthough, is that such technology should be developed in the **safest way\\npossible,** not the fastest way.\",\n",
       "  \"[The paperclip\\nproblem](https://www.lesswrong.com/tag/paperclip-maximizer)\\nshows how the potential for misalignment between AI systems and humans\\ncould dramatically reduce human utility and even compromise our\\ninterests. Imagine a system designed to manufacture paperclips... could\\nactually develop the intelligence to alter elements of society to favor\\npaper clips?! This thought experiments illustrates how self-learning\\nsystems could truly change our world for the worse in a misaligned way.\\n\\nThese ideas around existential risk are most associated with [the\\nEffective Altruism community](https://www.eaglobal.org/).\\nCheck out resources like [Giving What We\\nCan](https://www.givingwhatwecan.org/donate/organizations)\\nand [80,000 Hours](https://80000hours.org/) if you're\\ninterested!\\n\\n## 6 - What Is To Be Done?\\n\\nThis course can't end on a dour a note as existential risk. What can be\\ndone to mitigate these consequences and participate in developing truly\\nethical AI?\",\n",
       "  '1.  The first step is **to educate yourself on the topic**. There are\\nmany great books that give lengthy, useful treatment to this\\ntopic. We recommend [Automating\\nInequality](https://www.amazon.com/Automating-Inequality-High-Tech-Profile-Police/dp/1250074312),\\n[Weapons of Math\\nDestruction](https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality/dp/0553418815),\\nand [The Alignment\\nProblem](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821).\\n\\n2.  After reading this, **consider how to prioritize your actions**.\\nWhat do you want to impact? When do you want to do that? Place\\nthem in this two-by-two to get a sense of where their importance\\nis.\\n\\n![](./media/image7.png)\\n\\n**Ethics cannot be purely negative**. We do good, and we want to\\n*prevent* bad! Focus on the good you can do and be mindful of the harm\\nyou can prevent.',\n",
       "  'Leading organizations like\\n[DeepMind](https://www.deepmind.com/about/operating-principles)\\nand [OpenAI](https://openai.com/charter/) are leading from\\nthe front. Fundamentally, building ML well aligns with building ML for\\ngood. All the leading organizations emphasize effective *and*\\nresponsible best practices for building ML powered practices. Keep all\\nthis in mind as you make the world a better place with your AI-powered\\nproducts!'],\n",
       " 'https://fullstackdeeplearning.com/course/2022/lecture-6-continual-learning': ['---\\ndescription: Software engineering, Deep learning frameworks, Distributed training, GPUs, and Experiment Management.\\n---\\n\\n# Lecture 2: Development Infrastructure & Tooling\\n\\n<div align=\"center\">\\n<iframe width=\"720\" height=\"405\" src=\"https://www.youtube-nocookie.com/embed/BPYOsDCZbno?list=PL1T8fO7ArWleMMI8KPJ_5D5XSlovTW_Ur\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\\n</div>\\n\\nLecture by [Sergey Karayev](https://twitter.com/sergeykarayev).\\nNotes by [James Le](https://twitter.com/le_james94) and [Vishnu Rachakonda](https://www.linkedin.com/in/vrachakonda/).<br />\\nPublished August 15, 2022.\\n[Download slides](https://drive.google.com/open?id=16pEG5GesO4_UAWiD5jrIReMGzoyn165M).\\n\\n## 1 - Introduction\\n\\nThe **dream** of ML development is that given a project spec and some\\nsample data, you get a continually improving prediction system deployed\\nat scale.',\n",
       "  \"The **reality** is starkly different:\\n\\n-   You have to collect, aggregate, process, clean, label, and version\\nthe data.\\n\\n-   You have to find the model architecture and their pre-trained\\nweights and then write and debug the model code.\\n\\n-   You run training experiments and review the results, which will be\\nfed back into the process of trying out new architectures and\\ndebugging more code.\\n\\n-   You can now deploy the model.\\n\\n-   After model deployment, you have to monitor model predictions and\\nclose the data flywheel loop. Basically, your users generate fresh\\ndata for you, which needs to be added to the training set.\\n\\n![](./media/image3.png)\\n\\n\\nThis reality has roughly three components: data, development, and\\ndeployment. The tooling infrastructure landscape for them is large, so\\nwe'll have three lectures to cover it all. **This lecture focuses on the\\ndevelopment component**.\\n\\n## 2 - Software Engineering\\n\\n![](./media/image7.png)\\n\\n\\n### Language\",\n",
       "  'For your choice of **programming language**, Python is the clear winner\\nin scientific and data computing because of all the libraries that have\\nbeen developed. There have been some contenders like Julia and C/C++,\\nbut Python has really won out.\\n\\n### Editors\\n\\nTo write Python code, you need an **editor**. You have many options,\\nsuch as Vim, Emacs, Jupyter Notebook/Lab, VS Code, PyCharm, etc.\\n\\n-   We recommend [VS Code](https://code.visualstudio.com/)\\nbecause of its nice features such as built-in git version control,\\ndocumentation peeking, remote projects opening, linters and type\\nhints to catch bugs, etc.',\n",
       "  '-   Many practitioners develop in [Jupyter\\nNotebooks](https://jupyter.org/), which is great as\\nthe \"first draft\" of a data science project. You have to put in\\nlittle thought before you start coding and seeing the immediate\\noutput. However, notebooks have a variety of problems: primitive\\neditor, out-of-order execution artifacts, and challenges to\\nversion and test them. A counterpoint to these problems is the\\n[nbdev package](https://nbdev.fast.ai/) that lets\\nyou write and test code all in one notebook environment.\\n\\n-   We recommend you use **VS Code with built-in support for\\nnotebooks** - where you can write code in modules imported into\\nnotebooks. It also enables awesome debugging.\\n\\nIf you want to build something more interactive,\\n[Streamlit](https://streamlit.io/) is an excellent choice.\\nIt lets you decorate Python code, get interactive applets, and publish\\nthem on the web to share with the world.\\n\\n![](./media/image10.png)',\n",
       "  'For setting up the Python environment, we recommend you see [how we did\\nit in the\\nlab.](https://github.com/full-stack-deep-learning/conda-piptools)\\n\\n## 3 - Deep Learning Frameworks\\n\\n![](./media/image15.png)\\n\\n\\nDeep learning is not a lot of code with a matrix math library like\\nNumpy. But when you have to deploy your code onto CUDA for GPU-powered\\ndeep learning, you want to consider deep learning frameworks as you\\nmight be writing weird layer types, optimizers, data interfaces, etc.\\n\\n### Frameworks\\n\\nThere are various frameworks, such as PyTorch, TensorFlow, and Jax. They\\nare all similar in that you first define your model by running Python\\ncode and then collect an optimized execution graph for different\\ndeployment patterns (CPU, GPU, TPU, mobile).',\n",
       "  '1.  We prefer PyTorch because [it is absolutely\\ndominant](https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2022/)\\nby measures such as the number of models, the number of papers,\\nand the number of competition winners. For instance, about [77%\\nof 2021 ML competition winners used\\nPyTorch](https://blog.mlcontests.com/p/winning-at-competitive-ml-in-2022?s=w).\\n\\n2.  With TensorFlow, you have TensorFlow.js (that lets you run deep\\nlearning models in your browser) and Keras (an unmatched developer\\nexperience for easy model development).\\n\\n3.  Jax is a meta-framework for deep learning.\\n\\n![](./media/image12.png)\\n\\n\\n[PyTorch](https://pytorch.org/) has excellent developer\\nexperience and is production-ready and even faster with TorchScript.\\nThere is a great distributed training ecosystem. There are libraries for\\nvision, audio, etc. There are also mobile deployment targets.',\n",
       "  '[PyTorch Lightning](https://www.pytorchlightning.ai/)\\nprovides a nice structure for organizing your training code, optimizer\\ncode, evaluation code, data loaders, etc. With that structure, you can\\nrun your code on any hardware. There are nice features such as\\nperformance and bottleneck profiler, model checkpointing, 16-bit\\nprecision, and distributed training libraries.\\n\\nAnother possibility is [FastAI\\nsoftware](https://www.fast.ai/), which is developed\\nalongside the fast.ai course. It provides many advanced tricks such as\\ndata augmentations, better initializations, learning rate schedulers,\\netc. It has a modular structure with low-level API, mid-level API,\\nhigh-level API, and specific applications. The main problem with FastAI\\nis that its code style is quite different from mainstream Python.',\n",
       "  'At FSDL, we prefer PyTorch because of its strong ecosystem, but\\n[TensorFlow](https://www.tensorflow.org/) is still\\nperfectly good. If you have a specific reason to prefer it, you are\\nstill going to have a good time.\\n\\n[Jax](https://github.com/google/jax) is a more recent\\nproject from Google that is not specific to deep learning. It provides\\ngeneral vectorization, auto-differentiation, and compilation to GPU/TPU\\ncode. For deep learning, there are separate frameworks like\\n[Flax](https://github.com/google/flax) and\\n[Haiku](https://github.com/deepmind/dm-haiku). You should\\nonly use Jax for a specific need.\\n\\n### Meta-Frameworks and Model Zoos\\n\\nMost of the time, you will start with at least a model architecture that\\nsomeone has developed or published. You will use a specific architecture\\n(trained on specific data with pre-trained weights) on a model hub.',\n",
       "  \"-   [ONNX](https://onnx.ai/) is an open standard for\\nsaving deep learning models and lets you convert from one type of\\nformat to another. It can work well but can also run into some\\nedge cases.\\n\\n-   [HuggingFace](https://huggingface.co/) has become an\\nabsolutely stellar repository of models. It started with NLP tasks\\nbut has then expanded into all kinds of tasks (audio\\nclassification, image classification, object detection, etc.).\\nThere are 60,000 pre-trained models for all these tasks. There is\\na Transformers library that works with PyTorch, TensorFlow, and\\nJax. There are 7,500 datasets uploaded by people. There's also a\\ncommunity aspect to it with a Q&A forum.\\n\\n-   [TIMM](https://github.com/rwightman/pytorch-image-models)\\nis a collection of state-of-the-art computer vision models and\\nrelated code that looks cool.\\n\\n## 4 - Distributed Training\\n\\n![](./media/image9.png)\",\n",
       "  \"Let's say we have multiple machines represented by little squares above\\n(with multiple GPUs in each machine). You are sending batches of data to\\nbe processed by a model with parameters. The data batch can fit on a\\nsingle GPU or not. The model parameters can fit on a single GPU or not.\\n\\nThe best case is that both your data batch and model parameters fit on a\\nsingle GPU. That's called **trivial parallelism**. You can either launch\\nmore independent experiments on other GPUs/machines or increase the\\nbatch size until it no longer fits on one GPU.\\n\\n### Data Parallelism\\n\\nIf your model still fits on a single GPU, but your data no longer does,\\nyou have to try out **data parallelism** - which lets you distribute a\\nsingle batch of data across GPUs and average gradients that are computed\\nby the model across GPUs. A lot of model development work is cross-GPU,\\nso you want to ensure that GPUs have fast interconnects.\",\n",
       "  \"If you are using a server card, expect [a linear\\nspeedup](https://lambdalabs.com/blog/best-gpu-2022-sofar/)\\nin training time. If you are using a consumer card, expect [a sublinear\\nspeedup](https://lambdalabs.com/blog/titan-v-deep-learning-benchmarks/)\\ninstead.\\n\\nData parallelism is implemented in PyTorch with the robust\\n[DistributedDataParallel\\nlibrary](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html).\\n[Horovod](https://github.com/horovod/horovod) is another\\n3rd-party library option. PyTorch Lightning makes it dead simple to use\\neither of these two libraries - where [speedup seems to be the\\nsame](https://www.reddit.com/r/MachineLearning/comments/hmgr9g/d_pytorch_distributeddataparallel_and_horovod/).\\n\\nA more advanced scenario is that you can't even fit your model on a\\nsingle GPU. You have to spread the model over multiple GPUs. There are\\nthree solutions to this.\\n\\n### Sharded Data-Parallelism\\n\\nSharded data parallelism starts with the question: What exactly takes up\\nGPU memory?\",\n",
       "  '-   The **model parameters** include the floats that make up our model\\nlayers.\\n\\n-   The **gradients** are needed to do back-propagation.\\n\\n-   The **optimizer states** include statistics about the gradients\\n\\n-   Finally, you have to send a **batch of data** for model development.\\n\\n![](./media/image5.png)',\n",
       "  'Sharding is a concept from databases where if you have one source of\\ndata, you actually break it into shards of data that live across your\\ndistributed system. Microsoft implemented an approach called\\n[ZeRO](https://arxiv.org/pdf/1910.02054.pdf) that shards\\nthe optimizer states, the gradients, and the model parameters. **This\\nresults in an insane order of magnitude reduction in memory use, which\\nmeans your batch size can be 10x bigger.** You should [watch the video\\nin this\\narticle](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)\\nto see how model parameters are passed around GPUs as computation\\nproceeds.',\n",
       "  \"Sharded data-parallelism is implemented by Microsoft's\\n[DeepSpeed](https://github.com/microsoft/DeepSpeed)\\nlibrary and Facebook's\\n[FairScale](https://github.com/facebookresearch/fairscale)\\nlibrary, as well as natively by PyTorch. In PyTorch, it's called\\n[Fully-Sharded\\nDataParallel](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/).\\nWith PyTorch Lightning, you can try it for a massive memory reduction\\nwithout changing the model code.\\n\\nThis same ZeRO principle can also be applied to a single GPU. You can\\ntrain a 13B-parameter model on a single V100 (32GB) GPU. Fairscale\\nimplements this (called\\n[CPU-offloading](https://fairscale.readthedocs.io/en/stable/deep_dive/offload.html)).\\n\\n### Pipelined Model-Parallelism\",\n",
       "  '**Model parallelism means that you can put each layer of your model on\\neach GPU**. It is trivial to implement natively but results in only one\\nGPU being active at a time. Libraries like DeepSpeed and FairScale make\\nit better by pipelining computation so that the GPUs are fully utilized.\\nYou need to tune the amount of pipelining on the batch size to the exact\\ndegree of how you will split up the model on the GPU.\\n\\n### Tensor-Parallelism\\n\\nTensor parallelism is another approach, which observes that there is\\nnothing special about matrix multiplication that requires the whole\\nmatrix to be on one GPU. **You can distribute the matrix over multiple\\nGPUs**. NVIDIA published [the Megatron-LM\\nrepo](https://github.com/NVIDIA/Megatron-LM), which does\\nthis for the Transformer model.',\n",
       "  \"You can actually use all of the three techniques mentioned above if you\\nreally want to scale a huge model (like a GPT-3 sized language model).\\nRead [this article on the technology behind BLOOM\\ntraining](https://huggingface.co/blog/bloom-megatron-deepspeed)\\nfor a taste.\\n\\n![](./media/image6.png)\\n\\n\\nIn conclusion:\\n\\n-   If your model and data fit on one GPU, that's awesome.\\n\\n-   If they do not, and you want to speed up training, try\\nDistributedDataParallel.\\n\\n-   If the model still doesn't fit, try ZeRO-3 or Full-Sharded Data\\nParallel.\\n\\nFor more resources to speed up model training, look at [this list\\ncompiled by DeepSpeed](https://www.deepspeed.ai/training/),\\n[MosaicML](https://www.mosaicml.com), and\\n[FFCV](https://ffcv.io).\\n\\n## 5 - Compute\\n\\n![](./media/image14.png)\\n\\n\\n**Compute** is the next essential ingredient to developing machine\\nlearning models and products.\",\n",
       "  'The compute-intensiveness of models has grown tremendously over the last\\nten years, as the below charts from\\n[OpenAI](https://openai.com/blog/ai-and-compute/) and\\n[HuggingFace](https://huggingface.co/blog/large-language-models)\\nshow.\\n\\n![](./media/image1.png)\\n\\n\\nRecent developments, including models like\\n[GPT-3](https://openai.com/blog/gpt-3-apps/), have\\naccelerated this trend. These models are extremely large and require a\\nlarge number of petaflops to train.\\n\\n### GPUs\\n\\n**To effectively train deep learning models**, **GPUs are required.**\\nNVIDIA has been the superior choice for GPU vendors, though Google has\\nintroduced TPUs (Tensor Processing Units) that are effective but are\\nonly available via Google Cloud. There are three primary considerations\\nwhen choosing GPUs:\\n\\n1.  How much data fits on the GPU?\\n\\n2.  How fast can the GPU crunch through data? To evaluate this, is your\\ndata 16-bit or 32-bit? The latter is more resource intensive.',\n",
       "  \"3.  How fast can you communicate between the CPU and the GPU and between\\nGPUs?\\n\\nLooking at recent NVIDIA GPUs, it becomes clear that a new\\nhigh-performing architecture is introduced every few years. There's a\\ndifference between these chips, which are licensed for personal use as\\nopposed to corporate use; businesses should only use **server**\\n**cards**.\\n\\n![](./media/image8.png)\\n\\n\\nTwo key factors in evaluating GPUs are **RAM** and **Tensor TFlops**.\\nThe more RAM, the better the GPU contains large models and datasets.\\nTensor TFlops are special tensor cores that NVIDIA includes specifically\\nfor deep learning operations and can handle more intensive\\nmixed-precision operations. **A tip**: leveraging 16-bit training can\\neffectively double your RAM capacity!\",\n",
       "  'While these theoretical benchmarks are useful, how do GPUs perform\\npractically? Lambda Labs offers [the best benchmarks\\nhere](https://lambdalabs.com/gpu-benchmarks). Their results\\nshow that the most recent server-grade NVIDIA GPU (A100) is more than\\n2.5 times faster than the classic V100 GPU. RTX chips also outperform\\nthe V100. [AIME is also another source of GPU\\nbenchmarks](https://www.aime.info/en/blog/deep-learning-gpu-benchmarks-2021/).\\n\\nCloud services such as Microsoft Azure, Google Cloud Platform, and\\nAmazon Web Services are the default place to buy access to GPUs. Startup\\ncloud providers like\\n[Paperspace](https://www.paperspace.com/),\\n[CoreWeave](https://www.coreweave.com/), and [Lambda\\nLabs](https://lambdalabs.com/) also offer such services.\\n\\n### TPUs',\n",
       "  \"Let's briefly discuss TPUs. There are four generations of TPUs, and the\\nmost recent v4 is the fastest possible accelerator for deep learning. V4\\nTPUs are not generally available yet, but **TPUs generally excel at\\nscaling to larger and model sizes**. The below charts compare TPUs to\\nthe fastest A100 NVIDIA chip.\\n\\n![](./media/image11.png)\\n\\n\\nIt can be overwhelming to compare the cost of cloud access to GPUs, so\\n[we made a tool that solves this\\nproblem](https://fullstackdeeplearning.com/cloud-gpus/)!\\nFeel free to contribute to [our repository of Cloud GPU cost\\nmetrics](https://github.com/full-stack-deep-learning/website/).\\nThe tool has all kinds of nifty features like enabling filters for only\\nthe most recent chip models, etc.\",\n",
       "  \"If we [combine the cost metrics with performance\\nmetrics](https://github.com/full-stack-deep-learning/website/blob/main/docs/cloud-gpus/benchmark-analysis.ipynb),\\nwe find that **the most expensive per hour chips are not the most\\nexpensive per experiment!** Case in point: running the same Transformers\\nexperiment on 4 V100s costs \\\\$1750 over 72 hours, whereas the same\\nexperiment on 4 A100s costs \\\\$250 over only 8 hours. Think carefully\\nabout cost and performance based on the model you're trying to train.\\n\\nSome helpful heuristics here are:\\n\\n1.  Use the most expensive per-hour GPU in the least expensive cloud.\\n\\n2.  Startups (e.g., Paperspace) tend to be cheaper than major cloud\\nproviders.\\n\\n### On-Prem vs. Cloud\",\n",
       "  'For **on-prem** use cases, you can build your own pretty easily or opt\\nfor a pre-built computer from a company like NVIDIA. You can build a\\ngood, quiet PC with 128 GB RAM and 2 RTX 3909s for about \\\\$7000 and set\\nit up in a day. Going beyond this can start to get far more expensive\\nand complicated. Lambda Labs offers a \\\\$60,000 machine with 8 A100s\\n(super fast!). Tim Dettmers offers a great (slightly outdated)\\nperspective on building a machine\\n[here](https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/).\\n\\nSome tips on on-prem vs. cloud use:\\n\\n-   It can be useful to have your own GPU machine to shift your mindset\\nfrom minimizing cost to maximizing utility.\\n\\n-   To truly scale-out experiments, you should probably just use the\\nmost expensive machines in the least expensive cloud.\\n\\n-   TPUs are worth experimenting with for large-scale training, given\\ntheir performance.\\n\\n-   Lambda Labs is a sponsor, and we highly encourage looking at them\\nfor on-prem and cloud GPU use!',\n",
       "  \"## 6 - Resource Management\\n\\n![](./media/image2.png)\\n\\n\\nNow that we've talked about raw compute, let's talk about options for\\n**how to manage our compute resources**. Let's say we want to manage a\\nset of experiments. Broadly speaking, we'll need hardware in the form of\\nGPUs, software requirements (e.g., PyTorch version), and data to train\\non.\\n\\n### Solutions\\n\\nLeveraging best practices for specifying dependencies (e.g., Poetry,\\nconda, pip-tools) makes the process of spinning up such experiments\\nquick and easy on a single machine.\\n\\nIf, however, you have a cluster of machines to run experiments on,\\n[SLURM](https://slurm.schedmd.com/documentation.html) is\\nthe tried and true solution for workload management that is still widely\\nused.\",\n",
       "  \"For more portability, [Docker](https://www.docker.com/) is\\na way to package up an entire dependency stack into a lighter-than-a-VM\\npackage. [Kubernetes](https://kubernetes.io/) is the most\\npopular way to run many Docker containers on top of a cluster. The OSS\\n[Kubeflow](https://www.kubeflow.org/) project helps manage\\nML projects that rely on Kubernetes.\\n\\nThese projects are useful, but they may not be the easiest or best\\nchoice. They're great if you already have a cluster up and running, but\\n**how do you actually set up a cluster or compute platform?**\\n\\n*Before proceeding, FSDL prefers open source and/or transparently priced\\nproducts. We discuss tools that fall into these categories, not SaaS\\nwith opaque pricing.*\\n\\n### Tools\",\n",
       "  \"For practitioners all in on AWS, [AWS\\nSagemaker](https://aws.amazon.com/sagemaker/) offers a\\nconvenient end-to-end solution for building machine learning models,\\nfrom labeling data to deploying models. Sagemaker has a ton of\\nAWS-specific configuration, which can be a turnoff, but it brings a lot\\nof easy-to-use old school algorithms for training and allows you to BYO\\nalgorithms as well. They're also increasing support for PyTorch, though\\nthe markup for PyTorch is about 15-20% more expensive.\\n\\n[Anyscale](https://www.anyscale.com/) is a company created\\nby the makers of the Berkeley OSS project\\n[Ray](https://github.com/ray-project/ray). Anyscale\\nrecently launched [Ray\\nTrain](https://docs.ray.io/en/latest/train/train.html),\\nwhich they claim is faster than Sagemaker with a similar value\\nproposition. Anyscale makes it really easy to provision a compute\\ncluster, but it's considerably more expensive than alternatives.\",\n",
       "  '[Grid.ai](https://www.grid.ai/) is created by the PyTorch\\nLightning creators. Grid allows you to specify what compute parameters\\nto use easily with \"grid run\" followed by the types of compute and\\noptions you want. You can use their instances or AWS under the hood.\\nGrid has an uncertain future, as its future compatibility with Lightning\\n(given their rebrand) has not been clarified.\\n\\nThere are several non-ML options for spinning up compute too! Writing\\nyour own scripts, using various libraries, or even Kubernetes are all\\noptions. This route is harder.\\n\\n[Determined.AI](https://determined.ai/) is an OSS solution\\nfor managing on-prem and cloud clusters. They offer cluster management,\\ndistributed training, and more. It\\'s pretty easy to use and is in active\\ndevelopment.\\n\\nWith all this said, **there is still room to improve the ease of\\nexperience for launching training on many cloud providers**.\\n\\n## 7 - Experiment and Model Management\\n\\n![](./media/image4.png)',\n",
       "  'In contrast to compute, **experiment management is quite close to being\\nsolved**. Experiment management refers to tools and processes that help\\nus keep track of code, model parameters, and data sets that are iterated\\non during the model development lifecycle. Such tools are essential to\\neffective model development. There are several solutions here:\\n\\n-   [TensorBoard](https://www.tensorflow.org/tensorboard):\\nA non-exclusive Google solution effective at one-off experiment\\ntracking. It is difficult to manage many experiments.\\n\\n-   [MLflow](https://mlflow.org/): A non-exclusive\\nDatabricks project that includes model packaging and more, in\\naddition to experiment management. It must be self-hosted.\\n\\n-   [Weights and Biases](https://wandb.ai/site): An\\neasy-to-use solution that is free for personal and academic projects! Logging\\nstarts simply with an \"experiment config\" command.',\n",
       "  '-   Other options include [Neptune\\nAI](https://neptune.ai/), [Comet\\nML](https://www.comet.ml/), and [Determined\\nAI](https://determined.ai/), all of which have solid\\nexperiment tracking options.\\n\\nMany of these platforms also offer **intelligent hyperparameter\\noptimization**, which allows us to control the cost of searching for the\\nright parameters for a model. For example, Weights and Biases has a\\nproduct called [Sweeps](https://wandb.ai/site/sweeps) that\\nhelps with hyperparameter optimization. It\\'s best to have it as part of\\nyour regular ML training tool; there\\'s no need for a dedicated tool.\\n\\n## 8 - \"All-In-One\"\\n\\n![](./media/image13.png)',\n",
       "  'There are machine learning infrastructure solutions that offer\\neverything\\\\--training, experiment tracking, scaling out, deployment,\\netc. These \"all-in-one\" platforms simplify things but don\\'t come cheap!\\nExamples include [Gradient by\\nPaperspace](https://www.paperspace.com/gradient), [Domino\\nData Lab](https://www.dominodatalab.com/), [AWS\\nSagemaker](https://aws.amazon.com/sagemaker/), etc.'],\n",
       " 'https://fullstackdeeplearning.com/course/2022/lecture-7-foundation-models': ['---\\ndescription: How to continuously improve models in production\\n---\\n\\n# Lecture 6: Continual Learning\\n\\n<div align=\"center\">\\n<iframe width=\"720\" height=\"405\" src=\"https://www.youtube-nocookie.com/embed/nra0Tt3a-Oc?list=PL1T8fO7ArWleMMI8KPJ_5D5XSlovTW_Ur\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\\n</div>\\n\\nLecture by [Josh Tobin](https://twitter.com/josh_tobin_).\\nNotes by [James Le](https://twitter.com/le_james94) and [Vishnu Rachakonda](https://www.linkedin.com/in/vrachakonda/).<br />\\nPublished September 12, 2022.\\n[Download slides](https://fsdl.me/2022-lecture-06-slides).\\n\\n## 1 - Overview',\n",
       "  'The core justification for continual learning is that, unlike in\\nacademia, we never deal with static data distributions in the real\\nworld. The implication is that: **if you want to use ML in production\\nand build ML-powered products, you need to think about your goal of\\nbuilding a continual learning system, not just a static model**.\\n\\nRecalling the data flywheel that we\\'ve described in this class before:\\nas you get more users, those users bring more data. You can use the data\\nto make a better model. A better model helps you attract even more users\\nand build a better model over time. Andrej Karpathy described the most\\noptimistic version of it as \"[Operation\\nVacation](https://www.youtube.com/watch?v=hx7BXih7zx8)\" -\\nif we make our continual learning system good enough, it\\'ll get better\\non its own over time, and ML engineers can just go on vacation.\\n\\n![](./media/image6.png)',\n",
       "  \"The reality is quite different. Initially, we gather, clean, and label\\nsome data. We train a model on that data. Then we evaluate the model and\\nloop back to training the model to improve it based on our evaluations.\\nFinally, we get a minimum viable model and deploy it into production.\\n\\n![](./media/image1.png)\\n\\nThe problem begins after we deploy the model: we generally don't have a\\ngreat way of measuring how our models are actually performing in\\nproduction. Often, we just spot-check some predictions to see if they\\nare doing what they are supposed to do. If it seems to work, then it's\\ngreat. We move on to work on other things.\\n\\n![](./media/image8.png)\\n\\nUnfortunately, the ML engineer is probably not the one who discovers the\\nproblems, to begin with. Some business user or product manager gets\\ncomplaints from users about a dipping metric, which leads to an\\ninvestigation. This already costs the company money because the product\\nand business teams must investigate the problem.\",\n",
       "  \"![](./media/image12.png)\\n\\nEventually, they point back to the ML engineer and the model he is\\nresponsible for. At this point, we are stuck on doing ad-hoc analyses\\nbecause we don't know what caused the model failure. Eventually, we can\\nrun a bunch of SQL queries and paste together some Jupyter notebooks to\\nfigure out what the problem is. If we are lucky, we can run an A/B test.\\nIf the test looks good, we'll deploy it into production. Then, we are\\nback to where we started - **not getting ongoing feedback about how the\\nmodel is doing in production**.\\n\\nThe upshot is that **continual learning is the least well-understood\\npart of the production ML lifecycle**. Very few companies are doing this\\nin production today. This lecture focuses on how to improve different\\nsteps of the continual learning process, pointers to learn about each\\nstep, and recommendations for doing it pragmatically and adopting it\\ngradually.\\n\\n## 2 - How to Think About Continual Learning\",\n",
       "  'Our opinionated view about continual learning is **training a sequence\\nof models that can adapt to a continuous stream of data that comes into\\nproduction.** You can think about continual learning as an outer loop in\\nyour training process. On one end of the loop is your application, which\\nconsists of a model and some other code that users interact with that\\napplication by submitting requests, getting predictions back, and\\nsubmitting feedback about how well the model did at providing that\\nprediction.',\n",
       "  \"The continual learning loop starts with **logging**, which is how we get\\nall the data into the loop. Then we have **data curation**, **triggers**\\nfor the retraining process, **dataset formation** to pick the data to\\nretrain on, the **training** process itself, and **offline testing** to\\nvalidate whether the retrained model is good enough to go into\\nproduction. After the model is deployed, we have **online testing**, and\\nthat brings the next version of the model into production, where we can\\nstart the loop all over.\\n\\nEach of these stages passes the output to the next step. Output is\\ndefined by a set of rules. These rules combine to form our **retraining\\nstrategy**. Let's discuss what the retraining strategy looks like for\\neach stage:\\n\\n![](./media/image7.png)\",\n",
       "  'At the **logging** stage, the key question answered by the retraining\\nstrategy is **what data should we store**? At the end of this stage, we\\nhave an \"infinite stream\" of potentially unlabeled data coming from\\nproduction and can be used for downstream analysis.\\n\\n![](./media/image3.png)\\n\\n\\nAt the **curation** stage, the key rules we need to define are **what\\ndata from that infinite stream will we prioritize for labeling and\\npotential retraining?** At the end of this stage, we have a reservoir of\\ncandidate training points that have labels and are fully ready to be fed\\nback into a training process.\\n\\n![](./media/image5.png)\\n\\n\\nAt the **retraining trigger** stage, the key question is **when should\\nwe retrain?** The output of this stage is a signal to kick off a\\nretraining job.\\n\\n![](./media/image2.png)',\n",
       "  'At the **dataset formation** stage, the key rules we need to define are\\n**from this entire reservoir of data, what specific subset of that data\\nare we using to train on for this particular training job?** The output\\nof this stage is a view into that reservoir or training data that\\nspecifies the exact data points to be used for the training job.\\n\\n![](./media/image22.png)\\n\\n\\nAt the **offline testing** stage, the key rule we need to define is\\n**what \"good enough\" looks like for all stakeholders.** The output of\\nthis stage is equivalent to a \"pull request\" report card for your model\\nwith a clear sign-off process. Once you are signed off, the new model\\nwill roll out into production.\\n\\n![](./media/image21.png)\\n\\n\\nFinally, at the **deployment and online testing** stage, the key rule to\\ndefine is **how do we know if this deployment was successful?** The\\noutput of this stage is a signal to roll this model out fully to all of\\nyour users.',\n",
       "  \"In an idealized world, from an ML engineer's perspective, once the model\\nis deployed, the first version of the model is to not retrain the model\\ndirectly. Instead, we want the model to sit on top of the retraining\\nstrategy and try to improve that strategy over time. Rather than\\ntraining models daily, we look at metrics about how well the strategy is\\nworking and how well it's solving the task of improving our model over\\ntime in response to changes in the world. The input that we provide is\\nby tuning the strategy to do a better job of solving that task.\",\n",
       "  \"For most ML engineers, our jobs don't feel like that at a high level.\\n**Our retraining strategy is just retraining models whenever we feel\\nlike it**. We can get good results from ad-hoc retraining, but when you\\nstart getting consistent results and no one is actively working on the\\nmodel day to day anymore, then it's worth starting to add some\\nautomation. Alternatively, if you find yourself needing to retrain the\\nmodel more than once a week (or even more frequently than that) to deal\\nwith changing results in the real world, then it's worth investing in\\nautomation just to save yourself.\\n\\n## 3 - Periodic Retraining\\n\\nThe first baseline retraining strategy that you should consider after\\nyou move on from ad-hoc is just **periodic retraining**:\\n\\n1.  At the logging stage, we simply log everything.\",\n",
       "  \"2.  At the curation stage, we sample uniformly at random from the data\\nthat we've logged up until we get the maximum number of data\\npoints that we are able to handle. Then we label them using some\\nautomated tools.\\n\\n3.  Our retraining trigger will just be periodic.\\n\\n4.  We train once a week, but we do it on the last month's data, for\\nexample.\\n\\n5.  Then we compute the test set accuracy after each training, set a\\nthreshold on that, or more likely manual review the results each\\ntime, and spot-check some of the predictions.\\n\\n6.  When we deploy the model, we do spot evaluations of that deployed\\nmodel on a few individual predictions to make sure things look\\nhealthy.\\n\\n![](./media/image17.png)\\n\\n\\nPeriodic retraining won't work in every circumstance. There are several\\nfailure modes:\",\n",
       "  '1.  The first category is that you have more data than you can log or\\nlabel. If you have a **high volume** of data, you might need to be\\nmore careful about what data to sample and enrich, particularly if\\nthat data comes from **a long-tail distribution** - where you have\\nedge cases that your model needs to perform well on, but those\\nedge cases might not be caught by just doing standard uniform\\nsampling. Or if that data is expensive to label like in a\\n**human-in-the-loop** scenario - where you need custom labeling\\nrules or labeling is a part of the product. In either of those\\ncases, you need to be more careful about what subset of your data\\nyou log and enrich to be used down the road.',\n",
       "  '2.  The second category has to do with **managing the cost of\\nretraining**. If your model is expensive to retrain, retraining it\\nperiodically is not going to be the most cost-efficient way to go,\\nespecially if you do it on a rolling window of data every single\\ntime. You will leave a lot of performance on the table by not\\nretraining more frequently. You can partially solve this by\\nincreasing the retraining frequency, but this will increase the\\ncosts even further.',\n",
       "  \"3.  The final failure mode is situations where you have **a high cost of\\nbad predictions**. Every time you retrain your model, it\\nintroduces risk, which comes from the fact that the data you're\\ntraining the model on might be bad in some way. It might be\\ncorrupted, might have been attacked by an adversary, or might not\\nbe representative anymore of all the cases that your model needs\\nto perform well on. The more frequently you retrain and the more\\nsensitive you are to model failures, the more thoughtful you need\\nto be about careful model evaluation such that you are not unduly\\ntaking on too much risk from frequent retraining.\\n\\n## 4 - Iterating On Your Retraining Strategy\\n\\nThe main takeaway from this section is that **we will use monitoring and\\nobservability to determine what changes we want to make to our\\nretraining strategy**.\\n\\n1.  We'll do that by monitoring just the metrics that actually that\\nmatter and using all other metrics for debugging.\",\n",
       "  \"2.  When we debug an issue with our model, that will lead to potentially\\nretraining our model. But more broadly than that, we can think of\\nit as a change to the retraining strategy - changing our\\nretraining triggers, our offline tests, our sampling strategies,\\nthe metrics for observability, etc.\\n\\n3.  As we get more confident in our monitoring, we can introduce more\\nautomation to our system.\\n\\nThere are no real standards or best practices on model monitoring yet.\\nThe main principles we'll follow are: (1) We'll focus on monitoring what\\nmatters and what breaks empirically; and (2) We'll compute other signals\\ntoo but use them for observability and debugging.\\n\\n![](./media/image13.png)\\n\\n\\nWhat does it mean to monitor a model in production? We think about it\\nas: You have some metric to assess the model quality (i.e, accuracy) and\\na time series of how that metric changes over time. The question you try\\nto answer is: **Is this bad or okay?** Do you need to pay attention to\\nthis degradation or not?\",\n",
       "  \"The questions we'll need to answer are:\\n\\n1.  What metrics should we be looking at when we are monitoring?\\n\\n2.  How can we tell if those metrics are bad and warrant an\\nintervention?\\n\\n3.  What are the tools that help us with this process?\\n\\n### What Metrics to Monitor\\n\\nChoosing the right metric to monitor is probably the most important part\\nof this process. Below you can find different types of metrics ranked in\\norder of how valuable they are.\\n\\n![](./media/image11.png)\\n\\n\\n#### Outcomes and Feedback From Users\\n\\nThe most valuable one to look at is **outcome data or feedback from your\\nusers**. Unfortunately, there are no one-size-fits-all ways to do this\\nbecause it depends a lot on the specifics of the product you are\\nbuilding. This is more of a product management question of how to design\\nyour product in a way that you can capture feedback from your users as\\npart of the product experience.\\n\\n#### Model Performance Metrics\",\n",
       "  \"The next most valuable signal to look at is **model performance\\nmetrics**. These are offline metrics such as accuracy. This is less\\nuseful than user feedback because of loss mismatch. A common experience\\nmany ML practitioners have is that improving model performance leads to\\nthe same or worse outcome. There's very little excuse for not doing\\nthis. To some degree, you can label some production data each day by\\nsetting up an on-call rotation or throwing a labeling party. These\\npractices will give you some sense of how your model performance trends\\nover time.\\n\\n![](./media/image10.png)\\n\\n\\n#### Proxy Metrics\",\n",
       "  'The next best thing to look at is **proxy metrics**, which are\\ncorrelated with bad model performance. These are mostly domain-specific.\\nFor example, if you are building text generation with a language model,\\ntwo examples would be repetitive and toxic outputs. If you are building\\na recommendation system, an example would be the share of personalized\\nresponses. **Edge cases** can be good proxy metrics. If there are\\ncertain problems you know that you have with your model, if those\\nincrease in prevalence, that might mean your model is not doing very\\nwell.',\n",
       "  \"There's an academic direction that aims at being able to take any metric\\nyou care about and approximate it on previously unseen data. How well do\\nwe think our model is doing on this new data? Which would make these\\nproxy metrics a lot more practically useful? There are a number of\\ndifferent approaches here: from training an auxiliary model to predict\\nhow well your main model might do on this offline data, to using\\nheuristics and human-in-the-loop methods.\\n\\n![](./media/image20.png)\\n\\n\\nAn unfortunate result from this literature is that it's not possible to\\nhave a single method you use in all circumstances to approximate how\\nyour model is doing on out-of-distribution data. Let's say you are\\nlooking at the input data to predict how the model will perform on those\\ninput points. Then the label distribution changes. As a result, you\\nwon't be able to take into account that change in your approximate\\nmetric.\\n\\n#### Data Quality\",\n",
       "  \"The next signal to look at is **data quality.** [Data quality\\ntesting](https://lakefs.io/data-quality-testing/) is a set\\nof rules you apply to measure the quality of your data. This deals with\\nquestions such as: How well does a piece of information reflect reality?\\nDoes it fulfill your expectations of what's comprehensive? Is your\\ninformation available when you need it? Some common examples include\\nchecking whether the data has the right schema, the data is in the\\nexpected range, and the number of records is not anomalous.\\n\\n![](./media/image19.png)\\n\\nThis is useful because data problems tend to be the most common issue\\nwith ML models in practice. In [a Google\\nreport](https://www.usenix.org/conference/opml20/presentation/papasian)\\nwhich covered 15 years of different pipeline outages with a particular\\nML model, most of the outages that happened with that model were\\ndistributed systems problems, commonly data problems.\\n\\n#### Distribution Drift\\n\\n##### Why Measure Distribution Drift?\",\n",
       "  \"Your model's performance is only guaranteed on **data sampled from the\\nsame distribution** as it was trained on. This can have a huge impact in\\npractice. A recent example includes changes in model behavior during the\\npandemic. A bug in the retraining pipeline caused the recommendations\\nnot to be updated for new users, leading to millions of dollars in\\nrevenue lost.\\n\\n##### Types of Distribution Drift\\n\\nDistribution drift manifests itself in different ways in the wild:\\n\\n1.  **Instantaneous drift** happens when a model is deployed in a new\\ndomain, a bug is introduced in the pre-processing pipeline, or a\\nbig external shift like COVID occurs.\\n\\n2.  **Gradual drift** happens when users\\\\' preferences change or new\\nconcepts get introduced to the corpus over time.\\n\\n3.  **Periodic drift** happens when users' preferences are seasonal or\\npeople in different time zones use your model differently.\",\n",
       "  '4.  **Temporary drift** happens when a malicious user attacks your\\nmodel, a new user tries your product and churns, or someone uses\\nyour product in an unintended way.\\n\\n##### How to Measure It?\\n\\nHow to tell if your distribution is drifted?\\n\\n1.  Your first **select a window of \"good\" data to serve as a\\nreference**. To select that reference, you can use a fixed window\\nof production data you believe to be healthy. [Some\\npapers](https://arxiv.org/abs/1908.04240) advocate\\nfor using a sliding window of production data. In practice, most\\nof the time you probably should use your validation data as the\\nreference.\\n\\n2.  Once you have that reference data, you **select a new window of\\nproduction data to measure your distribution distance on**. This\\nis not a super principled approach and tends to be\\nproblem-dependent. A pragmatic solution is to pick one or several\\nwindow sizes with a reasonable amount of data and slide them.',\n",
       "  \"3.  Finally, once you have your reference window and production window,\\nyou **compare the windows using a distribution distance metric**.\\n\\n##### What Metrics To Use?\\n\\nLet's start by considering the one-dimensional case, where you have a\\nparticular feature that is one-dimensional and can compute a density of\\nthat feature on your reference/production windows. You want some metric\\nthat approximates the distance between these two distributions.\\n\\n![](./media/image9.png)\\n\\n\\nThere are a few options here:\\n\\n1.  The commonly recommended ones are the KL divergence and the KS test.\\nBut they are actually bad choices.\\n\\n2.  Sometimes-better options would be (1) infinity norm or 1-norm of the\\ndiff between probabilities for each category, and (2)\\nEarth-mover's distance (a more statistically principled approach).\\n\\nCheck out [this Gantry blog\\npost](https://gantry.io/blog/youre-probably-monitoring-your-models-wrong/)\\nto learn more about why the commonly recommended metrics are not so good\\nand the other ones are better.\",\n",
       "  \"##### Dealing with High-Dimensional Data\\n\\nIn the real world for most models, we have potentially many input\\nfeatures or even unstructured data that is very high-dimensional. How do\\nwe deal with detecting distribution drift in those cases?\\n\\n1.  You can measure **drift on all of the features independently**: If\\nyou have a lot of features, you will hit [the multiple hypothesis\\ntesting\\nproblem](https://multithreaded.stitchfix.com/blog/2015/10/15/multiple-hypothesis-testing/).\\nFurthermore, this doesn't capture cross-correlation.\\n\\n2.  You can measure **drift on only the important features**: Generally\\nspeaking, it's a lot more useful to measure drift on the outputs\\nof the model than the inputs. You can also [rank the importance\\nof your input\\nfeatures](https://christophm.github.io/interpretable-ml-book/feature-importance.html)\\nand measure drift on the most important ones.\",\n",
       "  \"3.  You can look at **metrics that natively compute or approximate the\\ndistribution distance between high-dimensional distributions**:\\nThe two that are worth checking out are [maximum mean\\ndiscrepancy](https://jmlr.csail.mit.edu/papers/v13/gretton12a.html)\\nand [approximate Earth-mover's\\ndistance](https://arxiv.org/abs/1904.05877). The\\ncaveat here is that they are pretty hard to interpret.\\n\\n![](./media/image14.png)\\n\\nA more principled way to measure distribution drift for high-dimensional\\ninputs to the model is to use **projections**. The idea of a projection\\nis that:\\n\\n1.  You first take some high-dimensional input to the model and run that\\nthrough a function.\\n\\n2.  Each data point your model makes a prediction on gets tagged by this\\nprojection function. The goal of this projection function is to\\nreduce the dimensionality of that input.\\n\\n3.  Once you've reduced the dimensionality, you can do drift detection\\non that lower-dimensional representation of the high-dimensional\\ndata.\",\n",
       "  \"This approach works for any kind of data, no matter what the\\ndimensionality is or what the data type is. It's also highly flexible.\\nThere are different types of projections that can be useful:\\n**analytical projections** (e.g., mean pixel value, length of sentence,\\nor any other function), **random projections** (e.g., linear), and\\n**statistical projections** (e.g., autoencoder or other density models,\\nT-SNE).\\n\\n##### Cons of Looking at Distribution Drift\\n\\n![](./media/image18.png)\",\n",
       "  \"**Models are designed to be robust to some degree of distribution\\ndrift**. The figure on the left above shows a toy example to demonstrate\\nthis point. We have a classifier that's trained to predict two classes.\\nWe've induced a synthetic distribution shift to shift the red points on\\nthe top left to bottom. These two distributions are extremely different,\\nbut the model performs equally well on the training data and the\\nproduction data. In other words, knowing the distribution shift doesn't\\ntell you how the model has reacted to that shift.\\n\\nThe figure on the right is a research project that used data generated\\nfrom a physics simulator to solve problems on real-world robots. The\\ntraining data was highly out of distribution (low-fidelity, random\\nimages). However, by training on this set of training data, the model\\nwas able to generalize to real-world scenarios on the test data.\",\n",
       "  \"Beyond the theoretical limitations of measuring distribution drift, this\\nis just hard to do in practice. You have to window size correctly. You\\nhave to keep all this data around. You need to choose metrics. You need\\nto define projections to make your data lower-dimensional.\\n\\n#### System Metrics\\n\\nThe last thing to consider looking at is your standard **system\\nmetrics** such as CPU utilization, GPU memory usage, etc. These don't\\ntell you anything about how your model is actually performing, but they\\ncan tell you when something is going wrong.\\n\\n#### Practical Recommendations\\n\\nWe also want to look at how hard it is to compute the aforementioned\\nstages in practice. As seen below, the Y-axis shows the **value** of\\neach signal and the X-axis shows the **feasibility** of measuring each\\nsignal.\\n\\n1.  Measuring outcomes or feedback has pretty wide variability in terms\\nof how feasible it is to do, as it depends on how your product is\\nset up.\",\n",
       "  \"2.  Measuring model performance tends to be the least feasible thing to\\ndo because it involves collecting some labels.\\n\\n3.  Proxy metrics are easier to compute because they don't involve\\nlabels.\\n\\n4.  System metrics and data quality metrics are highly feasible because\\nyou have off-the-shelf tools for them.\\n\\n![](./media/image15.png)\\n\\n\\nHere are our practical recommendations:\\n\\n1.  Basic data quality checks are zero-regret, especially if you are\\nretraining your model.\\n\\n2.  Get some way to measure feedback, model performance, or proxy\\nmetrics, even if it's hacky or not scalable.\\n\\n3.  If your model produces low-dimensional outputs, monitoring those for\\ndistribution shifts is also a good idea.\\n\\n4.  As you evolve your system, practice the **observability** mindset.\",\n",
       "  \"While you can think of monitoring as measuring the known unknowns (e.g.,\\nsetting alerts on a few key metrics), [observability is measuring\\nunknown\\nunknowns](https://www.honeycomb.io/blog/observability-a-manifesto/)\\n(e.g., having the power to ask arbitrary questions about your system\\nwhen it breaks). An observability mindset means two implications:\\n\\n1.  You should keep around the context or raw data that makes up the\\nmetrics that you are computing since you want to be able to drill\\nall the way down to potentially the data points themselves that\\nmake up the degraded metric.\\n\\n2.  You can go crazy with measurement by defining a lot of different\\nmetrics. You shouldn't necessarily set alerts on each of those\\nsince you don't want too many alerts. Drift is a great example\\nsince it is useful for debugging but less so for monitoring.\",\n",
       "  'Finally, it\\'s important to **go beyond aggregate metrics**. If your\\nmodel is 99% accurate in aggregate but only 50% accurate for your most\\nimportant user, is it still \"good\"? The way to deal with this is by\\nflagging important subgroups or cohorts of data and alerting on\\nimportant metrics across them. Some examples are categories you don\\'t\\nwant to be biased against, \"important\" categories of users, and\\ncategories you might expect to perform differently on (languages,\\nregions, etc.).\\n\\n### How To Tell If Those Metrics are \"Bad\"\\n\\nWe don\\'t recommend statistical tests (e.g., KS-Test) because they try to\\nreturn a p-value for the likelihood that the data distributions are not\\nthe same. When you have a lot of data, you will get very small p-values\\nfor small shifts. This is not what we actually care about since models\\nare robust to a small number of distribution shifts.',\n",
       "  'Better options than statistical tests include fixed rules, specific\\nranges, predicted ranges, and unsupervised detection of new patterns.\\n[This article on dynamic data\\ntesting](https://blog.anomalo.com/dynamic-data-testing-f831435dba90?gi=fb4db0e2ecb4)\\nhas the details.\\n\\n![](./media/image16.png)\\n\\n### Tools for Monitoring\\n\\nThe first category is **system monitoring** tools, a premature category\\nwith different companies in it\\n([Datadog](https://www.datadoghq.com/),\\n[Honeycomb](https://www.honeycomb.io/), [New\\nRelic](https://newrelic.com/), [Amazon\\nCloudWatch](https://aws.amazon.com/cloudwatch/), etc.).\\nThey help you detect problems with any software system, not just ML\\nmodels. They provide functionality for setting alarms when things go\\nwrong. Most cloud providers have decent monitoring solutions, but if you\\nwant something better, you can look at monitoring-specific tools to\\nmonitor anything.',\n",
       "  \"This raises the question of whether we should just use these system\\nmonitoring tools to monitor ML metrics as well. [This blog\\npost](https://www.shreya-shankar.com/rethinking-ml-monitoring-3/)\\nexplains that it's feasible but highly painful due to many technical\\nreasons. Thus, it's better to use ML-specific tools.\\n\\nTwo popular open-source monitoring tools are\\n[EvidentlyAI](https://github.com/evidentlyai) and\\n[whylogs](https://github.com/whylabs/whylogs).\\n\\n-   Both are similar in that you provide them with samples of data and\\nthey produce a nice report that tells you where their distribution\\nshifts are.\\n\\n-   The big limitation of both is that they don't solve the data\\ninfrastructure and the scale problem. You still need to be able to\\nget all that data into a place where you can analyze it with these\\ntools.\",\n",
       "  \"-   The main difference between them is that whylogs is more focused on\\ngathering data from the edge by aggregating the data into\\nstatistical profiles at inference time. You don't need to\\ntransport all the data from your inference devices back to your\\ncloud.\\n\\n![](./media/image4.png)\\n\\nLastly, there are a bunch of different SaaS vendors for ML monitoring\\nand observability: [Gantry](https://gantry.io/),\\n[Aporia](https://www.aporia.com/),\\n[Superwise](https://superwise.ai/),\\n[Arize](https://arize.com/),\\n[Fiddler](https://www.fiddler.ai/),\\n[Arthur](https://arthur.ai/), etc.\\n\\n\\n## 5 - Retraining Strategy\\n\\nWe’ve talked about monitoring and observability, which allow you to identify issues with your continual learning system. Now, we’ll talk about how we will fix the various stages of the continual learning process based on what we learn from monitoring and observability.\\n\\n\\n### Logging\",\n",
       "  \"The first stage of the continual learning loop is **logging**. As a reminder, the goal of logging is to get data from your model to a place where you can analyze it. The key question to answer here is: “**what data should I actually log?**”\\n\\nFor most of us, the best answer is just to log all of the data. Storage is cheap. It's better to have data than not to have it. There are, however, some situations where you can't do that. For example, if you have too much traffic going through your model to the point where it's too expensive to log all of it, or if you have data privacy concerns, or if you're running your model at the edge, you simply may not be able to able to log all your data.\",\n",
       "  'In these situations, there are two approaches that you can take. The first approach is **profiling**. With profiling, rather than sending all the data back to your cloud and then using that to monitor, you instead compute **statistical profiles** of your data on the edge that describe the data distribution that you\\'re seeing. This is great from a data security perspective because it doesn\\'t require you to send all the data back home. It minimizes your storage cost. Finally, you don\\'t miss things that happen in the tails, which is an issue for the next approach. That\\'ll describe the place to use. This approach is best used for security-critical applications. Computing statistical profiles is a pretty interesting topic in computer science and data summarization that is worth checking out if you’re interested in this approach. \\n\\n![alt_text](./media/image22.png \"image_tooltip\")',\n",
       "  \"The other approach is **sampling**. With sampling, you'll just take certain data points and send those back to your monitoring and logging system. The advantage of sampling is that it has minimal impact on your inference resources. You don't have to actually spend the computational budget to compute profiles. You also get to have access to the raw data for debugging and retraining, albeit a smaller amount. This is the approach we recommend for any other kind of application.\\n\\n\\n### Curation\\n\\nThe next step in the continual learning loop is **curation**. The goal of curation is to take the infinite stream of production data, which is potentially unlabeled, and turn it into a finite reservoir of enriched data suitable for training. Here, we must answer, “**what data should be enriched?**”\",\n",
       "  'You could **sample and enrich data randomly**, but that may not prove helpful to your model. Importantly, you miss rare classes or events. A better approach can be to perform **stratified subsampling**, wherein you sample specific proportions of individuals from various subpopulations (e.g. race). The most advanced strategy for picking data to enrich is to **curate data points** that are somehow interesting for the purpose of improving your model. \\n\\nThere are a few different ways of doing this: **user-driven curation loops** via feedback loops, **manual curation** via error analysis, and **automatic curation** via active learning. \\n\\nUser-driven curation is a great approach that is easy to implement, assuming you have a clear way of gathering user feedback. If your user churns, clicks thumbs down, or performs some other similar activity on the model’s output, you have an easy way of understanding data that could be enriched for future training jobs.',\n",
       "  '![alt_text](./media/image23.png \"image_tooltip\")\\n\\nIf you don\\'t have user feedback, or if you need even more ways of gathering interesting data from your system, the second most effective way is by doing **manual error analysis**. In this approach, we look at the errors that our model is making, reason about the different types of failure modes that we\\'re seeing, and try to write functions or rules that help capture these error modes. We\\'ll use those functions to gather more data that might represent those error cases. Some examples of these function-based approaches are **similarity-based curation**, which uses nearest neighbors, and **projection-based curation**, wherein we train a new function or model to recognize key data points.',\n",
       "  'The last way to curate data is to do so automatically using a class of algorithms called **[active learning](https://lilianweng.github.io/posts/2022-02-20-active-learning/)**. The way active learning works is that, given a large amount of unlabeled data, we will try to determine which data points would improve model performance the most (if you were to label those data points next and train on them). These algorithms define **a sampling strategy**, rank all of your unlabeled examples using **a scoring function** that defines the sampling strategy, and mark the data points with the highest scores for future labeling. \\n\\nThere are a number of different scoring function approaches that are shown below.',\n",
       "  '1. **Most uncertain**: sample low-confidence and high-entropy predictions or predictions that an ensemble disagrees on.\\n2. **Highest predicted loss**: train a separate model that predicts loss on unlabeled points, then sample the highest predicted loss.\\n3. **Most different from labels**: train a model to distinguish labeled and unlabeled data, then sample the easiest to distinguish.\\n4. **Most representative**: choose points such that no data is too far away from anything we sampled.\\n5. **Big impact on training**: choose points such that the expected gradient is large or points where the model changes its mind the most about its prediction during training.\\n\\nUncertainty scoring tends to be the most commonly used method since it is simple and easy to implement.',\n",
       "  'You might have noticed that there\\'s a lot of similarity between some of the ways that we do data curation and the way that we do monitoring. That\\'s no coincidence--**monitoring and data curation are two sides of the same coin!** They\\'re both interested in solving the problem of finding data points where the model may not be performing well or where we\\'re uncertain about how the model is performing on those data points.\\n\\n![alt_text](./media/image24.png \"image_tooltip\")\\n\\nSome examples of people practically applying data curation are OpenAI’s DALL-E 2, which uses [active learning and manual curation](https://openai.com/blog/dall-e-2-pre-training-mitigations/), Tesla, which uses [feedback loops and manual curation](https://www.youtube.com/watch?v=hx7BXih7zx8), and Cruise, which uses feedback loops.\\n\\nSome tools that help with data curation are [Scale Nucleus](https://scale.com/nucleus), [Aquarium](https://www.aquariumlearning.com/), and [Gantry](https://gantry.io/).',\n",
       "  \"To summarize then, here are our final set of recommendations for applying data curation.\\n\\n\\n\\n1. Random sampling is a fine starting point. If you want to avoid bias or have rare classes, do stratified sampling instead.\\n2. If you have a feedback loop, then user-driven curation is a no-brainer. If not, confidence-based active learning is easy to implement.\\n3. As your model performance increases, you’ll have to look harder for challenging training points. Manual techniques are unavoidable and should be embraced. Know your data!\\n\\n\\n### Retraining Triggers\\n\\nAfter we've curated our infinite stream of unlabeled data down to a reservoir of labeled data that's ready to potentially train on, the next thing that we'll need to decide is “**what trigger are we gonna use to retrain?**”\",\n",
       "  \"The main takeaway here is that moving to automated retraining is **not** always necessary. In many cases, just manually retraining is good enough. It can save you time and lead to better model performance. It's worth understanding when it makes sense to actually make the harder move to automated retraining.\\n\\nThe main prerequisite for moving to automated retraining is being able to reproduce model performance when retraining in a fairly automated fashion. If you're able to do that and you are not really working on the model actively, it's probably worth implementing some automated retraining. As a rule of thumb, if you’re retraining the model more than once a month, automated retraining may make sense.\\n\\nWhen it's time to move to automated training, the main recommendation is to just keep it simple and **retrain periodically**, e.g. once a week. The main question though is, how do you pick the right training schedule? The recommendation here is to:\",\n",
       "  '1. Apply measurement to figure out a reasonable retraining schedule.\\n2. Plot your model performance and degradation over time.\\n3. Compare how retraining the model at various intervals would have resulted in improvements to its performance.\\n\\nAs seen below, the area between the curves represents the opportunity cost, so always remember to balance the upside of retraining with the operational costs of retraining.\\n\\n![alt_text](./media/image25.png \"image_tooltip\")\\n\\nThis is a great area for future academic research! More specifically, we can look at ways to automate determining the optimal retraining strategy based on performance decay, sensitivity to performance, operational costs, and retraining costs.',\n",
       "  \"An additional option for retraining, rather than time-based intervals, is **performance triggers** (e.g. retrain when the model accuracy dips below 90%). This helps react more quickly to unexpected changes and is more cost-optimal, but requires very good instrumentation to process these signals along with operational complexity.\\n\\nAn idea that probably won't be relevant but is worth thinking about is **online learning**. In this paradigm, you train on every single data point as it comes in. It's not very commonly used in practice.\",\n",
       "  \"A version of this idea that is used fairly frequently in practice is **online adaptation**. This method operates not at the level of retraining the whole model itself but rather on the level of adapting the policy that sits on top of the model. What is a policy you ask? A policy is the set of rules that takes the raw prediction that the model made, like the score or the raw output of the model, and turns it into the output the user sees. In online adaptation, we use algorithms like multi-armed bandits to tune these policies. If your data changes very frequently, it is worth looking into this method.\\n\\n\\n### Dataset Formation\\n\\nImagine we've fired off a trigger to start a new training job. The next question we need to answer is, among all of the labeled data in our reservoir of data, **what specific data points should we train on for this particular new training job?**\",\n",
       "  'We have four options here. Most of the time in deep learning, we\\'ll just use the first option and **train on all the data that we have available** to us. Remember to keep your data version controlled and your curation rules consistent.\\n\\n![alt_text](./media/image26.png \"image_tooltip\")\\n\\nIf you have too much data to do that, you can use recency as a heuristic for a second option and **train on only a sliding window of the most recent data** (if recency is important) or **sample a smaller portion** (if recency isn’t). In the latter case, compare the aggregate statistics between the old and new windows to ensure there aren’t any bugs. It’s also important in both cases to compare the old and new datasets as they may not be related in straightforward ways. \\n\\n![alt_text](./media/image27.png \"image_tooltip\")',\n",
       "  'A useful third option is **online batch selection**, which can be used when recency doesn’t quite matter. In this method, we leverage label-aware selection functions to choose which items in mini-batches to train on. \\n\\n![alt_text](./media/image28.png \"image_tooltip\")\\n\\nA more difficult fourth option that isn’t quite recommended is **continual fine-tuning**. Rather than retraining from scratch every single time, you train your existing model on just new data. The reason why you might wanna do this primarily is because it\\'s much more cost-effective. The paper below shares some findings from GrubHub, where they found a 45x cost improvement by doing this technique relative to sliding windows.\\n\\n![alt_text](./media/image29.png \"image_tooltip\")',\n",
       "  \"The big challenge here is that unless you're very careful, it's easy for the model to forget what it learned in the past. The upshot is that you need to have mature evaluation practices to be very careful that your model is performing well on all the types of data that it needs to perform well on.\\n\\n\\n### Offline Testing\\n\\nAfter the previous steps, we now have a new candidate model that we think is ready to go into production. The next step is to test that model. The goal of this stage is to produce a report that our team can sign off on that answers the question of whether this new model is good enough or whether it's better than the old model. The key question here is, “**what should go into that report?**”\",\n",
       "  'This is a place where there\\'s not a whole lot of standardization, but the recommendation we have here is to compare your current model with the previous version of the model on all of the metrics that you care about, all of the subsets of data that you\\'ve flagged are important, and all the edge cases you’ve defined. Remember to adjust the comparison to account for any sampling bias.\\n\\nBelow is a sample comparison report. Note how the validation set is broken out into concrete subgroups. Note also how there are specific validation sets assigned to common error cases.\\n\\n![alt_text](./media/image30.png \"image_tooltip\")\\n\\nIn continual learning, evaluation sets are dynamically refined just as much as training sets are. Here are some guidelines for how to manage evaluation sets in a continual learning system:',\n",
       "  '1. As you curate new data, add some of it to your evaluation sets. For example, if you change how you do sampling, add that newly sampled data to your evaluation set. Or if you encounter a new edge case, create a test case for it.\\n2. Corollary 1: you should version control your evaluation sets as well.\\n3. Corollary 2: if your data changes quickly, always hold out the most recent data for evaluation.\\n\\nOnce you have the testing basics in place, a more advanced option that you can look into here is **expectation testing**. Expectation tests work by taking pairs of examples where you know the relationship between the two. These tests help a lot with understanding the generalizability of models.\\n\\n![alt_text](./media/image31.png \"image_tooltip\")',\n",
       "  'Just like how data curation is highly analogous to monitoring, so is offline testing. We want to observe our metrics, not just in aggregate but also across all of our important subsets of data and across all of our edge cases. One difference between these two is that **you will have different metrics available in offline testing and online testing**. For example, you’re much more likely to have labels offline. Online, you’re much more likely to have feedback. We look forward to more research that can predict online metrics from offline ones.\\n\\n\\n### Online Testing\\n\\nMuch of this we covered in the last lecture, so we’ll keep it brief! Use shadow mode and A/B tests, roll out models gradually, and roll back models if you see issues during rollout. \\n\\n\\n## 6 - The Continual Improvement Workflow',\n",
       "  'To tie it all together, we’ll conclude with an example. Monitoring and continual learning are two sides of the same coin. We should be using the signals that we monitor to very directly change our retraining strategy. This section describes the future state that comes as a result of investing in the steps laid out previously. \\n\\nStart with a place to store and version your strategy. The components of your continual learning strategy should include the following:\\n\\n\\n\\n* Inputs, predictions, user feedback, and labels.\\n* Metric definitions for monitoring, observability, and offline testing.\\n* Projection definitions for monitoring and manual data curation.\\n* Subgroups and cohorts of interest for monitoring and offline testing.\\n* Data curation logic.\\n* Datasets for training and evaluation.\\n* Model comparison reports.\\n\\nWalk through this example to understand how changes to the retraining strategy occur as issues surface in our machine learning system.\\n\\n![alt_text](./media/image32.png \"image_tooltip\")',\n",
       "  '## 7 - Takeaways\\n\\nTo summarize, continual learning is a nascent, poorly understood topic that is worth continuing to pay attention to. Watch this space! In this lecture, we focused on all the steps and techniques that allow you to use retraining effectively. As MLEs, leverage monitoring to strategically improve your model. Always start simple, and get better!'],\n",
       " 'https://fullstackdeeplearning.com/course/2022/lecture-8-teams-and-pm': ['---\\ndescription: Building on Transformers, GPT-3, CLIP, StableDiffusion, and other Large Models.\\n---\\n\\n# Lecture 7: Foundation Models\\n\\n<div align=\"center\">\\n<iframe width=\"720\" height=\"405\" src=\"https://www.youtube-nocookie.com/embed/Rm11UeGwGgk?list=PL1T8fO7ArWleMMI8KPJ_5D5XSlovTW_Ur\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\\n</div>\\n\\nLecture by [Sergey Karayev](https://twitter.com/sergeykarayev).\\nNotes by [James Le](https://twitter.com/le_james94) and [Vishnu Rachakonda](https://www.linkedin.com/in/vrachakonda/).<br />\\nPublished September 19, 2022.\\n[Download slides](https://fsdl.me/2022-lecture-07-slides).\\n\\nFoundation models are very large models trained on very large datasets that\\ncan be used for multiple downstream tasks.',\n",
       "  'We’ll talk about fine-tuning, Transformers, large language models, prompt engineering, other applications of large models, and vision and text-based models like CLIP and image generation.\\n\\n![alt_text](media/image-1.png \"image_tooltip\")\\n\\n## 1 - Fine-Tuning\\n\\nTraditional ML uses a lot of data and a large model, which takes a long time. But if you have a small amount of data, you can use **transfer learning** to benefit from the training on a lot of data. You basically use the same model that you have pre-trained, add a few layers, and unlock some weights.\\n\\nWe have been doing this in computer vision since 2014. Usually, you train a model on ImageNet, keep most of the layers, and replace the top three or so layers with newly learned weights. Model Zoos are full of these models like AlexNet, ResNet, etc. in both TensorFlow and PyTorch.',\n",
       "  'In NLP, pre-training was initially limited only to the first step: word embeddings. The input to a language model is words. One way you can encode them to be a vector (instead of a word) is **one-hot encoding**. Given a large matrix of words, you can make an embedding matrix and embed each word into a real-valued vector space. This new matrix is down to the dimension on the order of a thousand magnitude. Maybe those dimensions correspond to some semantic notion.\\n\\n![alt_text](media/image-2.png \"image_tooltip\")\\n\\n\\n[Word2Vec](https://jalammar.github.io/illustrated-word2vec/) trained a model like this in 2013. It looked at which words frequently co-occur together. The learning objective was to maximize cosine similarity between their embeddings. It could do cool demos of vector math on these embeddings. For example, when you embed the words “king,” “man,” and “woman,” you can do vector math to get a vector that is close to the word “queen” in this embedding space.',\n",
       "  'It’s useful to see more context to embed words correctly because words can play different roles in the sentence (depending on their context). If you do this, you’ll improve accuracy on all downstream tasks. In 2018, a number of models such as ELMO and ULMFit [published pre-trained LSTM-based models that set state-of-the-art results on most NLP tasks](https://ruder.io/nlp-imagenet/).\\n\\nBut if you look at the model zoos today, you won’t see any LSTMs. You’ll only see Transformers everywhere. What are they?\\n\\n\\n## 2 - Transformers\\n\\nTransformers come from a paper called “[Attention Is All You Need](https://arxiv.org/abs/1706.03762)” in 2017, which introduced a groundbreaking architecture that sets state-of-the-art results on translation first and a bunch of NLP tasks later.\\n\\n![alt_text](media/image-3.png \"image_tooltip\")\\n\\n\\nIt has a decoder and an encoder. For simplicity, let’s take a look at the encoder. The interesting components here are self-attention, positional encoding, and layer normalization.',\n",
       "  '### Self-Attention\\n\\n![alt_text](media/image-4.png \"image_tooltip\")\\n\\n\\nBasic self-attention follows: Given an input sequence of vectors x of size t, we will produce an output sequence of tensors of size t. Each tensor is a weighted sum of the input sequence. The weight here is just a dot product of the input vectors. All we have to do is to make that weighted vector sum to 1. We can represent it visually, as seen below. The input is a sentence in English, while the output is a translation in French.\\n\\n![alt_text](media/image-5.png \"image_tooltip\")',\n",
       "  'So far, there are no learned weights and no sequence order. Let’s learn some weights!* If we look at the input vectors, we use them in three ways: as **queries** to compare two other input vectors, as **keys** to compare them to input vectors and produce the corresponding output vector, and as **values **to sum up all the input vectors and produce the output vector.\\n* We can process each input vector with three different matrices to fulfill these roles of query, key, and value. We will have three weighted matrices, and everything else remains the same. If we learn these matrices, we learn attention.\\n* It’s called **multi-head attention **because we learn different sets of weighted matrices simultaneously, but we implement them as just a single matrix.\\n\\nSo far, we have learned the query, key, and value. Now we need to introduce some notion of order to the sequence by encoding each vector with its position. This is called **positional encoding**.\\n\\n\\n### Positional Encoding',\n",
       "  '![alt_text](media/image-6.png \"image_tooltip\")\\n\\n\\nLet’s say we have an input sequence of words\\n\\n]* The first step is to embed the words into a dense, real-valued word embedding. This part can be learned.\\n* However, there is no order to that embedding. Thus, we will add another embedding that only encodes the position.\\n* In brief, the first embedding encodes only the content, while the second embedding encodes only the position. If you add them, you now have information about both the content and the position.\\n\\n\\n### Layer Normalization\\n\\n![alt_text](media/image-7.png \"image_tooltip\")\\n\\n\\nNeural network layers work best when the input vectors have uniform mean and standard deviation in each dimension. As activations flow through the network, the means and standard deviations get blown out by the weight matrices. [Layer normalization](https://arxiv.org/pdf/1803.08494.pdf) is a hack to re-normalize every activation to where we want them between each layer.',\n",
       "  'That’s it! All the amazing results you’ll see from now on are just increasingly large Transformers with dozens of layers, dozens of heads within each layer, large embedding dimensions, etc. The fundamentals are the same. It’s just the Transformer model.\\n\\n[Anthropic](https://www.anthropic.com/) has been publishing great work lately to investigate why Transformers work so well. Check out these publications:\\n\\n1. [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html)\\n2. [In-Context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)\\n3. [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html)\\n\\n\\n## 3 - Large Language Models\\n\\n\\n### Models',\n",
       "  'GPT and [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) came out in 2018 and 2019, respectively. The name means “generative pre-trained Transformers.” They are decoder-only models and use masked self-attention. This means: At a poi that at the output sequence, you can only attend to two input sequence vectors that came before that point in the sequence.\\n\\n![alt_text](media/image-8.png \"image_tooltip\")\\n\\n\\nThese models were trained on 8 million web pages. The largest model has 1.5 billion parameters. The task that GPT-2 was trained on is predicting the next word in all of this text on the web. They found that it works increasingly well with an increasing number of parameters.\\n\\n![alt_text](media/image-9.png \"image_tooltip\")',\n",
       "  '[BERT](https://arxiv.org/abs/1810.04805) came out around the same time as Bidirectional Encoder Representations for Transformers. It is encoder-only and does not do attention masking. It has 110 million parameters. During training, BERT masks out random words in a sequence and has to predict whatever the masked word is.\\n\\n![alt_text](media/image-10.png \"image_tooltip\")\\n\\n\\n[T5 (Text-to-Text Transformer)](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) came out in 2020. The input and output are both text strings, so you can specify the task that the model supposes to be doing. T5 has an encoder-decoder architecture. It was trained on the C4 dataset (Colossal Clean Crawled Corpus), which is 100x larger than Wikipedia. It has around 10 billion parameters. You can download [the open-sourced model](https://github.com/google-research/text-to-text-transfer-transformer#released-model-checkpoints) and run it on your machine.',\n",
       "  '[GPT-3](https://openai.com/blog/gpt-3-apps/) was one of the state-of-the-art models in 2020. It was 100x larger than GPT/GPT-2 with 175 billion parameters. Because of its size, GPT-3 exhibits unprecedented capabilities of few-shot and zero-shot learning. As seen in the graph below, the more examples you give the model, the better its performance is. The larger the model is, the better its performance is. If a larger model was trained, it would be even better.\\n\\n![alt_text](media/image-11.png \"image_tooltip\")\\n\\n\\nOpenAI also released [Instruct-GPT](https://openai.com/blog/instruction-following/) earlier this year. It had humans rank different GPT-3 outputs and used reinforcement learning to fine-tune the model. Instruct-GPT was much better at following instructions. OpenAI has put this model, titled ‘text-davinci-002,’ in their API. It is unclear how big the model is. It could be ~10x smaller than GPT-3.\\n\\n![alt_text](media/image-12.png \"image_tooltip\")',\n",
       "  'DeepMind released [RETRO (Retrieval-Enhanced Transformers)](https://arxiv.org/pdf/2112.04426.pdf) in 2021. Instead of learning language and memorizing facts in the model’s parameters, why don’t we just learn the language in parameters and retrieve facts from a large database of internal text? To implement RETRO, they encode a bunch of sentences with BERT and store them in a huge database with more than 1 trillion tokens. At inference time, they fetch matching sentences and attend to them. This is a powerful idea because RETRO is connected to an always updated database of facts.\\n\\n![alt_text](media/image-13.png \"image_tooltip\")',\n",
       "  'DeepMind released another model called [Chinchilla](https://gpt3demo.com/apps/chinchilla-deepmind) in 2022, which observed the scaling laws of language models. They [trained over 400 language models](https://arxiv.org/pdf/2203.15556.pdf) from 70 million to 16 billion parameters on 5 billion to 500 billion tokens. They then derived formulas for optimal model and training set size, given a fixed compute budget. They found that most large language models are “undertrained,” meaning they haven’t seen enough data.\\n\\n![alt_text](media/image-14.png \"image_tooltip\")',\n",
       "  'To prove this, they trained a large model called [Gopher](https://gpt3demo.com/apps/deepmind-gopher) with 280 billion parameters and 300 billion tokens. With Chincilla, they reduced the number of parameters to 70 billion and used four times as much data (1.4 trillion tokens). Chinchilla not only matched Gopher’s performance but actually exceeded it. Check out [this LessWrong post](https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications) if you want to read about people’s opinions on it.\\n\\n\\n### Vendors',\n",
       "  'OpenAI offers four model sizes: Ada, Babbage, Curie, and Davinci. [Each has a different price](https://openai.com/api/pricing/) and different capabilities. Most of the impressive GPT-3 results on the Internet came from Davinci. These correspond to 350M, 1.3B, 6.7B, and 175B parameters. You can also fine-tune models for an extra cost. The quota you get when you sign up is pretty small, but you can raise it over time. You have to apply for review before going into production.\\n\\nThere are some alternatives to OpenAI:',\n",
       "  '1. [Cohere AI](https://cohere.ai/) has similar models for [similar prices](https://cohere.ai/pricing).\\n2. [AI21](https://www.ai21.com/) also has some large models.\\n3. There are also open-source large language models, such as [Eleuther GPT-NeoX](https://www.eleuther.ai/projects/gpt-neox/) (20B parameters), [Facebook OPT-175B](https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/) (175B parameters), and [BLOOM from BigScience](https://bigscience.huggingface.co/blog/bloom) (176B parameters). If you want to use one of these open-source models but do not have to be responsible for deploying it, you can use [HuggingFace’s inference API](https://huggingface.co/inference-api).\\n\\n\\n## 4 - Prompt Engineering',\n",
       "  'GPT-3 and other large language models are mostly alien technologies. It’s unclear how they exactly work. People are finding out how they work by playing with them. We will cover some notable examples below. Note that if you play around with them long enough, you are likely to discover something new.\\n\\nGPT-3 is surprisingly bad at reversing words due to **tokenization**: It doesn’t see letters and words as humans do. Instead, it sees “tokens,” which are chunks of characters. Furthermore, it gets confused with long-ish sequences. Finally, it has trouble merging characters. For it to work, you have to teach GPT-3 the algorithm to use to get around its limitations. Take a look at [this example from Peter Welinder](https://twitter.com/npew/status/1525900849888866307).\\n\\n![alt_text](media/image-15.jpg \"image_tooltip\")',\n",
       "  'Another crazy prompt engineering is “Let’s Think Step By Step.” This comes from a paper called “[Large Language Models are Zero-Shot Reasoners](https://arxiv.org/pdf/2205.11916.pdf).” Simply adding “Let’s Think Step By Step” into the prompt increases the accuracy of GPT-3 on one math problem dataset from 17% to 78% and another math problem dataset from 10% to 40%.\\n\\n![alt_text](media/image-16.png \"image_tooltip\")\\n\\n\\nAnother unintuitive thing is that the context length of GPT is long. You can give it a **long instruction** and it can return the desired output. [This example](https://twitter.com/goodside/status/1557381916109701120) shows how GPT can output a CSV file and write the Python code as stated. You can also use **formatting tricks **to reduce the training cost, as you can do multiple tasks per call. Take a look at [this example](https://twitter.com/goodside/status/1561569870822653952) for inspiration.',\n",
       "  'We have to be careful since our models might get pwnage or possessed. User input in the prompt may instruct the model to do something naughty. This input can even reveal your prompt to [prompt injection attacks](https://simonwillison.net/2022/Sep/12/prompt-injection/) and [possess your AI](https://twitter.com/goodside/status/1564112369806151680). This actually works in GPT-3-powered production apps.\\n\\n![alt_text](media/image-17.png \"image_tooltip\")\\n\\n\\nFurther work is needed before putting GPT-3-powered apps into production. There are some tools for prompt engineering such as [PromptSource](https://github.com/bigscience-workshop/promptsource) and [OpenPrompt](https://github.com/thunlp/OpenPrompt), but we definitely need better tools.\\n\\n\\n## 5 - Other Applications\\n\\n\\n### Code\\n\\n![alt_text](media/image-18.png \"image_tooltip\")',\n",
       "  'One notable application of large foundation models is **code generation**. With a 40- billion-parameter Transformer model pre-trained on all the Github code it could find, [DeepMind Alphacode](https://www.deepmind.com/blog/competitive-programming-with-alphacode) was able to achieve an above-average score on the Codeforce competition. To do this, they used a model to generate a large set of potential solutions and another model to winnow down the options by actually executing them. \\n\\nThe general idea to highlight from this is **filtering the outputs of a model**. You can have a separate model that does filtering, or you can have some kind of verification + validation process. This can really significantly boost accuracy. OpenAI demonstrates impressive results on [different math word problems](https://openai.com/blog/grade-school-math/), as seen below.\\n\\n![alt_text](media/image-19.png \"image_tooltip\")',\n",
       "  'Code generation has moved into products of late, like [Github Copilot](https://github.blog/2022-06-21-github-copilot-is-generally-available-to-all-developers/). We highly recommend trying it out! Another option for a similar tool is [replit’s new tool for coding](https://blog.replit.com/ai). \\n\\nWe’re just getting started with the applications of foundation models to the programming workflow. In fact, things are about to start getting really wild. [A recent paper](https://arxiv.org/pdf/2207.14502.pdf) showed that a large language model that generated its own synthetic puzzles to learn to code could improve significantly. **Models are teaching themselves to get better!**\\n\\n![alt_text](media/image-20.png \"image_tooltip\")',\n",
       "  'Playing around with systems like GPT-3 and their ability to generate code can feel quite remarkable! Check out some fun experiments Sergey ran ([here](https://twitter.com/sergeykarayev/status/1569377881440276481) and [here](https://twitter.com/sergeykarayev/status/1570848080941154304)).\\n\\n![alt_text](media/image-21.jpg \"image_tooltip\")\\n\\n### Semantic Search\\n\\n**Semantic search** is another interesting application area. If you have texts like words, sentences, paragraphs, or even whole documents, you can embed that text with large language models to get vectors. If you have queries in sentences or paragraphs, you can also embed them in the same way. With this function, you can generate embeddings and easily find semantic overlap by examining the cosine similarity between embedding vectors.\\n\\n![alt_text](media/image-22.png \"image_tooltip\")',\n",
       "  'Implementing this semantic search is hard. Computations on large, dense vectors with float data types are intensive. Companies like Google and Facebook that use this approach have developed libraries like [FAISS](https://towardsdatascience.com/using-faiss-to-search-in-multidimensional-spaces-ccc80fcbf949) and [ScaNN](https://cloud.google.com/blog/topics/developers-practitioners/find-anything-blazingly-fast-googles-vector-search-technology) to solve the challenges of implementing semantic search. \\n\\nSome open-source options for this include [Haystack from DeepSet](https://www.deepset.ai/haystack) and [Jina.AI](https://github.com/jina-ai/jina). Other vendor options include [Pinecone](https://www.pinecone.io/), [Weaviate](https://weaviate.io/), [Milvus](https://milvus.io/), [Qdrant](https://qdrant.tech/), [Google Vector AI Matching Engine](https://cloud.google.com/vertex-ai/docs/matching-engine/overview), etc.\\n\\n\\n### Going Cross-Modal',\n",
       "  'Newer models are bridging the gap between data modalities (e.g. using both vision and text). One such model is [the Flamingo model](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/tackling-multiple-tasks-with-a-single-visual-language-model/flamingo.pdf), which uses a special model component called a **perceiver resampler** (an attention module that translates images into fixed-length sequences of tokens).\\n\\n![alt_text](media/image-23.png \"image_tooltip\")\\n\\n\\nAnother paper about [Socratic Models](https://socraticmodels.github.io/) was recently published. The author trained several large models (a vision model, a language model, and an audio model) that are able to interface with each other using language prompts to perform new tasks.',\n",
       "  'Finally, the concept of “Foundation Models” came from the paper “[On the Opportunities and Risks of Foundation Models](https://arxiv.org/abs/2108.07258)” by researchers at Stanford Institute for Human-Centered AI. We think “Large Language Models” or “Large Neural Networks” might be more useful terms.\\n\\n\\n## 6 - CLIP and Image Generation\\n\\nNow, let\\'s talk about some of the most exciting applications of this kind of model: in vision!\\n\\nIn a 2021 OpenAI paper called “[Learning transferrable visual models from natural language supervision](https://arxiv.org/abs/2103.00020)”, **CLIP (Contrastive Language–Image Pre-training)** was introduced. In this paper, the authors encode text via Transforms, encode images via ResNets or Visual Transformers, and apply contrastive training to train the model. Contrastive training matches correct image and text pairs using cosine similarity. The code for this is tremendously simple!\\n\\n![alt_text](media/image-24.png \"image_tooltip\")',\n",
       "  'With this powerful trained model, you can map images and text using embeddings, even on unseen data. There are two ways of doing this. One is to use a “linear probe” by training a simple logistic regression model on top of the features CLIP outputs after performing inference. Otherwise, you can use a “zero-shot” technique that encodes all the text labels and compares them to the encoded image. Zero-shot tends to be better, but not always.\\n\\nSince OpenAI CLIP was released in an open-source format, there have been many attempts to improve it, including [the OpenCLIP model](https://github.com/mlfoundations/open_clip), which actually outperforms CLIP.',\n",
       "  'To clarify, CLIP doesn’t go directly from image to text or vice versa. It uses embeddings. This  embedding space, however, is super helpful for actually performing searches across modalities. This goes back to our section on vector search. There are so many cool projects that have come out of these efforts! (like [this](https://rom1504.github.io/clip-retrieval/) and [this](https://github.com/haltakov/natural-language-image-search))\\n\\nTo help develop mental models for these operations, consider how to actual perform **image captioning** (image -> text) and image generation (text -> image). There are two great examples of this written in [the ClipCap paper](https://arxiv.org/pdf/2111.09734.pdf). At a high level, image captioning is performed through training a separate model to mediate between a frozen CLIP, which generates a series of word embeddings, and a frozen GPT-2, which takes these word embeddings and generates texts.',\n",
       "  'The intermediate model is a Transformer model that gets better at modeling images and captions.\\n\\n![alt_text](media/image-25.png \"image_tooltip\")\\n\\n\\nIn **image generation**, the most well-known approach is taken by [DALL-E 2 or unCLIP](https://cdn.openai.com/papers/dall-e-2.pdf). In this method, two additional components are introduced to a CLIP system, a prior that maps from text embedding to image embeddings and a decoder that maps from image embedding to image. The prior exists to solve the problem that many text captions can accurately work for an image. \\n\\n![alt_text](media/image-26.png \"image_tooltip\")\\n\\n\\nIn DALL-E 2’s case, they use an approach for the prior called **a diffusion model**. [Diffusion models](https://towardsdatascience.com/diffusion-models-made-easy-8414298ce4da) are trained to denoise data effectively through training on incrementally noisy data.\\n\\n![alt_text](media/image-27.png \"image_tooltip\")',\n",
       "  'In DALL-E 2, the diffusion method is applied to the **prior** model, which trains its denoising approach on a sequence of encoded text, CLIP text embedding, the diffusion timestamp, and the noised CLIP embedding, all so it can predict the un-noised CLIP image embedding. In doing so, it helps us bridge the gap between the raw text caption to the model, which can be infinitely complicated and “noisy”, and the CLIP image embedding space.\\n\\n![alt_text](media/image-28.png \"image_tooltip\")\\n\\n\\nThe **decoder** helps us go from the prior’s output of an image embedding to an image. This is a much simpler approach for us to understand. We apply a U-Net structure to a diffusion training process that is able to ultimately “de-noise” the input image embedding and output an image.\\n\\n![alt_text](media/image-29.png \"image_tooltip\")',\n",
       "  'The results of this model are incredible! You can even generate images and merge images using CLIP embeddings. There are all kinds of funky ways of playing with the embeddings to create various image outputs.\\n\\n![alt_text](media/image-30.png \"image_tooltip\")\\n\\n\\nOther models of interest are Parti and StableDiffusion.\\n\\n* Google published [Parti](https://parti.research.google/) soon after DALL-E 2. Parti uses a VQGAN model instead of a diffusion model, where the image is represented as a sequence of high-dimensional tokens).\\n* [StableDiffusion](https://stability.ai/blog/stable-diffusion-public-release) has been released publicly, so definitely [check it out](https://github.com/CompVis/latent-diffusion)! It uses a “latent diffusion” model, which diffuses the image in a low-dimensional latent space and decodes the image back into a pixel space.\\n\\n![alt_text](media/image-31.png \"image_tooltip\")',\n",
       "  'There has been an absolute explosion of these applications. Check out these examples on [image-to-image](https://twitter.com/DiffusionPics/status/1568219366097039361/), [video generation](https://twitter.com/jakedowns/status/1568343105212129280), and [photoshop plugins](https://www.reddit.com/r/StableDiffusion/comments/wyduk1/). The sky is the limit.\\n\\nPrompting these models is interesting and can get pretty involved. Someday this may even be tool and code-based. You can learn from other people on [Lexica](https://lexica.art/) and [promptoMANIA](https://promptomania.com/).\\n\\nIt’s truly a remarkable time to be involved with AI models as they scale to new heights.'],\n",
       " 'https://fullstackdeeplearning.com/course/2022/lecture-9-ethics': ['---\\ndescription: Principles for testing software, tools for testing Python code, practices for debugging models and testing ML\\n---\\n\\n# Lecture 3: Troubleshooting & Testing\\n\\n<div align=\"center\">\\n<iframe width=\"720\" height=\"405\" src=\"https://www.youtube-nocookie.com/embed/RLemHNAO5Lw?list=PL1T8fO7ArWleMMI8KPJ_5D5XSlovTW_Ur\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\\n</div>\\n\\nLecture by [Charles Frye](https://twitter.com/charles_irl).<br />\\nNotes by [James Le](https://twitter.com/le_james94) and [Vishnu Rachakonda](https://www.linkedin.com/in/vrachakonda/).<br />\\nPublished August 22, 2022.\\n[Download slides](https://fsdl.me/2022-lecture-03-slides).\\n\\n## 1 - Testing Software\\n\\n1.  The general approach is that tests will help us ship faster with\\nfewer bugs, but they won\\'t catch all of our bugs.\\n\\n2.  That means we will use testing tools but won\\'t try to achieve 100%\\ncoverage.',\n",
       "  \"3.  Similarly, we will use linting tools to improve the development\\nexperience but leave escape valves rather than pedantically\\nfollowing our style guides.\\n\\n4.  Finally, we'll discuss tools for automating these workflows.\\n\\n### 1.1 - Tests Help Us Ship Faster. They Don't Catch All Bugs\\n\\n![](./media/image1.png)\\n\\n**Tests are code we write that are designed to fail intelligibly when\\nour other code has bugs**. These tests can help catch some bugs before\\nthey are merged into the main product, but they can't catch all bugs.\\nThe main reason is that test suites are not certificates of correctness.\\nIn some formal systems, tests can be proof of code correctness. But we\\nare writing in Python (a loosely goosey language), so all bets are off\\nin terms of code correctness.\",\n",
       "  '[Nelson Elhage](https://twitter.com/nelhage?lang=en)\\nframed test suites more like classifiers. The classification problem is:\\ndoes this commit have a bug, or is it okay? The classifier output is\\nwhether the tests pass or fail. We can then **treat test suites as a\\n\"prediction\" of whether there is a bug**, which suggests a different way\\nof designing our test suites.\\n\\nWhen designing classifiers, we need to trade off detection and false\\nalarms. **If we try to catch all possible bugs, we can inadvertently\\nintroduce false alarms**. The classic signature of a false alarm is a\\nfailed test - followed by a commit that fixes the test rather than the\\ncode.\\n\\nTo avoid introducing too many false alarms, it\\'s useful to ask yourself\\ntwo questions before adding a test:\\n\\n1.  Which real bugs will this test catch?\\n\\n2.  Which false alarms will this test raise?\\n\\nIf you can think of more examples for the second question than the first\\none, maybe you should reconsider whether you need this test.',\n",
       "  \"One caveat is that: **in some settings, correctness is important**.\\nExamples include medical diagnostics/intervention, self-driving\\nvehicles, and banking/finance. A pattern immediately arises here: If you\\nare operating in a high-stakes situation where errors have consequences\\nfor people's lives and livelihoods, even if it's not regulated yet, it\\nmight be regulated soon. These are examples of **low-feasibility,\\nhigh-impact ML projects** discussed in the first lecture.\\n\\n![](./media/image19.png)\\n\\n\\n### 1.2 - Use Testing Tools, But Don't Chase Coverage\\n\\n-   *[Pytest](https://docs.pytest.org/)* is the standard\\ntool for testing Python code. It has a Pythonic implementation and\\npowerful features such as creating separate suites, sharing\\nresources across tests, and running parametrized variations of\\ntests.\",\n",
       "  \"-   Pure text docs can't be checked for correctness automatically, so\\nthey are hard to maintain or trust. Python has a nice module,\\n[*[doctests]*](https://docs.python.org/3/library/doctest.html),\\nfor checking code in the documentation and preventing rot.\\n\\n-   Notebooks help connect rich media (charts, images, and web pages)\\nwith code execution. A cheap and dirty solution to test notebooks\\nis adding some *asserts* and using *nbformat* to run the\\nnotebooks.\\n\\n![](./media/image17.png)\",\n",
       "  'Once you start adding different types of tests and your codebase grows,\\nyou will want coverage tools for recording which code is checked or\\n\"covered\" by tests. Typically, this is done in lines of code, but some\\ntools can be more fine-grained. We recommend\\n[Codecov](https://about.codecov.io/), which generates nice\\nvisualizations you can use to drill down and get a high-level overview\\nof the current state of your testing. Codecov helps you understand your\\ntests and can be incorporated into your testing. You can say you want to\\nreject commits not only where tests fail, but also where test coverage\\ngoes down below a certain threshold.',\n",
       "  'However, we recommend against that. Personal experience, interviews, and\\npublished research suggest that only a small fraction of the tests you\\nwrite will generate most of your value. **The right tactic,\\nengineering-wise, is to expand the limited engineering effort we have on\\nthe highest-impact tests and ensure that those are super high quality**.\\nIf you set a coverage target, you will instead write tests in order to\\nmeet that coverage target (regardless of their quality). You end up\\nspending more effort to write tests and deal with their low quality.\\n\\n![](./media/image16.png)\\n\\n\\n### 1.3 - Use Linting Tools, But Leave Escape Valves\\n\\n**Clean code is of uniform and standard style**.',\n",
       "  \"1.  Uniform style helps avoid spending engineering time on arguments\\nover style in pull requests and code review. It also helps improve\\nthe utility of our version control by cutting down on noisy\\ncomponents of diffs and reducing their size. Both benefits make it\\neasier for humans to visually parse the diffs in our version\\ncontrol system and make it easier to build automation around them.\\n\\n2.  Standard style makes it easier to accept contributions for an\\nopen-source repository and onboard new team members for a\\nclosed-source system.\\n\\n![](./media/image18.png)\\n\\n\\nOne aspect of consistent style is consistent code formatting (with\\nthings like whitespace). The standard tool for that in Python is\\n[the] *[black]* [Python\\nformatter](https://github.com/psf/black). It's a very\\nopinionated tool with a fairly narrow scope in terms of style. It\\nfocuses on things that can be fully automated and can be nicely\\nintegrated into your editor and automated workflows.\",\n",
       "  \"For non-automatable aspects of style (like missing docstrings), we\\nrecommend [*[flake8]*](https://flake8.pycqa.org/). It comes\\nwith many extensions and plugins such as docstring completeness, type\\nhinting, security, and common bugs.\\n\\nML codebases often have both Python code and shell scripts in them.\\nShell scripts are powerful, but they also have a lot of sharp edges.\\n*[shellcheck](https://www.shellcheck.net/)* knows all the\\nweird behaviors of bash that often cause errors and issues that aren't\\nimmediately obvious. It also provides explanations for why it's raising\\na warning or an error. It's very fast to run and can be easily\\nincorporated into your editor.\\n\\n![](./media/image6.png)\\n\\n\\nOne caveat to this is: **pedantic enforcement of style is obnoxious.**\\nTo avoid frustration with code style and linting, we recommend:\\n\\n1.  Filtering rules down to the minimal style that achieves the goals we\\nset out (sticking with standards, avoiding arguments, keeping\\nversion control history clean, etc.)\",\n",
       "  '2.  Having an \"opt-in\" application of rules and gradually growing\\ncoverage over time - which is especially important for existing\\ncodebases (which may have thousands of lines of code that we need\\nto be fixed).\\n\\n### 1.4 - Always Be Automating\\n\\n**To make the best use of testing and linting practices, you want to\\nautomate these tasks and connect to your cloud version control system\\n(VCS)**. Connecting to the VCS state reduces friction when trying to\\nreproduce or understand errors. Furthermore, running things outside of\\ndeveloper environments means that you can run tests automatically in\\nparallel to other development work.\\n\\nPopular, open-source repositories are the best place to learn about\\nautomation best practices. For instance, the PyTorch Github library has\\ntons of automated workflows built into the repo - such as workflows that\\nautomatically run on every push and pull.\\n\\n![](./media/image15.png)',\n",
       "  'The tool that PyTorch uses (and that we recommend) is [GitHub\\nActions](https://docs.github.com/en/actions), which ties\\nautomation directly to VCS. It is powerful, flexible, performant, and\\neasy to use. It gets great documentation, can be used with a YAML file,\\nand is embraced by the open-source community. There are other options\\nsuch as [pre-commit.ci](https://pre-commit.ci/),\\n[CircleCI](https://circleci.com/), and\\n[Jenkins](https://www.jenkins.io/); but GitHub Actions\\nseems to have won the hearts and minds in the open-source community in\\nthe last few years.\\n\\nTo keep your version control history as clean as possible, you want to\\nbe able to run tests and linters locally before committing. We recommend\\n*[pre-commit](https://github.com/pre-commit/pre-commit)*\\nto enforce hygiene checks. You can use it to run formatting, linting,\\netc. on every commit and keep the total runtime to a few seconds.\\n*pre-commit* is easy to run locally and easy to automate with GitHub\\nActions.',\n",
       "  \"**Automation to ensure the quality and integrity of our software is a\\nproductivity enhancer.** That's broader than just CI/CD. Automation\\nhelps you avoid context switching, surfaces issues early, is a force\\nmultiplier for small teams, and is better documented by default.\\n\\nOne caveat is that: **automation requires really knowing your tools.**\\nKnowing Docker well enough to use it is not the same as knowing Docker\\nwell enough to automate it. Bad automation, like bad tests, takes more\\ntime than it saves. Organizationally, that makes automation a good task\\nfor senior engineers who have knowledge of these tools, have ownership\\nover code, and can make these decisions around automation.\\n\\n### Summary\\n\\n1.  Automate tasks with GitHub Actions to reduce friction.\\n\\n2.  Use the standard Python toolkit for testing and cleaning your\\nprojects.\\n\\n3.  Choose testing and linting practices with the 80/20 principle,\\nshipping velocity, and usability/developer experience in mind.\\n\\n## 2 - Testing ML Systems\",\n",
       "  '1.  Testing ML is hard, but not impossible.\\n\\n2.  We should stick with the low-hanging fruit to start.\\n\\n3.  Test your code in production, but don\\'t release bad code.\\n\\n### 2.1 - Testing ML Is Hard, But Not Impossible\\n\\nSoftware engineering is where many testing practices have been\\ndeveloped. In software engineering, we compile source code into\\nprograms. In machine learning, training compiles data into a model.\\nThese components are harder to test:\\n\\n1.  Data is heavier and more inscrutable than source code.\\n\\n2.  Training is more complex and less well-defined.\\n\\n3.  Models have worse tools for debugging and inspection than compiled\\nprograms.\\n\\nIn this section, we will focus primarily on \"smoke\" tests. These tests\\nare easy to implement and still effective. They are among the 20% of\\ntests that get us 80% of the value.\\n\\n### 2.2 - Use Expectation Testing on Data',\n",
       "  '**We test our data by checking basic properties**. We express our\\nexpectations about the data, which might be things like there are no\\nnulls in this column or the completion date is after the start date.\\nWith expectation testing, you will start small with only a few\\nproperties and grow them slowly. You only want to test things that are\\nworth raising alarms and sending notifications to others.\\n\\n![](./media/image14.png)\\n\\n\\nWe recommend\\n[*[great_expectations]*](https://greatexpectations.io/) for\\ndata testing. It automatically generates documentation and quality\\nreports for your data, in addition to built-in logging and alerting\\ndesigned for expectation testing. To get started, check out [this\\nMadeWithML tutorial on\\ngreat_expectations](https://github.com/GokuMohandas/testing-ml).\\n\\n![](./media/image13.png)\\n\\nTo move forward, you want to stay as close to the data as possible:',\n",
       "  \"1.  A common pattern is that there's a benchmark dataset with\\nannotations (in academia) or an external annotation team (in the\\nindustry). A lot of the detailed information about that data can\\nbe extracted by simply looking at it.\\n\\n2.  One way for data to get internalized into the organization is that\\nat the start of the project, model developers annotate data ad-hoc\\n(especially if you don't have the budget for an external\\nannotation team).\\n\\n3.  However, if the model developers at the start of the project move on\\nand more developers get onboarded, that knowledge is diluted. A\\nbetter solution is an internal annotation team that has a regular\\ninformation flow with the model developers is a better solution.\",\n",
       "  \"4.  The best practice ([recommended by Shreya\\nShankar](https://twitter.com/sh_reya/status/1521903046392877056))\\nis t**o have a regular on-call rotation where model developers\\nannotate data themselves**. Ideally, these are fresh data so that\\nall members of the team who are developing models know about the\\ndata and build intuition/expertise in the data.\\n\\n### 2.3 - Use Memorization Testing on Training\\n\\n**Memorization is the simplest form of learning**. Deep neural networks\\nare very good at memorizing data, so checking whether your model can\\nmemorize a very small fraction of the full data set is a great smoke\\ntest for training. If a model can\\\\'t memorize, then something is clearly\\nvery wrong!\",\n",
       "  'Only really gross issues with training will show up with this test. For\\nexample, your gradients may not be calculated correctly, you have a\\nnumerical issue, or your labels have been shuffled; serious issues like\\nthese. Subtle bugs in your model or your data are not going to show up.\\nA way to catch smaller bugs is to include the length of run time in your\\ntest coverage. It\\'s a good way to detect if smaller issues are making it\\nharder for your model to learn. If the number of epochs it takes to\\nreach an expected performance suddenly goes up, it may be due to a\\ntraining bug. PyTorch Lightning has an \"*overfit_batches*\" feature that\\ncan help with this.\\n\\n**Make sure to tune memorization tests to run quickly, so you can\\nregularly run them**. If they are under 10 minutes or some short\\nthreshold, they can be run every PR or code change to better catch\\nbreaking changes. A couple of ideas for speeding up these tests are\\nbelow:\\n\\n![](./media/image3.png)',\n",
       "  \"Overall, these ideas lead to memorization tests that implement model\\ntraining on different time scale and allow you to mock out scenarios.\\n\\nA solid, if expensive idea for testing training is to **rerun old\\ntraining jobs with new code**. It's not something that can be run\\nfrequently, but doing so can yield lessons about what unexpected changes\\nmight have happened in your training pipeline. The main drawback is the\\npotential expense of running these tests. CI platforms like\\n[CircleCI](https://circleci.com/) charge a great deal for\\nGPUs, while others like Github Actions don't offer access to the\\nrelevant machines easily.\",\n",
       "  \"The best option for testing training is to **regularly run training with\\nnew data that's coming in from production**. This is still expensive,\\nbut it is directly related to improvements in model development, not\\njust testing for breakages. Setting this up requires **a data flywheel**\\nsimilar to what we talked about in Lecture 1. Further tooling needed to\\nachieve will be discussed down the line.\\n\\n### 2.4 - Adapt Regression Testing for Models\\n\\n**Models are effectively functions**. They have inputs and produce\\noutputs like any other function in code. So, why not test them like\\nfunctions with regression testing? For specific inputs, we can check to\\nsee whether the model consistently returns the same outputs. This is\\nbest done with simpler models like classification models. It's harder to\\nmaintain such tests with more complex models. However, even in a more\\ncomplex model scenario, regression testing can be useful for comparing\\nchanges from training to production.\\n\\n![](./media/image11.png)\",\n",
       "  'A more sophisticated approach to testing for ML models is to **use loss\\nvalues and model metrics to build documented test suites out of your\\ndata**. Consider this similar to [the test-driven\\ndevelopment](https://en.wikipedia.org/wiki/Test-driven_development)\\n(TDD) code writing paradigm. The test that is written before your code\\nin TDD is akin to your model\\'s loss performance; both represent the gap\\nbetween where your code needs to be and where it is. Over time, as we\\nimprove the loss metric, our model is getting closer to passing \"the\\ntest\" we\\'ve imposed on it. The gradient descent we use to improve the\\nmodel can be considered a TDD approach to machine learning models!\\n\\n![](./media/image9.png)\\n\\n\\nWhile gradient descent is somewhat like TDD, it\\'s not *exactly* the same\\nbecause simply reviewing metrics doesn\\'t tell us how to resolve model\\nfailures (the way traditional software tests do).',\n",
       "  'To fill in this gap, **start by [looking at the data points that have\\nthe highest loss](https://arxiv.org/abs/1912.05283)**. Flag\\nthem for a test suite composed of \"hard\" examples. Doing this provides\\ntwo advantages: it helps find where the model can be improved, and it\\ncan also help find errors in the data itself (i.e. poor labels).\\n\\nAs you examine these failures, you can aggregate types of failures into\\nnamed suites. For example in a self-driving car use case, you could have\\na \"night time\" suite and a \"reflection\" suite. **Building these test\\nsuites can be considered the machine learning version of regression\\ntesting**, where you take bugs that you\\\\\\'ve observed in production and\\nadd them to your test suite to make sure that they don\\\\\\'t come up again.\\n\\n![](./media/image8.png)',\n",
       "  \"The method can be quite manual, but there are some options for speeding\\nit up. Partnering with the annotation team at your company can help make\\ndeveloping these tests a lot faster. Another approach is to use a method\\ncalled [Domino](https://arxiv.org/abs/2203.14960) that\\nuses foundation models to find errors. Additionally, for testing NLP\\nmodels, use the\\n[CheckList](https://arxiv.org/abs/2005.04118) approach.\\n\\n### 2.5 - Test in Production, But Don't YOLO\\n\\nIt's crucial to test in true production settings. This is especially\\ntrue for machine learning models, because data is an important component\\nof both the production and the development environments. It's difficult\\nto ensure that both are very close to one another.\\n\\n**The best way to solve the training and production difference is to\\ntest in production**.\",\n",
       "  'Testing in production isn\\'t sufficient on its own. Rather, testing in\\nproduction allows us to develop tooling and infrastructure that allows\\nus to resolve production errors quickly (which are often quite\\nexpensive). It reduces pressure on other kinds of testing, but does not\\nreplace them.\\n\\n![](./media/image7.png)\\n\\n\\nWe will cover in detail the tooling needed for production monitoring and\\ncontinual learning of ML systems in a future lecture.\\n\\n### 2.6 - ML Test Score\\n\\nSo far, we have discussed writing \"smoke\" tests for ML: expectation\\ntests for data, memorization tests for training, and regression tests\\nfor models.',\n",
       "  \"**As your code base and team mature, adopt a more full-fledged approach\\nto testing ML systems like the approach identified in the [ML Test\\nScore](https://research.google/pubs/pub46555/) paper**. The\\nML Test Score is a rubric that evolved out of machine learning efforts\\nat Google. It's a strict rubric for ML test quality that covers data,\\nmodels, training, infrastructure, and production monitoring. It overlaps\\nwith, but goes beyond some of the recommendations we've offered.\\n\\n![](./media/image2.png)\\n\\nIt's rather expensive, but worth it for high stakes use cases that need\\nto be really well-engineered! To be really clear, this rubric is\\n*really* strict. Even our Text Recognizer system we've designed so far\\nmisses a few categories. Use the ML Test Score as inspiration to develop\\nthe right testing approach that works for your team's resources and\\nneeds.\\n\\n![](./media/image5.png)\\n\\n## 3 - Troubleshooting Models\",\n",
       "  '**Tests help us figure out something is wrong, but troubleshooting is\\nrequired to actually fix broken ML systems**. Models often require the\\nmost troubleshooting, and in this section we\\'ll cover a three step\\napproach to troubleshooting them.\\n\\n1.  \"Make it run\" by avoiding common errors.\\n\\n2.  \"Make it fast\" by profiling and removing bottlenecks.\\n\\n3.  \"Make it right\" by scaling model/data and sticking with proven\\narchitectures.\\n\\n### 3.1 - Make It Run\\n\\nThis is the easiest step for models; only a small portion of bugs cause\\nthe kind of loud failures that prevent a model from running at all.\\nWatch out for these bugs in advance and save yourself the trouble of\\nmodels that don\\'t run.',\n",
       "  \"The first type of bugs that prevent models from running at all are\\n**shape errors.** When the shape of the tensors don't match for the\\noperations run on them, models can't be trained or run. Prevent these\\nerrors by keeping notes on the expected size of tensors, annotate the\\nsizes in the code, and even step through your model code with a debugger\\nto check tensor size as you go.\\n\\n![](./media/image10.png)\",\n",
       "  \"The second type of bugs is out of **memory errors**. This occurs when\\nyou try to push a tensor to a GPU that is too large to fit. PyTorch\\nLightning has good tools to prevent this. Make sure you're using the\\nlowest precision your training can tolerate; a good default is 16 bit\\nprecision. Another common reason for this is trying to run a model on\\ntoo much data or too large a batch size. Use the autoscale batch size\\nfeature in PyTorch Lightning to pick the right size batch. You can use\\ngradient accumulation if these batch sizes get too small. If neither of\\nthese options work, you can look into manual techniques like tensor\\nparallelism and gradient checkpoints.\",\n",
       "  \"**Numerical errors** also cause machine learning failures. This is when\\nNaNs or infinite values show up in tensors. These issues most commonly\\nappear first in the gradient and then cascade through the model. PyTorch\\nLightning has a good tool for tracking and logging gradient norms. A\\ngood tip to check whether these issues are caused by precision issues is\\nto switch to Python 64 bit floats and see if that causes these issues to\\ngo away. Normalization layers tend to cause these issues, generally\\nspeaking. So watch out for how you do normalization!\\n\\n### 3.2 - Make It Fast\\n\\n![](./media/image4.png)\\n\\nOnce you can run a model, you'll want it to run fast. This can be tricky\\nbecause the performance of DNN training code is very counterintuitive.\\nFor example, transformers can actually spend more time in the MLP layer\\nthan the attention layer. Similarly, trivial components like loading\\ndata can soak up performance.\",\n",
       "  'To solve these issues, the primary solution is to **roll up your sleeves\\nand profile your code**. You can often find pretty easy Python changes\\nthat yield big results. Read these two tutorials by\\n[Charles](https://wandb.ai/wandb/trace/reports/A-Public-Dissection-of-a-PyTorch-Training-Step--Vmlldzo5MDE3NjU?galleryTag=&utm_source=fully_connected&utm_medium=blog&utm_campaign=using+the+pytorch+profiler+with+w%26b)\\nand [Horace](https://horace.io/brrr_intro.html) for more\\ndetails.\\n\\n### 3.3 - Make It Right\\n\\nAfter you make it run fast, make the model right. Unlike traditional\\nsoftware, machine learning models never are truly perfect. Production\\nperformance is never perfect. As such, it might be more appropriate to\\nsay \"make it as right as needed\".',\n",
       "  \"Knowing this, making the model run and run fast allows us to make the\\nmodel right through applying **scale.** To achieve performance benefits,\\nscaling a model or its data are generally fruitful and achievable\\nroutes. It's a lot easier to scale a fast model. [Research from OpenAI\\nand other institutions](https://arxiv.org/abs/2001.08361)\\nis showing that benefits from scale can be rigorously measured and\\npredicted across compute budget, dataset size, and parameter count.\\n\\n![](./media/image12.png)\\n\\nIf you can't afford to scale yourself, consider finetuning a model\\ntrained at scale for your task.\\n\\nSo far, all of the advice given has been model and task-agnostic.\\nAnything more detailed has to be specific to the model and the relevant\\ntask. Stick close to working architectures and hyperparameters from\\nplaces like HuggingFace, and try not to reinvent the wheel!\\n\\n## 4 - Resources\\n\\nHere are some helpful resources that discuss this topic.\\n\\n### Tweeters\\n\\n1.  [Julia Evans](https://twitter.com/b0rk)\",\n",
       "  '2.  [Charity Majors](https://twitter.com/mipsytipsy)\\n\\n3.  [Nelson Elhage](https://twitter.com/nelhage)\\n\\n4.  [kipply](https://twitter.com/kipperrii)\\n\\n5.  [Horace He](https://twitter.com/cHHillee)\\n\\n6.  [Andrej Karpathy](https://twitter.com/karpathy)\\n\\n7.  [Chip Huyen](https://twitter.com/chipro)\\n\\n8.  [Jeremy Howard](https://twitter.com/jeremyphoward)\\n\\n9.  [Ross Wightman](https://twitter.com/wightmanr)\\n\\n### Templates\\n\\n1.  [Lightning Hydra\\nTemplate](https://github.com/ashleve/lightning-hydra-template)\\n\\n2.  [NN Template](https://github.com/grok-ai/nn-template)\\n\\n3.  [Generic Deep Learning Project\\nTemplate](https://github.com/sudomaze/deep-learning-project-template)\\n\\n### Texts\\n\\n1.  [Reliable ML Systems\\ntalk](https://www.usenix.org/conference/opml20/presentation/papasian)\\n\\n2.  [\"ML Test Score\"\\npaper](https://research.google/pubs/pub46555/)\\n\\n3.  [\"Attack of the Cosmic\\nRays!\"](https://blogs.oracle.com/linux/post/attack-of-the-cosmic-rays)',\n",
       "  '4.  [\"Computers can be\\nunderstood\"](https://blog.nelhage.com/post/computers-can-be-understood/)\\n\\n5.  [\"Systems that defy detailed\\nunderstanding\"](https://blog.nelhage.com/post/systems-that-defy-understanding/)\\n\\n6.  [Testing section from MadeWithML course on\\nMLOps](https://madewithml.com/courses/mlops/testing/)']}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_to_text_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('docs/lecture-08.srt'),\n",
       " PosixPath('docs/lecture-09.srt'),\n",
       " PosixPath('docs/lecture-04.srt'),\n",
       " PosixPath('docs/lecture-05.srt'),\n",
       " PosixPath('docs/lecture-06.srt'),\n",
       " PosixPath('docs/lecture-02.srt'),\n",
       " PosixPath('docs/lecture-03.srt'),\n",
       " PosixPath('docs/lecture-01.srt')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "srt_filenames = [\n",
    "    elem for elem in DOCS_FOLDER.iterdir() if elem.is_file() and str(elem).endswith(\"srt\")]\n",
    "srt_filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'https://www.youtube.com/watch?v=-Iob-FW5jVM',\n",
       " 2: 'https://www.youtube.com/watch?v=BPYOsDCZbno',\n",
       " 3: 'https://www.youtube.com/watch?v=RLemHNAO5Lw',\n",
       " 4: 'https://www.youtube.com/watch?v=Jlm4oqW41vY',\n",
       " 5: 'https://www.youtube.com/watch?v=W3hKjXg7fXM',\n",
       " 6: 'https://www.youtube.com/watch?v=nra0Tt3a-Oc',\n",
       " 7: 'https://www.youtube.com/watch?v=Rm11UeGwGgk',\n",
       " 8: 'https://www.youtube.com/watch?v=a54xH6nT4Sw',\n",
       " 9: 'https://www.youtube.com/watch?v=7FQpbYTqjAA'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srt_urls = get_srt_urls()\n",
    "srt_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_from_timedelta(timedelta):\n",
    "    return int(timedelta.total_seconds())\n",
    "\n",
    "def create_srt_texts_and_metadatas(subtitles, base_url):\n",
    "    query_params_format = \"&t={start}s\"\n",
    "    texts, metadatas = [], []\n",
    "\n",
    "    for subtitle in subtitles:\n",
    "        raw_text = subtitle.content\n",
    "        text = subtitle.content.strip()\n",
    "        start = timestamp_from_timedelta(subtitle.start)\n",
    "        url = base_url + query_params_format.format(start=start)\n",
    "\n",
    "        texts.append(text)\n",
    "        metadatas.append(url)\n",
    "\n",
    "    return texts, metadatas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fn in srt_filenames:\n",
    "    idx = int(\"\".join(elem for elem in str(fn) if elem in string.digits))\n",
    "    srt_url = srt_urls[idx]\n",
    "\n",
    "    \n",
    "    srt_text = fn.open().read()\n",
    "    subtitles = list(srt.parse(srt_text))\n",
    "    texts, metadatas = create_srt_texts_and_metadatas(subtitles, srt_url)\n",
    "    \n",
    "    for text, url in zip(texts, metadatas):\n",
    "        url_to_text_split[url] = [text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_to_text_split.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"is today much less well defined than your software engineering interviews some common types of Assessments that I've seen are your normal sort of background and culture fit interviews whiteboard coding interviews similar to you'd see in software engineering pair coding like in software engineering but some more ml specific ones include pair debugging where you and an interviewer will sit down and run some ml code and try to find Hey where's the bug in this code oftentimes this is ml specific code and the goal is to test for how well is this person able to find bugs in ml code since bugs tend to be where we spend most of our time in machine learning math puzzles are often common especially involving things like linear algebra\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_to_text_split[\"https://www.youtube.com/watch?v=a54xH6nT4Sw&t=1351s\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_splits = []\n",
    "all_text_metadata = []\n",
    "\n",
    "for source_url, text_splits in url_to_text_split.items():\n",
    "    for text in text_splits:\n",
    "        all_text_splits.append(text)\n",
    "        all_text_metadata.append({\"source\": source_url})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(all_text_splits) == len(all_text_metadata)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENAI_KEY = \"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\"\n",
    "\n",
    "# embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_KEY)\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "docsearch = FAISS.from_texts(all_text_splits, embeddings, all_text_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSONAL_KEY = \"use your OpenAI key\"\n",
    "chain = VectorDBQAWithSourcesChain.from_chain_type(\n",
    "    OpenAI(temperature=0, openai_api_key=PERSONAL_KEY), chain_type=\"stuff\", vectorstore=docsearch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': ' Full Stack Deep Learning (FSDL) is the course and community for people who are building products that are powered by machine learning (ML).\\n',\n",
       " 'sources': 'https://fullstackdeeplearning.com/course/2022/lecture-2-development-infrastructure-and-tooling'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({\"question\": \"What is FSDL\"}, return_only_outputs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on StuffDocumentsChain in module langchain.chains.combine_documents.stuff object:\n",
      "\n",
      "class StuffDocumentsChain(langchain.chains.combine_documents.base.BaseCombineDocumentsChain, pydantic.main.BaseModel)\n",
      " |  StuffDocumentsChain(*, memory: langchain.chains.base.Memory = None, callback_manager: langchain.callbacks.base.BaseCallbackManager = None, verbose: bool = None, input_key: str = 'input_documents', output_key: str = 'output_text', llm_chain: langchain.chains.llm.LLMChain, document_prompt: langchain.prompts.base.BasePromptTemplate = None, document_variable_name: str) -> None\n",
      " |  \n",
      " |  Chain that combines documents by stuffing into context.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StuffDocumentsChain\n",
      " |      langchain.chains.combine_documents.base.BaseCombineDocumentsChain\n",
      " |      langchain.chains.base.Chain\n",
      " |      pydantic.main.BaseModel\n",
      " |      pydantic.utils.Representation\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  combine_docs(self, docs: List[langchain.docstore.document.Document], **kwargs: Any) -> Tuple[str, dict]\n",
      " |      Stuff all documents into one prompt and pass to LLM.\n",
      " |  \n",
      " |  prompt_length(self, docs: List[langchain.docstore.document.Document], **kwargs: Any) -> Optional[int]\n",
      " |      Get the prompt length by formatting the prompt.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  get_default_document_variable_name(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      " |      Get default document variable name, if not provided.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  Config = <class 'langchain.chains.combine_documents.stuff.StuffDocumen...\n",
      " |      Configuration for this pydantic object.\n",
      " |  \n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'document_prompt': <class 'langchain.prompts.base.B...\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __config__ = <class 'langchain.chains.combine_documents.stuff.Config'>\n",
      " |  \n",
      " |  __custom_root_type__ = False\n",
      " |  \n",
      " |  __exclude_fields__ = {'callback_manager': True}\n",
      " |  \n",
      " |  __fields__ = {'callback_manager': ModelField(name='callback_manager', ...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __include_fields__ = None\n",
      " |  \n",
      " |  __post_root_validators__ = []\n",
      " |  \n",
      " |  __pre_root_validators__ = [<function StuffDocumentsChain.get_default_d...\n",
      " |  \n",
      " |  __private_attributes__ = {}\n",
      " |  \n",
      " |  __schema_cache__ = {}\n",
      " |  \n",
      " |  __signature__ = <Signature (*, memory: langchain.chains.base.Mem...ate...\n",
      " |  \n",
      " |  __validators__ = {'callback_manager': [<pydantic.class_validators.Vali...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain.chains.combine_documents.base.BaseCombineDocumentsChain:\n",
      " |  \n",
      " |  input_keys\n",
      " |      Expect input key.\n",
      " |      \n",
      " |      :meta private:\n",
      " |  \n",
      " |  output_keys\n",
      " |      Return output key.\n",
      " |      \n",
      " |      :meta private:\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain.chains.base.Chain:\n",
      " |  \n",
      " |  __call__(self, inputs: Union[Dict[str, Any], Any], return_only_outputs: bool = False) -> Dict[str, Any]\n",
      " |      Run the logic of this chain and add to output if desired.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Dictionary of inputs, or single input if chain expects\n",
      " |              only one param.\n",
      " |          return_only_outputs: boolean for whether to return only outputs in the\n",
      " |              response. If True, only new keys generated by this chain will be\n",
      " |              returned. If False, both input keys and new keys generated by this\n",
      " |              chain will be returned. Defaults to False.\n",
      " |  \n",
      " |  apply(self, input_list: List[Dict[str, Any]]) -> List[Dict[str, str]]\n",
      " |      Call the chain on all inputs in the list.\n",
      " |  \n",
      " |  dict(self, **kwargs: Any) -> Dict\n",
      " |      Return dictionary representation of chain.\n",
      " |  \n",
      " |  run(self, *args: str, **kwargs: str) -> str\n",
      " |      Run the chain as text in, text out or multiple variables, text out.\n",
      " |  \n",
      " |  save(self, file_path: Union[pathlib.Path, str]) -> None\n",
      " |      Save the chain.\n",
      " |      \n",
      " |      Args:\n",
      " |          file_path: Path to file to save the chain to.\n",
      " |      \n",
      " |      Example:\n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          chain.save(file_path=\"path/chain.yaml\")\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain.chains.base.Chain:\n",
      " |  \n",
      " |  set_callback_manager(callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager]) -> langchain.callbacks.base.BaseCallbackManager from pydantic.main.ModelMetaclass\n",
      " |      If callback manager is None, set it.\n",
      " |      \n",
      " |      This allows users to pass in None as callback manager, which is a nice UX.\n",
      " |  \n",
      " |  set_verbose(verbose: Optional[bool]) -> bool from pydantic.main.ModelMetaclass\n",
      " |      If verbose is None, set it.\n",
      " |      \n",
      " |      This allows users to pass in None as verbose to access the global setting.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |  \n",
      " |  __getstate__(self) -> 'DictAny'\n",
      " |  \n",
      " |  __init__(__pydantic_self__, **data: Any) -> None\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |      \n",
      " |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      so `dict(model)` works\n",
      " |  \n",
      " |  __repr_args__(self) -> 'ReprArgs'\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |  \n",
      " |  __setstate__(self, state: 'DictAny') -> None\n",
      " |  \n",
      " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny')] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny')] = None, update: 'DictStrAny' = None, deep: bool = False) -> 'Model'\n",
      " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      " |      \n",
      " |      :param include: fields to include in new model\n",
      " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      " |          the new model: you should trust this data\n",
      " |      :param deep: set to `True` to make a deep copy of the model\n",
      " |      :return: new model instance\n",
      " |  \n",
      " |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny')] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny')] = None, by_alias: bool = False, skip_defaults: bool = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      " |      \n",
      " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  __try_update_forward_refs__() -> None from pydantic.main.ModelMetaclass\n",
      " |      Same as update_forward_refs but will not raise exception\n",
      " |      when forward references are not defined.\n",
      " |  \n",
      " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      " |  \n",
      " |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      " |  \n",
      " |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.utils.Representation:\n",
      " |  \n",
      " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      " |  \n",
      " |  __repr__(self) -> 'unicode'\n",
      " |  \n",
      " |  __repr_name__(self) -> 'unicode'\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      " |  \n",
      " |  __str__(self) -> 'unicode'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(chain.combine_documents_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['page_content', 'source'], output_parser=None, template='Content: {page_content}\\nSource: {source}', template_format='f-string')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.combine_documents_chain.document_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \n",
      "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
      "ALWAYS return a \"SOURCES\" part in your answer.\n",
      "\n",
      "QUESTION: Which state/country's law governs the interpretation of the contract?\n",
      "=========\n",
      "Content: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts in  relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for an  injunction or other relief to protect its Intellectual Property Rights.\n",
      "Source: 28-pl\n",
      "Content: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)  right or remedy.\n",
      "\n",
      "11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuation  in force of the remainder of the term (if any) and this Agreement.\n",
      "\n",
      "11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of any  kind between the parties.\n",
      "\n",
      "11.9 No Third-Party Beneficiaries.\n",
      "Source: 30-pl\n",
      "Content: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (as  defined in Clause 8.5) or that such a violation is reasonably likely to occur,\n",
      "Source: 4-pl\n",
      "=========\n",
      "FINAL ANSWER: This Agreement is governed by English law.\n",
      "SOURCES: 28-pl\n",
      "\n",
      "QUESTION: What did the president say about Michael Jackson?\n",
      "=========\n",
      "Content: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \n",
      "\n",
      "Last year COVID-19 kept us apart. This year we are finally together again. \n",
      "\n",
      "Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \n",
      "\n",
      "With a duty to one another to the American people to the Constitution. \n",
      "\n",
      "And with an unwavering resolve that freedom will always triumph over tyranny. \n",
      "\n",
      "Six days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \n",
      "\n",
      "He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \n",
      "\n",
      "He met the Ukrainian people. \n",
      "\n",
      "From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \n",
      "\n",
      "Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\n",
      "Source: 0-pl\n",
      "Content: And we won’t stop. \n",
      "\n",
      "We have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \n",
      "\n",
      "Let’s use this moment to reset. Let’s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \n",
      "\n",
      "Let’s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \n",
      "\n",
      "We can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \n",
      "\n",
      "I recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \n",
      "\n",
      "They were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \n",
      "\n",
      "Officer Mora was 27 years old. \n",
      "\n",
      "Officer Rivera was 22. \n",
      "\n",
      "Both Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. \n",
      "\n",
      "I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\n",
      "Source: 24-pl\n",
      "Content: And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \n",
      "\n",
      "To all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \n",
      "\n",
      "And I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. \n",
      "\n",
      "Tonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \n",
      "\n",
      "America will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \n",
      "\n",
      "These steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming. \n",
      "\n",
      "But I want you to know that we are going to be okay.\n",
      "Source: 5-pl\n",
      "Content: More support for patients and families. \n",
      "\n",
      "To get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \n",
      "\n",
      "It’s based on DARPA—the Defense Department project that led to the Internet, GPS, and so much more.  \n",
      "\n",
      "ARPA-H will have a singular purpose—to drive breakthroughs in cancer, Alzheimer’s, diabetes, and more. \n",
      "\n",
      "A unity agenda for the nation. \n",
      "\n",
      "We can do this. \n",
      "\n",
      "My fellow Americans—tonight , we have gathered in a sacred space—the citadel of our democracy. \n",
      "\n",
      "In this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \n",
      "\n",
      "We have fought for freedom, expanded liberty, defeated totalitarianism and terror. \n",
      "\n",
      "And built the strongest, freest, and most prosperous nation the world has ever known. \n",
      "\n",
      "Now is the hour. \n",
      "\n",
      "Our moment of responsibility. \n",
      "\n",
      "Our test of resolve and conscience, of history itself. \n",
      "\n",
      "It is in this moment that our character is formed. Our purpose is found. Our future is forged. \n",
      "\n",
      "Well I know this nation.\n",
      "Source: 34-pl\n",
      "=========\n",
      "FINAL ANSWER: The president did not mention Michael Jackson.\n",
      "SOURCES:\n",
      "\n",
      "QUESTION: {question}\n",
      "=========\n",
      "{summaries}\n",
      "=========\n",
      "FINAL ANSWER:\n"
     ]
    }
   ],
   "source": [
    "print(chain.combine_documents_chain.llm_chain.prompt.template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['summaries', 'question'], output_parser=None, template='Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \\nIf you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\nALWAYS return a \"SOURCES\" part in your answer.\\n\\nQUESTION: Which state/country\\'s law governs the interpretation of the contract?\\n=========\\nContent: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts in  relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for an  injunction or other relief to protect its Intellectual Property Rights.\\nSource: 28-pl\\nContent: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)  right or remedy.\\n\\n11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuation  in force of the remainder of the term (if any) and this Agreement.\\n\\n11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of any  kind between the parties.\\n\\n11.9 No Third-Party Beneficiaries.\\nSource: 30-pl\\nContent: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (as  defined in Clause 8.5) or that such a violation is reasonably likely to occur,\\nSource: 4-pl\\n=========\\nFINAL ANSWER: This Agreement is governed by English law.\\nSOURCES: 28-pl\\n\\nQUESTION: What did the president say about Michael Jackson?\\n=========\\nContent: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\n\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\\nSource: 0-pl\\nContent: And we won’t stop. \\n\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \\n\\nLet’s use this moment to reset. Let’s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \\n\\nLet’s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \\n\\nWe can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \\n\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\n\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\n\\nOfficer Mora was 27 years old. \\n\\nOfficer Rivera was 22. \\n\\nBoth Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. \\n\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\\nSource: 24-pl\\nContent: And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \\n\\nTo all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \\n\\nAnd I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. \\n\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \\n\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \\n\\nThese steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming. \\n\\nBut I want you to know that we are going to be okay.\\nSource: 5-pl\\nContent: More support for patients and families. \\n\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\n\\nIt’s based on DARPA—the Defense Department project that led to the Internet, GPS, and so much more.  \\n\\nARPA-H will have a singular purpose—to drive breakthroughs in cancer, Alzheimer’s, diabetes, and more. \\n\\nA unity agenda for the nation. \\n\\nWe can do this. \\n\\nMy fellow Americans—tonight , we have gathered in a sacred space—the citadel of our democracy. \\n\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\n\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\n\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\n\\nNow is the hour. \\n\\nOur moment of responsibility. \\n\\nOur test of resolve and conscience, of history itself. \\n\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged. \\n\\nWell I know this nation.\\nSource: 34-pl\\n=========\\nFINAL ANSWER: The president did not mention Michael Jackson.\\nSOURCES:\\n\\nQUESTION: {question}\\n=========\\n{summaries}\\n=========\\nFINAL ANSWER:', template_format='f-string')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.combine_documents_chain.llm_chain.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StuffDocumentsChain(memory=None, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x103686980>, verbose=False, input_key='input_documents', output_key='output_text', llm_chain=LLMChain(memory=None, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x103686980>, verbose=False, prompt=PromptTemplate(input_variables=['summaries', 'question'], output_parser=None, template='Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \\nIf you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\nALWAYS return a \"SOURCES\" part in your answer.\\n\\nQUESTION: Which state/country\\'s law governs the interpretation of the contract?\\n=========\\nContent: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts in  relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for an  injunction or other relief to protect its Intellectual Property Rights.\\nSource: 28-pl\\nContent: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)  right or remedy.\\n\\n11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuation  in force of the remainder of the term (if any) and this Agreement.\\n\\n11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of any  kind between the parties.\\n\\n11.9 No Third-Party Beneficiaries.\\nSource: 30-pl\\nContent: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (as  defined in Clause 8.5) or that such a violation is reasonably likely to occur,\\nSource: 4-pl\\n=========\\nFINAL ANSWER: This Agreement is governed by English law.\\nSOURCES: 28-pl\\n\\nQUESTION: What did the president say about Michael Jackson?\\n=========\\nContent: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\n\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\\nSource: 0-pl\\nContent: And we won’t stop. \\n\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \\n\\nLet’s use this moment to reset. Let’s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \\n\\nLet’s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \\n\\nWe can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \\n\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\n\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\n\\nOfficer Mora was 27 years old. \\n\\nOfficer Rivera was 22. \\n\\nBoth Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. \\n\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\\nSource: 24-pl\\nContent: And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \\n\\nTo all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \\n\\nAnd I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. \\n\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \\n\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \\n\\nThese steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming. \\n\\nBut I want you to know that we are going to be okay.\\nSource: 5-pl\\nContent: More support for patients and families. \\n\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\n\\nIt’s based on DARPA—the Defense Department project that led to the Internet, GPS, and so much more.  \\n\\nARPA-H will have a singular purpose—to drive breakthroughs in cancer, Alzheimer’s, diabetes, and more. \\n\\nA unity agenda for the nation. \\n\\nWe can do this. \\n\\nMy fellow Americans—tonight , we have gathered in a sacred space—the citadel of our democracy. \\n\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\n\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\n\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\n\\nNow is the hour. \\n\\nOur moment of responsibility. \\n\\nOur test of resolve and conscience, of history itself. \\n\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged. \\n\\nWell I know this nation.\\nSource: 34-pl\\n=========\\nFINAL ANSWER: The president did not mention Michael Jackson.\\nSOURCES:\\n\\nQUESTION: {question}\\n=========\\n{summaries}\\n=========\\nFINAL ANSWER:', template_format='f-string'), llm=OpenAI(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x103686980>, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key='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', batch_size=20, request_timeout=None, logit_bias={}), output_key='text'), document_prompt=PromptTemplate(input_variables=['page_content', 'source'], output_parser=None, template='Content: {page_content}\\nSource: {source}', template_format='f-string'), document_variable_name='summaries')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.combine_documents_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7627417350aa5515f2ac88c39cd826da69537c1e0e08d66e1a381333c448e74a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
